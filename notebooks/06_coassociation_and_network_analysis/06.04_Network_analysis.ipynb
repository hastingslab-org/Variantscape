{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cf9ecd9",
   "metadata": {},
   "source": [
    "# Create network graph and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24329d27",
   "metadata": {},
   "source": [
    "# 1) Set up libraries and work directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70822ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set up libraries\n",
    "import os\n",
    "import time \n",
    "import random\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from rich.text import Text\n",
    "from rich.console import Console\n",
    "from pyvis.network import Network\n",
    "with pd.ExcelWriter(\"myfile.xlsx\", engine=\"xlsxwriter\") as writer:\n",
    "\n",
    "# Work directory\n",
    "input_directory = \"INPUT_DIRECTORY\"\n",
    "variantscape_directory = \"VARIANTSCAPE_DIRECTORY\"\n",
    "variantscape_llm_coas_directory = \"VARIANTSCAPE_LLM_COAS_DIRECTORY\"\n",
    "\n",
    "# Load the metadata and variant dataset\n",
    "os.chdir(variantscape_directory)\n",
    "metadata_mapping = pd.read_csv(os.path.join(variantscape_directory, \"metadata_mapping_transposed.csv\"), low_memory=False)\n",
    "variant_analysis_df = pd.read_csv(os.path.join(variantscape_directory, \"cleaned_df_v4.csv\"), low_memory=False)\n",
    "print(metadata_mapping.head(5))\n",
    "\n",
    "os.chdir(variantscape_LLM_coas_directory)\n",
    "df_consensus = pd.read_csv(\"final_variant_treatment_consensus.csv\")\n",
    "os.chdir(variantscape_directory)\n",
    "print(\"\\n\\n\")\n",
    "print(df_consensus.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b6854f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Explore variant dictionary\n",
    "# Filter metadata_mapping for entities where Category == 'Variant'\n",
    "variant_entities = metadata_mapping[metadata_mapping['Category'] == 'Variant']['Entity'].tolist()\n",
    "print(f\"\\nTotal unique entities with Category == 'Variant': {len(variant_entities)}\")\n",
    "print(\"ScoreLabel List for Entity = Variant:\\n\")\n",
    "for idx, entity in enumerate(variant_entities, 1):\n",
    "    print(f\"{idx}. {entity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51997f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cancer dictionary\n",
    "# Filter metadata_mapping for entities where Category == 'Cancer'\n",
    "cancer_entities = metadata_mapping[metadata_mapping['Category'] == 'Cancer']['Entity'].tolist()\n",
    "print(f\"\\nTotal unique entities with Category == 'Cancer': {len(cancer_entities)}\")\n",
    "print(\"ScoreLabel List for Entity = Cancer:\\n\")\n",
    "for idx, entity in enumerate(cancer_entities, 1):\n",
    "    print(f\"{idx}. {entity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7696e83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treatment dictionary\n",
    "# Filter for Treatment entities\n",
    "treatment_entities = metadata_mapping[metadata_mapping['Category'] == 'Treatment']['Entity'].tolist()\n",
    "print(f\"\\n\\033[1mTotal unique entities with Category == 'Treatment': {len(treatment_entities)}\\033[0m\")\n",
    "print(\"\\n\\033[1mScoreLabel List for Entity = Treatment:\\033[0m\")\n",
    "for idx, entity in enumerate(treatment_entities, 1):\n",
    "    print(f\"{idx}. {entity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f90e6bd",
   "metadata": {},
   "source": [
    "# ========================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440afa8b",
   "metadata": {},
   "source": [
    "# 2) Network graph analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5fbc47",
   "metadata": {},
   "source": [
    "## Create network graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080c7503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean variant_analysis_df by removing non-entity columns\n",
    "# Ingnore non binary columns\n",
    "entity_columns = [col for col in variant_analysis_df.columns if col not in ['PaperId', 'Study_design', 'Abstract', 'Study_weight', 'PaperTitle']]\n",
    "\n",
    "# Create network graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes for each entity based on metadata_mapping dictionary\n",
    "for col in entity_columns:\n",
    "    category = metadata_mapping.loc[metadata_mapping['Entity'] == col, 'Category'].values[0]\n",
    "    G.add_node(col, category=category)\n",
    "\n",
    "# Add edges based on co-occurrence\n",
    "for idx, row in tqdm(variant_analysis_df.iterrows(), total=variant_analysis_df.shape[0], desc=\"Adding edges\"):\n",
    "    present_entities = row[entity_columns] == 1 \n",
    "    present_columns = present_entities[present_entities].index\n",
    "    for col1 in present_columns:\n",
    "        for col2 in present_columns:\n",
    "            if col1 != col2:\n",
    "                if G.has_edge(col1, col2):\n",
    "                    G[col1][col2]['weight'] += 1 \n",
    "                else:\n",
    "                    G.add_edge(col1, col2, weight=1)\n",
    "\n",
    "# Exmplore network analysis\n",
    "print(\"Number of nodes:\", len(G.nodes))\n",
    "print(\"Number of edges:\", len(G.edges))\n",
    "print(\"Network successfully created!\")\n",
    "\n",
    "# Save the network\n",
    "nx.write_gml(G, 'network_graph.gml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4c88b9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Verify the network analysis\n",
    "G = nx.read_gml('network_graph.gml')\n",
    "\n",
    "# Example query: Find the neighbors of a particular variant of interest\n",
    "variant_of_interest = \"s768i_EGFR\"\n",
    "variant_neighbors = set(G.neighbors(variant_of_interest))\n",
    "\n",
    "# Find treatments and cancers associated with the variant\n",
    "treatments = []\n",
    "cancers = []\n",
    "\n",
    "for node in variant_neighbors:\n",
    "    if G.nodes[node]['category'] == 'Treatment':\n",
    "        treatments.append(node)\n",
    "    elif G.nodes[node]['category'] == 'Cancer':\n",
    "        cancers.append(node)\n",
    "        \n",
    "print(f\"Treatments associated with variant '{variant_of_interest}':\")\n",
    "print(treatments)\n",
    "print(f\"\\nCancers associated with variant '{variant_of_interest}':\")\n",
    "print(cancers)\n",
    "\n",
    "# Find the top 5 most connected nodes (by degree centrality)\n",
    "centrality = nx.degree_centrality(G)\n",
    "sorted_centrality = sorted(centrality.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nTop 5 most connected nodes (based on degree centrality):\")\n",
    "for node, score in sorted_centrality[:5]:\n",
    "    print(f\"{node}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2a4371",
   "metadata": {},
   "source": [
    "## Create weighted network graph based on study design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39590bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create weighted network graph based on study design\n",
    "\n",
    "# Study‑design weights\n",
    "study_design_weights = {\n",
    "    'Systematic review study':       1.0,\n",
    "    'Clinical study':                1.0,\n",
    "    'Observational/RWE study':       0.9,\n",
    "    'Case report study':             0.9,\n",
    "    'In vivo/Animal study':          0.8,\n",
    "    'In vitro study':                0.7,\n",
    "    'In silico study':               0.6,\n",
    "    'Undefined':                     0.1,\n",
    "    'Other':                         0.1,\n",
    "}\n",
    "\n",
    "# Entity columns\n",
    "non_entity_cols = ['PaperId', 'Study_design', 'Abstract', 'Study_weight', 'PaperTitle']\n",
    "entity_columns = [c for c in variant_analysis_df.columns if c not in non_entity_cols]\n",
    "\n",
    "# Initialize two graphs\n",
    "G   = nx.Graph()  # unweighted (increments by 1)\n",
    "G_w = nx.Graph()  # weighted by evidence level\n",
    "\n",
    "# 1) Add nodes to both\n",
    "for col in entity_columns:\n",
    "    cat = metadata_mapping.loc[\n",
    "        metadata_mapping['Entity'] == col, 'Category'\n",
    "    ].values[0]\n",
    "    G.add_node(col,   category=cat)\n",
    "    G_w.add_node(col, category=cat)\n",
    "\n",
    "# 2) Add edges\n",
    "for _, row in tqdm(variant_analysis_df.iterrows(), total=len(variant_analysis_df), desc=\"Building graphs\"):\n",
    "    ents = row[entity_columns][row[entity_columns] == 1].index.tolist()\n",
    "    design = str(row['Study_design']).strip()\n",
    "    w      = study_design_weights.get(design, 0.5)\n",
    "\n",
    "    # iterate unique pairs\n",
    "    for i, e1 in enumerate(ents):\n",
    "        for e2 in ents[i+1:]:\n",
    "            # --- unweighted: +1 per co‑occurrence ---\n",
    "            if G.has_edge(e1, e2):\n",
    "                G[e1][e2]['weight'] += 1\n",
    "            else:\n",
    "                G.add_edge(e1, e2, weight=1)\n",
    "\n",
    "            # --- weighted: +w per co‑occurrence ---\n",
    "            if G_w.has_edge(e1, e2):\n",
    "                G_w[e1][e2]['weight'] += w\n",
    "            else:\n",
    "                G_w.add_edge(e1, e2, weight=w)\n",
    "\n",
    "# 3) Compare basic metrics\n",
    "print(\"=== Unweighted graph\")\n",
    "print(\" Nodes:\", G.number_of_nodes(), \"Edges:\", G.number_of_edges())\n",
    "deg_unw = nx.degree_centrality(G)\n",
    "top_unw = sorted(deg_unw.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "print(\" Top 5 nodes by degree centrality:\", top_unw)\n",
    "print(\"\\n=== Weighted graph\")\n",
    "print(\" Nodes:\", G_w.number_of_nodes(), \"Edges:\", G_w.number_of_edges())\n",
    "deg_w = nx.degree_centrality(G_w)\n",
    "top_w = sorted(deg_w.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "print(\" Top 5 nodes by degree centrality:\", top_w)\n",
    "\n",
    "# Compare edge‐weight distributions\n",
    "weights_unw = np.array([d['weight'] for _, _, d in G.edges(data=True)])\n",
    "weights_w   = np.array([d['weight'] for _, _, d in G_w.edges(data=True)])\n",
    "\n",
    "print(\"Edge weights (unweighted): mean=%.2f, std=%.2f\" % (weights_unw.mean(), weights_unw.std()))\n",
    "print(\"Edge weights (weighted):   mean=%.2f, std=%.2f\" % (weights_w.mean(),   weights_w.std()))\n",
    "print(\"Number of edges re‑weighted:\", np.sum(weights_w != weights_unw))\n",
    "\n",
    "# Correlate the two weight vectors\n",
    "corr = np.corrcoef(weights_unw, weights_w)[0,1]\n",
    "print(\"Pearson corr between unweighted/weighted edge‐weights: %.3f\" % corr)\n",
    "\n",
    "# Compare degree centrality shifts per node\n",
    "dc_unw = nx.degree_centrality(G)\n",
    "dc_w   = nx.degree_centrality(G_w)\n",
    "delta  = {n: dc_w[n] - dc_unw[n] for n in G.nodes()}\n",
    "nx.write_gml(G_w, 'network_graph_weighted.gml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0181b013",
   "metadata": {},
   "source": [
    "# ================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c6b1f8",
   "metadata": {},
   "source": [
    "# 3) Query the network to find associations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90aaf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets needed for network query\n",
    "# UPDATE Load (or reload) the weighted graph into G_w, then alias it to G for downstream code!!\n",
    "try:\n",
    "    G_w\n",
    "    print(\"Weighted graph already loaded. Proceeding with analysis.\")\n",
    "except NameError:\n",
    "    print(\"Loading the weighted network graph...\")\n",
    "    path = os.path.join(variantscape_directory, \"network_graph_weighted.gml\")\n",
    "    G_w = nx.read_gml(path)\n",
    "# Use G as the graph variable everywhere else\n",
    "G = G_w.copy()\n",
    "\n",
    "try:\n",
    "    df_consensus\n",
    "    print(\"df_consensus already loaded. Proceeding with analysis.\")\n",
    "except NameError:\n",
    "    print(\"Loading the consensus file...\")\n",
    "    df_consensus_path = variantscape_LLM_coas_directory + '/final_variant_treatment_consensus.csv'\n",
    "    df_consensus = pd.read_csv(df_consensus_path)\n",
    "    \n",
    "# Define variant and cancer of interest (with aliasing)\n",
    "user_input_cancer = \"NSCLC\"\n",
    "#variant_of_interest = \"v600e_BRAF\"\n",
    "\n",
    "############## Variants of interest for publication ##############\n",
    "#variant_of_interest = 'l858r_EGFR' #as durggable usecase\n",
    "#variant_of_interest = 't790m_EGFR' #as resistant usecase\n",
    "\n",
    "######################## Rare variants #############################\n",
    "#variant_of_interest = 'g469v_BRAF'\n",
    "#variant_of_interest = 'g719x_EGFR' # does not exist in network\n",
    "#variant_of_interest = 's768i_EGFR'\n",
    "variant_of_interest = 'l861q_EGFR'\n",
    "#variant_of_interest = 'l747p_EGFR' # no information in the network\n",
    "\n",
    "\n",
    "# Define alias mapping for cancer names\n",
    "cancer_alias_map = {\n",
    "    \"nsclc\": \"lung cancer\",\n",
    "    \"non-small cell lung cancer\": \"lung cancer\",\n",
    "    \"tnbc\": \"breast cancer\",\n",
    "    \"her2+ breast cancer\": \"breast cancer\"\n",
    "}\n",
    "\n",
    "# Normalize input\n",
    "clean_input = user_input_cancer.strip().lower()\n",
    "\n",
    "# Use alias if it exists, otherwise just use the same name\n",
    "cancer_of_interest = cancer_alias_map.get(clean_input, clean_input)\n",
    "display_cancer_name = user_input_cancer\n",
    "\n",
    "print(f\"\\n\\n\\033[1mCancer of interest set to:\\033[0m {display_cancer_name} (cancer type:'{cancer_of_interest}')\")\n",
    "print(f\"\\033[1mVariant of interest set to:\\033[0m {variant_of_interest}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128d5a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Updated weighted network graph with automated threshold, based on qualitative analysis\n",
    "\n",
    "# Adjustable thresholds\n",
    "TREATMENT_THRESHOLD_PERCENTILE = 80    # highlight top X% of treatment weights\n",
    "TREATMENT_MIN_HIGHLIGHT        = 300   # and require ≥X total weight\n",
    "CANCER_THRESHOLD_PERCENTILE    = 80    # highlight top X% of cancer–variant weights\n",
    "CANCER_MIN_HIGHLIGHT           = 80    # and require ≥X total weight\n",
    "\n",
    "# Prepare consensus lookup\n",
    "df_consensus[\"Variant_Treatment_Pair\"] = (\n",
    "    df_consensus[\"Variant_Treatment_Pair\"]\n",
    "    .str.strip()\n",
    "    .str.lower()\n",
    ")\n",
    "consensus_dict = dict(\n",
    "    zip(df_consensus[\"Variant_Treatment_Pair\"], df_consensus[\"Resolved_Prediction\"])\n",
    ")\n",
    "\n",
    "excluded_treatments = {\n",
    "    'chemotherapy', 'tyrosine kinase inhibitor', 'radiotherapy', 'hormone therapy',\n",
    "    'adjuvant chemotherapy', 'immunotherapy', 'immune checkpoint inhibitor',\n",
    "    'mrna vaccine', 'mtor inhibitor', 'radiation ionizing radiotherapy'\n",
    "}\n",
    "\n",
    "# Step 1: Cancer‐only treatments\n",
    "canc_nei = set(G.neighbors(cancer_of_interest))\n",
    "treatments = [\n",
    "    n for n in canc_nei\n",
    "    if G.nodes[n]['category']=='Treatment'\n",
    "    and n.lower() not in excluded_treatments\n",
    "]\n",
    "t_weights = {t: G[cancer_of_interest][t]['weight'] for t in treatments}\n",
    "top_cancer_treats = sorted(t_weights.items(), key=lambda x: x[1], reverse=True)[:6]\n",
    "c_w = list(t_weights.values())\n",
    "treat_pct = np.percentile(c_w, TREATMENT_THRESHOLD_PERCENTILE) if c_w else 0\n",
    "\n",
    "\n",
    "# Step 2: Variant + cancer associations\n",
    "sensitive, resistant = [], []\n",
    "for t in treatments:\n",
    "    try:\n",
    "        w = G[cancer_of_interest][t]['weight'] + G[variant_of_interest][t]['weight']\n",
    "        pred = consensus_dict.get(f\"{variant_of_interest} + {t}\".lower())\n",
    "        if pred == \"Sensitive\":\n",
    "            sensitive.append((t, w))\n",
    "        elif pred == \"Resistant\":\n",
    "            resistant.append((t, w))\n",
    "    except KeyError:\n",
    "        continue\n",
    "\n",
    "top_sens = sorted(sensitive, key=lambda x: x[1], reverse=True)[:6]\n",
    "top_res  = sorted(resistant, key=lambda x: x[1], reverse=True)[:6]\n",
    "sens_w = [w for _, w in sensitive]\n",
    "res_w  = [w for _, w in resistant]\n",
    "sens_pct = np.percentile(sens_w, TREATMENT_THRESHOLD_PERCENTILE) if sens_w else 0\n",
    "res_pct  = np.percentile(res_w,   TREATMENT_THRESHOLD_PERCENTILE) if res_w else 0\n",
    "\n",
    "print(f\"\\n\\033[1mSensitive treatments for variant '{variant_of_interest}' \"\n",
    "      f\"(≥{TREATMENT_THRESHOLD_PERCENTILE}th pct & ≥{TREATMENT_MIN_HIGHLIGHT}):\\033[0m\")\n",
    "for t, w in top_sens:\n",
    "    if w >= sens_pct and w >= TREATMENT_MIN_HIGHLIGHT:\n",
    "        print(f\"\\033[1;32m{t}: {w:.0f}\\033[0m\")\n",
    "    else:\n",
    "        print(f\"\\033[2;37m{t}: {w:.0f}\\033[0m\")\n",
    "\n",
    "print(f\"\\n\\033[1mResistant treatments for variant '{variant_of_interest}' \"\n",
    "      f\"(≥{TREATMENT_THRESHOLD_PERCENTILE}th pct & ≥{TREATMENT_MIN_HIGHLIGHT}):\\033[0m\")\n",
    "for t, w in top_res:\n",
    "    if w >= res_pct and w >= TREATMENT_MIN_HIGHLIGHT:\n",
    "        print(f\"\\033[1;31m{t}: {w:.0f}\\033[0m\")\n",
    "    else:\n",
    "        print(f\"\\033[2;37m{t}: {w:.0f}\\033[0m\")\n",
    "\n",
    "# Step 3: Other cancers for variant\n",
    "var_nei = set(G.neighbors(variant_of_interest))\n",
    "var_cancers = [\n",
    "    n for n in var_nei\n",
    "    if G.nodes[n]['category']=='Cancer'\n",
    "    and n != cancer_of_interest\n",
    "]\n",
    "vc_weights = {}\n",
    "for c in var_cancers:\n",
    "    w_v = G[variant_of_interest][c]['weight']\n",
    "    w_c = G[cancer_of_interest][c]['weight'] if G.has_edge(cancer_of_interest, c) else 0\n",
    "    vc_weights[c] = w_v + w_c\n",
    "\n",
    "top_var_c = sorted(vc_weights.items(), key=lambda x: x[1], reverse=True)[:6]\n",
    "vc_w = list(vc_weights.values())\n",
    "cancer_pct = np.percentile(vc_w, CANCER_THRESHOLD_PERCENTILE) if vc_w else 0\n",
    "\n",
    "print(f\"\\n\\033[1mOther cancers for variant '{variant_of_interest}' \"\n",
    "      f\"(≥{CANCER_THRESHOLD_PERCENTILE}th pct & ≥{CANCER_MIN_HIGHLIGHT}):\\033[0m\")\n",
    "for c, w in top_var_c:\n",
    "    if w >= cancer_pct and w >= CANCER_MIN_HIGHLIGHT:\n",
    "        print(f\"\\033[1;34m{c}: {w:.0f}\\033[0m\")\n",
    "    else:\n",
    "        print(f\"\\033[2;37m{c}: {w:.0f}\\033[0m\")\n",
    "\n",
    "# Assemble & save\n",
    "results = []\n",
    "for t, w in top_cancer_treats:\n",
    "    results.append({\n",
    "        \"Cancer\": display_cancer_name, \"Variant\": None,\n",
    "        \"Treatment\": t, \"Association_Type\": \"Cancer-Only\",\n",
    "        \"Prediction\": \"NA\", \"Combined_Weight\": w\n",
    "    })\n",
    "for t, w in top_sens:\n",
    "    results.append({\n",
    "        \"Cancer\": display_cancer_name, \"Variant\": variant_of_interest,\n",
    "        \"Treatment\": t, \"Association_Type\": \"Variant-Cancer\",\n",
    "        \"Prediction\": \"Sensitive\", \"Combined_Weight\": w\n",
    "    })\n",
    "for t, w in top_res:\n",
    "    results.append({\n",
    "        \"Cancer\": display_cancer_name, \"Variant\": variant_of_interest,\n",
    "        \"Treatment\": t, \"Association_Type\": \"Variant-Cancer\",\n",
    "        \"Prediction\": \"Resistant\", \"Combined_Weight\": w\n",
    "    })\n",
    "for c, w in top_var_c:\n",
    "    results.append({\n",
    "        \"Cancer\": c, \"Variant\": variant_of_interest,\n",
    "        \"Treatment\": None, \"Association_Type\": \"Cross-Cancer\",\n",
    "        \"Prediction\": \"NA\", \"Combined_Weight\": w\n",
    "    })\n",
    "\n",
    "df_out = pd.DataFrame(results)\n",
    "safe_c = display_cancer_name.replace(\" \", \"_\").lower()\n",
    "safe_v = variant_of_interest.replace(\" \", \"_\").lower()\n",
    "\n",
    "csv_fname   = f\"network_results_{safe_c}_{safe_v}.csv\"\n",
    "excel_fname = f\"network_results_{safe_c}_{safe_v}.xlsx\"\n",
    "\n",
    "df_out.to_csv(csv_fname, index=False)\n",
    "print(f\"\\nCSV saved to: {csv_fname}\")\n",
    "\n",
    "with pd.ExcelWriter(excel_fname, engine='xlsxwriter') as w:\n",
    "    df_out.to_excel(w, sheet_name='Results', index=False)\n",
    "print(f\"Excel saved to: {excel_fname}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fceb139c",
   "metadata": {},
   "source": [
    "# ================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4de00e",
   "metadata": {},
   "source": [
    "# 4) Create network figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75f20e6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create figure\n",
    "try:\n",
    "    G\n",
    "    print(\"Graph already loaded. Proceeding to visualization.\")\n",
    "except NameError:\n",
    "    print(\"Loading the network graph...\")\n",
    "    gml_file_path = variantscape_directory + '/network_graph.gml'\n",
    "    for _ in tqdm(range(100), desc=\"Loading Graph\", ncols=100, ascii=True):\n",
    "        time.sleep(0.01)\n",
    "    G = nx.read_gml(gml_file_path)\n",
    "\n",
    "# Select nodes from each category to ensure each category is included\n",
    "variant_nodes = [node for node, data in G.nodes(data=True) if data.get('category') == 'Variant']\n",
    "treatment_nodes = [node for node, data in G.nodes(data=True) if data.get('category') == 'Treatment']\n",
    "cancer_nodes = [node for node, data in G.nodes(data=True) if data.get('category') == 'Cancer']\n",
    "\n",
    "# Downsample the graph and ensure representation of each category\n",
    "# Sample all nodes if there are fewer than 1000 in a category!!!\n",
    "subset_variant_nodes = random.sample(variant_nodes, min(1000, len(variant_nodes)))\n",
    "subset_treatment_nodes = random.sample(treatment_nodes, min(1000, len(treatment_nodes)))\n",
    "subset_cancer_nodes = random.sample(cancer_nodes, min(1000, len(cancer_nodes)))\n",
    "subset_nodes = subset_variant_nodes + subset_treatment_nodes + subset_cancer_nodes\n",
    "\n",
    "# Create subgraph with selected nodes\n",
    "subset_edges = [(u, v) for u, v in G.edges() if u in subset_nodes and v in subset_nodes]\n",
    "G_sub = G.subgraph(subset_nodes).copy()\n",
    "G_sub.add_edges_from(subset_edges)\n",
    "\n",
    "# Initialize the PyVis network\n",
    "net = Network(notebook=True, height=\"800px\", width=\"100%\", directed=False, cdn_resources='in_line')\n",
    "category_colors = {\n",
    "    'Variant': 'lightblue',  \n",
    "    'Treatment': 'darkgreen',  \n",
    "    'Cancer': 'red'   \n",
    "}\n",
    "\n",
    "print(\"Adding graph to visualization...\")\n",
    "for _ in tqdm(range(100), desc=\"Adding Graph to PyVis\", ncols=100, ascii=True):\n",
    "    time.sleep(0.01)\n",
    "net.from_nx(G_sub)\n",
    "print(\"Customizing nodes and edges...\")\n",
    "for node in tqdm(net.nodes, desc=\"Customizing Nodes\", ncols=100, ascii=True):\n",
    "    node_id = node['id']\n",
    "    category = G_sub.nodes[node_id].get('category', 'variant') \n",
    "    node_color = category_colors.get(category, 'gray') \n",
    "    node['color'] = node_color\n",
    "    node['title'] = f\"Node Info: {node_id}\"\n",
    "    node['size'] = 20 \n",
    "for edge in net.edges:\n",
    "    edge['color'] = 'whitesmoke' \n",
    "    edge['width'] = 0.3  \n",
    "\n",
    "print(\"Applying force-directed layout...\")\n",
    "net.force_atlas_2based()\n",
    "\n",
    "# Save the network as an HTML file (open separately in new window!!!)\n",
    "output_file = \"downsampled_network_visualization_with_custom_colors_and_nodes.html\"\n",
    "print(\"Saving network as HTML file...\")\n",
    "for _ in tqdm(range(100), desc=\"Saving Network\", ncols=100, ascii=True):\n",
    "    time.sleep(0.01)\n",
    "net.save_graph(output_file)\n",
    "print(f\"Network visualization saved as {output_file}. Open it in a browser to explore.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
