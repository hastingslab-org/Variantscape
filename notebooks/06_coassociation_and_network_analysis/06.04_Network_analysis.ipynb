{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cf9ecd9",
   "metadata": {},
   "source": [
    "# Create network graph and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24329d27",
   "metadata": {},
   "source": [
    "# 1) Set up libraries and work directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70822ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set up libraries\n",
    "import os\n",
    "import time \n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "from tqdm import tqdm\n",
    "from rich.text import Text\n",
    "from rich.console import Console\n",
    "from pyvis.network import Network\n",
    "from ipysigma import Sigma\n",
    "\n",
    "# Work directory\n",
    "input_directory = \"INPUT_DIRECTORY\"\n",
    "variantscape_directory = \"VARIANTSCAPE_DIRECTORY\"\n",
    "variantscape_llm_coas_directory = \"VARIANTSCAPE_LLM_COAS_DIRECTORY\"\n",
    "\n",
    "# Load the metadata and variant dataset\n",
    "os.chdir(variantscape_directory)\n",
    "metadata_mapping = pd.read_csv(os.path.join(variantscape_directory, \"metadata_mapping_transposed.csv\"), low_memory=False)\n",
    "variant_analysis_df = pd.read_csv(os.path.join(variantscape_directory, \"cleaned_df_v4.csv\"), low_memory=False)\n",
    "print(metadata_mapping.head(5))\n",
    "\n",
    "os.chdir(variantscape_LLM_coas_directory)\n",
    "df_consensus = pd.read_csv(\"final_variant_treatment_consensus.csv\")\n",
    "os.chdir(variantscape_directory)\n",
    "print(\"\\n\\n\")\n",
    "print(df_consensus.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b6854f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Explore variant dictionary\n",
    "# Filter metadata_mapping for entities where Category == 'Variant'\n",
    "variant_entities = metadata_mapping[metadata_mapping['Category'] == 'Variant']['Entity'].tolist()\n",
    "print(f\"\\nTotal unique entities with Category == 'Variant': {len(variant_entities)}\")\n",
    "print(\"ScoreLabel List for Entity = Variant:\\n\")\n",
    "for idx, entity in enumerate(variant_entities, 1):\n",
    "    print(f\"{idx}. {entity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51997f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cancer dictionary\n",
    "# Filter metadata_mapping for entities where Category == 'Cancer'\n",
    "cancer_entities = metadata_mapping[metadata_mapping['Category'] == 'Cancer']['Entity'].tolist()\n",
    "print(f\"\\nTotal unique entities with Category == 'Cancer': {len(cancer_entities)}\")\n",
    "print(\"ScoreLabel List for Entity = Cancer:\\n\")\n",
    "for idx, entity in enumerate(cancer_entities, 1):\n",
    "    print(f\"{idx}. {entity.capitalize()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7696e83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treatment dictionary\n",
    "# Filter for Treatment entities\n",
    "treatment_entities = metadata_mapping[metadata_mapping['Category'] == 'Treatment']['Entity'].tolist()\n",
    "print(f\"\\n\\033[1mTotal unique entities with Category == 'Treatment': {len(treatment_entities)}\\033[0m\")\n",
    "print(\"\\n\\033[1mScoreLabel List for Entity = Treatment:\\033[0m\")\n",
    "for idx, entity in enumerate(treatment_entities, 1):\n",
    "    print(f\"{idx}. {entity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f90e6bd",
   "metadata": {},
   "source": [
    "# ========================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440afa8b",
   "metadata": {},
   "source": [
    "# 2) Network graph analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5fbc47",
   "metadata": {},
   "source": [
    "## Create network graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080c7503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean variant_analysis_df by removing non-entity columns\n",
    "# Ingnore non binary columns\n",
    "entity_columns = [col for col in variant_analysis_df.columns if col not in ['PaperId', 'Study_design', 'Abstract', 'Study_weight', 'PaperTitle']]\n",
    "\n",
    "# Create network graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes for each entity based on metadata_mapping dictionary\n",
    "for col in entity_columns:\n",
    "    category = metadata_mapping.loc[metadata_mapping['Entity'] == col, 'Category'].values[0]\n",
    "    G.add_node(col, category=category)\n",
    "\n",
    "# Add edges based on co-occurrence\n",
    "for idx, row in tqdm(variant_analysis_df.iterrows(), total=variant_analysis_df.shape[0], desc=\"Adding edges\"):\n",
    "    present_entities = row[entity_columns] == 1 \n",
    "    present_columns = present_entities[present_entities].index\n",
    "    for col1 in present_columns:\n",
    "        for col2 in present_columns:\n",
    "            if col1 != col2:\n",
    "                if G.has_edge(col1, col2):\n",
    "                    G[col1][col2]['weight'] += 1 \n",
    "                else:\n",
    "                    G.add_edge(col1, col2, weight=1)\n",
    "\n",
    "# Exmplore network analysis\n",
    "print(\"Number of nodes:\", len(G.nodes))\n",
    "print(\"Number of edges:\", len(G.edges))\n",
    "print(\"Network successfully created!\")\n",
    "\n",
    "# Save the network\n",
    "nx.write_gml(G, 'network_graph.gml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4c88b9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Verify the network analysis\n",
    "G = nx.read_gml('network_graph.gml')\n",
    "\n",
    "# Example query: Find the neighbors of a particular variant of interest\n",
    "variant_of_interest = \"s768i_EGFR\"\n",
    "variant_neighbors = set(G.neighbors(variant_of_interest))\n",
    "\n",
    "# Find treatments and cancers associated with the variant\n",
    "treatments = []\n",
    "cancers = []\n",
    "\n",
    "for node in variant_neighbors:\n",
    "    if G.nodes[node]['category'] == 'Treatment':\n",
    "        treatments.append(node)\n",
    "    elif G.nodes[node]['category'] == 'Cancer':\n",
    "        cancers.append(node)\n",
    "        \n",
    "print(f\"Treatments associated with variant '{variant_of_interest}':\")\n",
    "print(treatments)\n",
    "print(f\"\\nCancers associated with variant '{variant_of_interest}':\")\n",
    "print(cancers)\n",
    "\n",
    "# Find the top 5 most connected nodes (by degree centrality)\n",
    "centrality = nx.degree_centrality(G)\n",
    "sorted_centrality = sorted(centrality.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nTop 5 most connected nodes (based on degree centrality):\")\n",
    "for node, score in sorted_centrality[:5]:\n",
    "    print(f\"{node}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2a4371",
   "metadata": {},
   "source": [
    "## Create weighted network graph based on study design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39590bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create weighted network graph based on study design\n",
    "\n",
    "# Study‑design weights\n",
    "study_design_weights = {\n",
    "    'Systematic review study':       1.0,\n",
    "    'Clinical study':                1.0,\n",
    "    'Observational/RWE study':       0.9,\n",
    "    'Case report study':             0.9,\n",
    "    'In vivo/Animal study':          0.8,\n",
    "    'In vitro study':                0.7,\n",
    "    'In silico study':               0.6,\n",
    "    'Undefined':                     0.1,\n",
    "    'Other':                         0.1,\n",
    "}\n",
    "\n",
    "# Entity columns\n",
    "non_entity_cols = ['PaperId', 'Study_design', 'Abstract', 'Study_weight', 'PaperTitle']\n",
    "entity_columns = [c for c in variant_analysis_df.columns if c not in non_entity_cols]\n",
    "\n",
    "# Initialize two graphs\n",
    "G   = nx.Graph()  # unweighted (increments by 1)\n",
    "G_w = nx.Graph()  # weighted by evidence level\n",
    "\n",
    "# 1) Add nodes to both\n",
    "for col in entity_columns:\n",
    "    cat = metadata_mapping.loc[\n",
    "        metadata_mapping['Entity'] == col, 'Category'\n",
    "    ].values[0]\n",
    "    G.add_node(col,   category=cat)\n",
    "    G_w.add_node(col, category=cat)\n",
    "\n",
    "# 2) Add edges\n",
    "for _, row in tqdm(variant_analysis_df.iterrows(), total=len(variant_analysis_df), desc=\"Building graphs\"):\n",
    "    ents = row[entity_columns][row[entity_columns] == 1].index.tolist()\n",
    "    design = str(row['Study_design']).strip()\n",
    "    w      = study_design_weights.get(design, 0.5)\n",
    "\n",
    "    # iterate unique pairs\n",
    "    for i, e1 in enumerate(ents):\n",
    "        for e2 in ents[i+1:]:\n",
    "            # --- unweighted: +1 per co‑occurrence ---\n",
    "            if G.has_edge(e1, e2):\n",
    "                G[e1][e2]['weight'] += 1\n",
    "            else:\n",
    "                G.add_edge(e1, e2, weight=1)\n",
    "\n",
    "            # --- weighted: +w per co‑occurrence ---\n",
    "            if G_w.has_edge(e1, e2):\n",
    "                G_w[e1][e2]['weight'] += w\n",
    "            else:\n",
    "                G_w.add_edge(e1, e2, weight=w)\n",
    "\n",
    "# 3) Compare basic metrics\n",
    "print(\"=== Unweighted graph\")\n",
    "print(\" Nodes:\", G.number_of_nodes(), \"Edges:\", G.number_of_edges())\n",
    "deg_unw = nx.degree_centrality(G)\n",
    "top_unw = sorted(deg_unw.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "print(\" Top 5 nodes by degree centrality:\", top_unw)\n",
    "print(\"\\n=== Weighted graph\")\n",
    "print(\" Nodes:\", G_w.number_of_nodes(), \"Edges:\", G_w.number_of_edges())\n",
    "deg_w = nx.degree_centrality(G_w)\n",
    "top_w = sorted(deg_w.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "print(\" Top 5 nodes by degree centrality:\", top_w)\n",
    "\n",
    "# Compare edge‐weight distributions\n",
    "weights_unw = np.array([d['weight'] for _, _, d in G.edges(data=True)])\n",
    "weights_w   = np.array([d['weight'] for _, _, d in G_w.edges(data=True)])\n",
    "\n",
    "print(\"Edge weights (unweighted): mean=%.2f, std=%.2f\" % (weights_unw.mean(), weights_unw.std()))\n",
    "print(\"Edge weights (weighted):   mean=%.2f, std=%.2f\" % (weights_w.mean(),   weights_w.std()))\n",
    "print(\"Number of edges re‑weighted:\", np.sum(weights_w != weights_unw))\n",
    "\n",
    "# Correlate the two weight vectors\n",
    "corr = np.corrcoef(weights_unw, weights_w)[0,1]\n",
    "print(\"Pearson corr between unweighted/weighted edge‐weights: %.3f\" % corr)\n",
    "\n",
    "# Compare degree centrality shifts per node\n",
    "dc_unw = nx.degree_centrality(G)\n",
    "dc_w   = nx.degree_centrality(G_w)\n",
    "delta  = {n: dc_w[n] - dc_unw[n] for n in G.nodes()}\n",
    "nx.write_gml(G_w, 'network_graph_weighted.gml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0181b013",
   "metadata": {},
   "source": [
    "# ================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c6b1f8",
   "metadata": {},
   "source": [
    "# 3) Query the network to find associations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90aaf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets needed for network query\n",
    "# UPDATE Load (or reload) the weighted graph into G_w, then alias it to G for downstream code!!\n",
    "try:\n",
    "    G_w\n",
    "    print(\"Weighted graph already loaded. Proceeding with analysis.\")\n",
    "except NameError:\n",
    "    print(\"Loading the weighted network graph...\")\n",
    "    path = os.path.join(variantscape_directory, \"network_graph_weighted.gml\")\n",
    "    G_w = nx.read_gml(path)\n",
    "# Use G as the graph variable everywhere else\n",
    "G = G_w.copy()\n",
    "\n",
    "try:\n",
    "    df_consensus\n",
    "    print(\"df_consensus already loaded. Proceeding with analysis.\")\n",
    "except NameError:\n",
    "    print(\"Loading the consensus file...\")\n",
    "    df_consensus_path = variantscape_LLM_coas_directory + '/final_variant_treatment_consensus.csv'\n",
    "    df_consensus = pd.read_csv(df_consensus_path)\n",
    "    \n",
    "# Define variant and cancer of interest (with aliasing)\n",
    "user_input_cancer = \"NSCLC\"\n",
    "#variant_of_interest = \"v600e_BRAF\"\n",
    "\n",
    "############## Variants of interest for publication ##############\n",
    "#variant_of_interest = 'l858r_EGFR' #as durggable usecase\n",
    "#variant_of_interest = 't790m_EGFR' #as resistant usecase\n",
    "\n",
    "######################## Rare variants #############################\n",
    "#variant_of_interest = 'g469v_BRAF'\n",
    "#variant_of_interest = 's768i_EGFR'\n",
    "variant_of_interest = 'l861q_EGFR'\n",
    "#variant_of_interest = 'l747p_EGFR' # no associations in the network\n",
    "\n",
    "\n",
    "# Define alias mapping for cancer names\n",
    "cancer_alias_map = {\n",
    "    \"nsclc\": \"lung cancer\",\n",
    "    \"non-small cell lung cancer\": \"lung cancer\",\n",
    "    \"tnbc\": \"breast cancer\",\n",
    "    \"her2+ breast cancer\": \"breast cancer\"\n",
    "}\n",
    "\n",
    "# Normalize input\n",
    "clean_input = user_input_cancer.strip().lower()\n",
    "cancer_of_interest = cancer_alias_map.get(clean_input, clean_input)\n",
    "display_cancer_name = user_input_cancer\n",
    "\n",
    "print(f\"\\n\\n\\033[1mCancer of interest set to:\\033[0m {display_cancer_name} (cancer type:'{cancer_of_interest}')\")\n",
    "print(f\"\\033[1mVariant of interest set to:\\033[0m {variant_of_interest}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128d5a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Updated weighted network graph with automated threshold, based on qualitative analysis\n",
    "\n",
    "# Adjustable thresholds\n",
    "TREATMENT_THRESHOLD_PERCENTILE = 80    # highlight top X% of treatment weights\n",
    "TREATMENT_MIN_HIGHLIGHT        = 300   # and require ≥X total weight\n",
    "CANCER_THRESHOLD_PERCENTILE    = 80    # highlight top X% of cancer–variant weights\n",
    "CANCER_MIN_HIGHLIGHT           = 80    # and require ≥X total weight\n",
    "\n",
    "# Prepare consensus lookup\n",
    "df_consensus[\"Variant_Treatment_Pair\"] = (\n",
    "    df_consensus[\"Variant_Treatment_Pair\"]\n",
    "    .str.strip()\n",
    "    .str.lower()\n",
    ")\n",
    "consensus_dict = dict(\n",
    "    zip(df_consensus[\"Variant_Treatment_Pair\"], df_consensus[\"Resolved_Prediction\"])\n",
    ")\n",
    "\n",
    "excluded_treatments = {\n",
    "    'chemotherapy', 'tyrosine kinase inhibitor', 'radiotherapy', 'hormone therapy',\n",
    "    'adjuvant chemotherapy', 'immunotherapy', 'immune checkpoint inhibitor',\n",
    "    'mrna vaccine', 'mtor inhibitor', 'radiation ionizing radiotherapy'\n",
    "}\n",
    "\n",
    "# Cancer‐only treatments\n",
    "canc_nei = set(G.neighbors(cancer_of_interest))\n",
    "treatments = [\n",
    "    n for n in canc_nei\n",
    "    if G.nodes[n]['category']=='Treatment'\n",
    "    and n.lower() not in excluded_treatments\n",
    "]\n",
    "t_weights = {t: G[cancer_of_interest][t]['weight'] for t in treatments}\n",
    "top_cancer_treats = sorted(t_weights.items(), key=lambda x: x[1], reverse=True)[:6]\n",
    "c_w = list(t_weights.values())\n",
    "treat_pct = np.percentile(c_w, TREATMENT_THRESHOLD_PERCENTILE) if c_w else 0\n",
    "\n",
    "\n",
    "# Variant + cancer associations\n",
    "sensitive, resistant = [], []\n",
    "for t in treatments:\n",
    "    try:\n",
    "        w = G[cancer_of_interest][t]['weight'] + G[variant_of_interest][t]['weight']\n",
    "        pred = consensus_dict.get(f\"{variant_of_interest} + {t}\".lower())\n",
    "        if pred == \"Sensitive\":\n",
    "            sensitive.append((t, w))\n",
    "        elif pred == \"Resistant\":\n",
    "            resistant.append((t, w))\n",
    "    except KeyError:\n",
    "        continue\n",
    "\n",
    "top_sens = sorted(sensitive, key=lambda x: x[1], reverse=True)[:6]\n",
    "top_res  = sorted(resistant, key=lambda x: x[1], reverse=True)[:6]\n",
    "sens_w = [w for _, w in sensitive]\n",
    "res_w  = [w for _, w in resistant]\n",
    "sens_pct = np.percentile(sens_w, TREATMENT_THRESHOLD_PERCENTILE) if sens_w else 0\n",
    "res_pct  = np.percentile(res_w,   TREATMENT_THRESHOLD_PERCENTILE) if res_w else 0\n",
    "\n",
    "print(f\"\\n\\033[1mSensitive treatments for variant '{variant_of_interest}' \"\n",
    "      f\"(≥{TREATMENT_THRESHOLD_PERCENTILE}th pct & ≥{TREATMENT_MIN_HIGHLIGHT}):\\033[0m\")\n",
    "for t, w in top_sens:\n",
    "    if w >= sens_pct and w >= TREATMENT_MIN_HIGHLIGHT:\n",
    "        print(f\"\\033[1;32m{t}: {w:.0f}\\033[0m\")\n",
    "    else:\n",
    "        print(f\"\\033[2;37m{t}: {w:.0f}\\033[0m\")\n",
    "\n",
    "print(f\"\\n\\033[1mResistant treatments for variant '{variant_of_interest}' \"\n",
    "      f\"(≥{TREATMENT_THRESHOLD_PERCENTILE}th pct & ≥{TREATMENT_MIN_HIGHLIGHT}):\\033[0m\")\n",
    "for t, w in top_res:\n",
    "    if w >= res_pct and w >= TREATMENT_MIN_HIGHLIGHT:\n",
    "        print(f\"\\033[1;31m{t}: {w:.0f}\\033[0m\")\n",
    "    else:\n",
    "        print(f\"\\033[2;37m{t}: {w:.0f}\\033[0m\")\n",
    "\n",
    "        \n",
    "# Other cancers for variant\n",
    "var_nei = set(G.neighbors(variant_of_interest))\n",
    "var_cancers = [\n",
    "    n for n in var_nei\n",
    "    if G.nodes[n]['category']=='Cancer'\n",
    "    and n != cancer_of_interest\n",
    "]\n",
    "vc_weights = {}\n",
    "for c in var_cancers:\n",
    "    w_v = G[variant_of_interest][c]['weight']\n",
    "    w_c = G[cancer_of_interest][c]['weight'] if G.has_edge(cancer_of_interest, c) else 0\n",
    "    vc_weights[c] = w_v + w_c\n",
    "\n",
    "top_var_c = sorted(vc_weights.items(), key=lambda x: x[1], reverse=True)[:6]\n",
    "vc_w = list(vc_weights.values())\n",
    "cancer_pct = np.percentile(vc_w, CANCER_THRESHOLD_PERCENTILE) if vc_w else 0\n",
    "\n",
    "print(f\"\\n\\033[1mOther cancers for variant '{variant_of_interest}' \"\n",
    "      f\"(≥{CANCER_THRESHOLD_PERCENTILE}th pct & ≥{CANCER_MIN_HIGHLIGHT}):\\033[0m\")\n",
    "for c, w in top_var_c:\n",
    "    if w >= cancer_pct and w >= CANCER_MIN_HIGHLIGHT:\n",
    "        print(f\"\\033[1;34m{c.capitalize()}: {w:.0f}\\033[0m\")\n",
    "\n",
    "    else:\n",
    "        print(f\"\\033[2;37m{c.capitalize()}: {w:.0f}\\033[0m\")\n",
    "\n",
    "# Assemble\n",
    "results = []\n",
    "for t, w in top_cancer_treats:\n",
    "    results.append({\n",
    "        \"Cancer\": display_cancer_name, \"Variant\": None,\n",
    "        \"Treatment\": t, \"Association_Type\": \"Cancer-Only\",\n",
    "        \"Prediction\": \"NA\", \"Combined_Weight\": w\n",
    "    })\n",
    "for t, w in top_sens:\n",
    "    results.append({\n",
    "        \"Cancer\": display_cancer_name, \"Variant\": variant_of_interest,\n",
    "        \"Treatment\": t, \"Association_Type\": \"Variant-Cancer\",\n",
    "        \"Prediction\": \"Sensitive\", \"Combined_Weight\": w\n",
    "    })\n",
    "for t, w in top_res:\n",
    "    results.append({\n",
    "        \"Cancer\": display_cancer_name, \"Variant\": variant_of_interest,\n",
    "        \"Treatment\": t, \"Association_Type\": \"Variant-Cancer\",\n",
    "        \"Prediction\": \"Resistant\", \"Combined_Weight\": w\n",
    "    })\n",
    "for c, w in top_var_c:\n",
    "    results.append({\n",
    "        \"Cancer\": c, \"Variant\": variant_of_interest,\n",
    "        \"Treatment\": None, \"Association_Type\": \"Cross-Cancer\",\n",
    "        \"Prediction\": \"NA\", \"Combined_Weight\": w\n",
    "    })\n",
    "\n",
    "df_out = pd.DataFrame(results)\n",
    "safe_c = display_cancer_name.replace(\" \", \"_\").lower()\n",
    "safe_v = variant_of_interest.replace(\" \", \"_\").lower()\n",
    "\n",
    "csv_fname   = f\"network_results_{safe_c}_{safe_v}.csv\"\n",
    "excel_fname = f\"network_results_{safe_c}_{safe_v}.xlsx\"\n",
    "\n",
    "df_out.to_csv(csv_fname, index=False)\n",
    "print(f\"\\nCSV saved to: {csv_fname}\")\n",
    "\n",
    "with pd.ExcelWriter(excel_fname, engine='xlsxwriter') as w:\n",
    "    df_out.to_excel(w, sheet_name='Results', index=False)\n",
    "print(f\"Excel saved to: {excel_fname}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fceb139c",
   "metadata": {},
   "source": [
    "# ================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4de00e",
   "metadata": {},
   "source": [
    "# 4) Create network figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b440a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure to display the full network\n",
    "\n",
    "# Load the graph\n",
    "try:\n",
    "    G_w\n",
    "    print(\"Weighted graph already loaded. Proceeding with analysis.\")\n",
    "except NameError:\n",
    "    print(\"Loading the weighted network graph...\")\n",
    "    path = os.path.join(variantscape_directory, \"network_graph_weighted.gml\")\n",
    "    G_w = nx.read_gml(path)\n",
    "# Use G as the graph variable everywhere else\n",
    "G = G_w.copy()\n",
    "\n",
    "\n",
    "# Filter large components only\n",
    "print(\"Filtering graph...\")\n",
    "if nx.is_directed(G):\n",
    "    G = G.to_undirected()\n",
    "components = list(tqdm(nx.connected_components(G), desc=\"Finding Components\", ncols=100, ascii=True))\n",
    "components = [c for c in components if len(c) >= 50]\n",
    "G = G.subgraph(set().union(*components)).copy()\n",
    "print(f\"Filtered to {len(G.nodes())} nodes and {len(G.edges())} edges.\")\n",
    "\n",
    "print(\"Computing spring layout...\")\n",
    "pos = nx.spring_layout(G, seed=42, k=0.15, iterations=50)\n",
    "\n",
    "# Prepare node visuals (category color, degree size)\n",
    "print(\"Processing node visuals...\")\n",
    "degrees = dict(G.degree())\n",
    "node_sizes = [degrees[n] for n in G.nodes()]\n",
    "max_degree = max(node_sizes)\n",
    "scaled_sizes = [5 + (deg / max_degree) * 20 for deg in node_sizes]\n",
    "\n",
    "\n",
    "category_colors = { \n",
    "    \"Variant\": \"#00b0f0\",     \n",
    "    \"Treatment\": \"#32cd32\",  \n",
    "    \"Cancer\": \"#ff4c4c\"      \n",
    "}\n",
    "\n",
    "\n",
    "node_x, node_y, node_text, node_colors = [], [], [], []\n",
    "for node in tqdm(G.nodes(), desc=\"Placing Nodes\", ncols=100, ascii=True):\n",
    "    x, y = pos[node]\n",
    "    node_x.append(x)\n",
    "    node_y.append(y)\n",
    "    category = G.nodes[node].get('category', 'Variant')\n",
    "    node_colors.append(category_colors.get(category, '#888888'))\n",
    "    node_text.append(f\"{node}<br>Degree: {degrees[node]}\")\n",
    "\n",
    "node_trace = go.Scatter(\n",
    "    x=node_x,\n",
    "    y=node_y,\n",
    "    mode='markers',\n",
    "    text=node_text,\n",
    "    hoverinfo='text',\n",
    "    marker=dict(\n",
    "        size=scaled_sizes,\n",
    "        color=node_colors,\n",
    "        opacity=0.85,\n",
    "        line=dict(width=0.3, color='white')\n",
    "    ),\n",
    "    name=\"\",\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "# Draw edges (but do not show in legend)\n",
    "print(\"Processing edge traces...\")\n",
    "edge_x, edge_y = [], []\n",
    "for u, v in tqdm(G.edges(), desc=\"Drawing Edges\", total=G.number_of_edges(), ncols=100, ascii=True):\n",
    "    x0, y0 = pos[u]\n",
    "    x1, y1 = pos[v]\n",
    "    edge_x.extend([x0, x1, None])\n",
    "    edge_y.extend([y0, y1, None])\n",
    "\n",
    "edge_trace = go.Scatter(\n",
    "    x=edge_x,\n",
    "    y=edge_y,\n",
    "    line=dict(width=0.2, color='rgba(200,200,200,0.15)'),\n",
    "    hoverinfo='none',\n",
    "    mode='lines',\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "# Category legend with note on size\n",
    "legend_entries = [\n",
    "    go.Scatter(x=[None], y=[None], mode='markers',\n",
    "               marker=dict(size=12, color=category_colors['Variant']),\n",
    "               name=\"Variant\"),\n",
    "    go.Scatter(x=[None], y=[None], mode='markers',\n",
    "               marker=dict(size=12, color=category_colors['Treatment']),\n",
    "               name=\"Treatment\"),\n",
    "    go.Scatter(x=[None], y=[None], mode='markers',\n",
    "               marker=dict(size=12, color=category_colors['Cancer']),\n",
    "               name=\"Cancer\"),\n",
    "    go.Scatter(x=[None], y=[None], mode='markers',\n",
    "               marker=dict(size=0.01, color='rgba(0,0,0,0)'),\n",
    "               name=\"Size = Node degree\")\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# Create figure\n",
    "print(\"Creating Plotly figure...\")\n",
    "fig = go.Figure(data=[edge_trace, node_trace] + legend_entries,\n",
    "                layout=go.Layout(\n",
    "                    title=dict(\n",
    "                        text=\"Variantscape: Full network graph of molecular variants, treatments and cancer types\",\n",
    "                        font=dict(size=20, color='white'),\n",
    "                        x=0.5\n",
    "                    ),\n",
    "                    showlegend=True,\n",
    "                    legend=dict(\n",
    "                        font=dict(color='white'),\n",
    "                        title=dict(text=\"Legend\", font=dict(size=14, color='white')),\n",
    "                        bgcolor='rgba(0,0,0,0)',\n",
    "                        x=0.01,\n",
    "                        y=0.99\n",
    "                    ),\n",
    "                    hovermode='closest',\n",
    "                    margin=dict(b=10, l=10, r=10, t=80),\n",
    "                    xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "                    yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "                    plot_bgcolor='black',\n",
    "                    paper_bgcolor='black'\n",
    "                ))\n",
    "print(\"Saving to HTML...\")\n",
    "for _ in tqdm(range(100), desc=\"Writing HTML\", ncols=100, ascii=True):\n",
    "    time.sleep(0.002)\n",
    "fig.write_html(\"variantscape_network_graph.html\")\n",
    "print(\"Saved: variantscape_network_graph.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2ab04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if any nodes are disconnected from the network \n",
    "# (should not be the case, as only articles with all 3 entitiy mentions have been included in the analysis)\n",
    "\n",
    "G_full = G.copy()\n",
    "\n",
    "# Filter large components only\n",
    "print(\"Filtering graph...\")\n",
    "if nx.is_directed(G):\n",
    "    G = G.to_undirected()\n",
    "\n",
    "components = list(tqdm(nx.connected_components(G), desc=\"Finding Components\", ncols=100, ascii=True))\n",
    "components = [c for c in components if len(c) >= 50]\n",
    "G = G.subgraph(set().union(*components)).copy()\n",
    "\n",
    "dropped_nodes = len(G_full.nodes()) - len(G.nodes())\n",
    "print(f\"Filtered to {len(G.nodes())} nodes and {len(G.edges())} edges.\")\n",
    "print(f\"Dropped {dropped_nodes} nodes not in components ≥ 50.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
