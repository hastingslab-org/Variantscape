{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5f95fb2",
   "metadata": {},
   "source": [
    "# Normalization of cancers and treatments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c191c05",
   "metadata": {},
   "source": [
    "# 1) Set up libraries and datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7fbf24",
   "metadata": {},
   "source": [
    "## 1.1) Import libraries and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d33868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import networkx as nx\n",
    "import community\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.patches as mpatches\n",
    "from scipy.cluster.hierarchy import fcluster, linkage, dendrogram\n",
    "\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156ff2e3",
   "metadata": {},
   "source": [
    "## 1.2) Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075d91e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the working directory and file paths\n",
    "input_directory = \"INPUT_DIRECTORY\"\n",
    "output_directory = \"OUTPUT_DIRECTORY\"\n",
    "variantscape_directory = \"VARIANTSCAPE_DIRECTORY\"\n",
    "figure_directory = \"FIGURE_DIRECTORY\"\n",
    "\n",
    "os.chdir(variantscape_directory)\n",
    "print(\"Current directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0280f7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "#### In output directory\n",
    "os.chdir(output_directory)\n",
    "CIVIC_file = \"CIVIC_cancer_synonyms_cleaned.csv\"\n",
    "CIVIC_cancer_synonyms_df = pd.read_csv(CIVIC_file)\n",
    "print(\"- CIVIC cancer synonyms are loaded!\")\n",
    "print(CIVIC_cancer_synonyms_df.head(5))\n",
    "print(\"\\n\",\"###\" * 20, \"\\n\")\n",
    "\n",
    "## Treatment dataset\n",
    "columns_to_load = ['name', 'therapyAliases', 'final_parent', 'parent_treatment_category']\n",
    "treatment_file = \"CIVIC_ncit_df_finalparent_treatmentcategory.csv\"\n",
    "treatment_synonyms_df = pd.read_csv(treatment_file, usecols=columns_to_load)\n",
    "treatment_synonyms_df['therapyAliases'] = treatment_synonyms_df['therapyAliases'].apply(\n",
    "    lambda x: ', '.join(eval(x)) if isinstance(x, str) and x.startswith('[') else x\n",
    ")\n",
    "print(\"- Treatment synonyms are loaded!\")\n",
    "print(treatment_synonyms_df)\n",
    "print(len(treatment_synonyms_df))\n",
    "print(\"\\n\",\"###\" * 20, \"\\n\")\n",
    "\n",
    "#### In variantscape directory\n",
    "os.chdir(variantscape_directory)\n",
    "variant_df = pd.read_csv(\"final_variant_df_for_analysis.csv\", low_memory=False)\n",
    "print(\"\\n\\nFull variant dataset to analyze loaded!\")\n",
    "len_variant_df_rows, len_variant_df_cols = variant_df.shape\n",
    "print(f\"\\nContains {len_variant_df_rows:,} rows, {len_variant_df_cols:,} columns\")\n",
    "print(\"\\nSuccess!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c01adf",
   "metadata": {},
   "source": [
    "# Step 1) Harmonize cancers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad22d466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set of columns to ignore\n",
    "ignore_columns = {\n",
    "    'PaperId', 'PaperTitle', 'Citations', 'CoFoS', 'Authors', 'Abstract', \n",
    "    'Language', 'PubYear', 'PubDate', 'BioBERT', 'Sum_Gene_Mentions', \n",
    "    'Extracted_Cancer_Terms_old', 'Extracted_Cancer_Terms','Cancer_Type_Sum',\n",
    "    'Mapped_Cancer_Terms', 'Unmatched_Cancer_Terms', 'Remapped_Cancer_Terms', \n",
    "    'Final_Mapped_Cancer_Terms', 'Treatment_matches', 'Sum_treatments', 'Study_design',\n",
    "    'total_variant_count','LLM_Prompt', 'LLM_Response', 'Cleaned_Variant_Gene_Pairs'\n",
    "}\n",
    "\n",
    "# Check first row values for 'Cancer' (case-insensitive)\n",
    "cancer_cols_in_row0 = [\n",
    "    col for col in variant_df.columns\n",
    "    if \"cancer\" in str(variant_df.iloc[0][col]).lower()\n",
    "    and col not in ignore_columns\n",
    "]\n",
    "print(f\"Number of relevant 'Cancer' columns (excluding ignored ones): {len(cancer_cols_in_row0)}\")\n",
    "print(\"Columns list:\")\n",
    "print(cancer_cols_in_row0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d23d24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure final_parent values are lowercase and unique\n",
    "final_parents_cleaned = set(CIVIC_cancer_synonyms_df[\"final_parent\"].dropna().str.lower().unique())\n",
    "\n",
    "# Keyword mapping dictionary\n",
    "keyword_mapping = {\n",
    "    \"skin\": \"skin cancer\",\n",
    "    \"breast\": \"breast cancer\",\n",
    "    \"mammary\": \"breast cancer\",\n",
    "    \"mucinous\": \"mucinous cancer\",\n",
    "    \"lung\": \"lung cancer\", #this is Non-Small Cell Lung Cancer NSCLC!\n",
    "    \"bronchio\": \"lung cancer\",\n",
    "    \"spindle cell\": \"spindle cell cancer\",  \n",
    "    \"acute myeloid leukemia\": \"acute myeloid leukemia\",    \n",
    "    \"salivary gland\": \"salivary gland cancer\",\n",
    "    \"renal\": \"renal cancer\",\n",
    "    \"prostate\": \"prostate cancer\",\n",
    "    \"pancreatic\": \"pancreatic cancer\",\n",
    "    \"medulloblastoma\": \"medulloblastoma\",\n",
    "    \"lymphoblastic leukemia\": \"lymphoblastic leukemia\",\n",
    "    \"myeloid\": \"myeloid cancer\",\n",
    "    \"kidney\": \"kidney cancer\",\n",
    "    \"head and neck\": \"head and neck cancer\",\n",
    "    \"gastrointestinal\": \"gastrointestinal cancer\",\n",
    "    \"neurofibroma\": \"neurofibroma\",\n",
    "    \"ovarian\": \"ovarian cancer\",\n",
    "    \"ovary\": \"ovarian cancer\",\n",
    "    \"supratentorial ependymoma\": \"supratentorial ependymoma\",\n",
    "    \"cervix\": \"cervix cancer\", \n",
    "    \"cervical\": \"cervix cancer\",\n",
    "    \"colorectal\": \"colon cancer\",\n",
    "    \"colon\": \"colon cancer\",\n",
    "    \"endometri\": \"endometrial cancer\",\n",
    "    \"melano\": \"melanoma\",\n",
    "    \"laryngeal\": \"laryngeal cancer\",\n",
    "    \"glioma\": \"glioma\",\n",
    "    \"bone\": \"bone cancer\",\n",
    "    \"osteo\": \"bone cancer\",\n",
    "    \"peritoneal\": \"peritoneal cancer\",\n",
    "    \"astrocytoma\": \"astrocytoma\",\n",
    "    \"glioblastoma\": \"glioblastoma\",\n",
    "    \"gastric\": \"gastric cancer\",\n",
    "    \"mesothelioma\": \"mesothelioma\",\n",
    "    \"esophag\": \"esophagus cancer\",\n",
    "    \"thyroid\": \"thyroid cancer\",\n",
    "    \"thymus\": \"thymus cancer\",\n",
    "    \"uterus\": \"uterine cancer\",\n",
    "    \"spinal\": \"spinal cancer\",\n",
    "    \"hepatocellular\": \"liver cancer\",\n",
    "    \"cholangio\": \"cholangio cancer\",\n",
    "    \"bile duct\": \"biliary tract cancer\",\n",
    "    \"gliosarcoma\": \"glioma\",\n",
    "    \"myeloid cancer\": \"hematologic cancer\",\n",
    "    \"myeloproliferative cancer\": \"hematologic cancer\",\n",
    "    \"myelodysplastic syndrome\": \"hematologic cancer\",\n",
    "    \"essential thrombocythemia\": \"hematologic cancer\",\n",
    "    \"myelofibrosis\": \"hematologic cancer\",\n",
    "    \"barrett\": \"esophagus cancer\",\n",
    "    \"fraumeni\": \"li-fraumeni syndrome\",\n",
    "    \"liposarcoma\": \"liposarcoma\",\n",
    "    \"papillary\": \"papillary cancer\"\n",
    "}\n",
    "\n",
    "# Classifier for leukemia/lymphoma terms\n",
    "def classify_leukemia_lymphoma(name):\n",
    "    if isinstance(name, str):\n",
    "        name_lower = name.lower()\n",
    "        has_leukemia = any(word in name_lower for word in [\"leukemia\", \"leukemic\"])\n",
    "        has_lymphoma = \"lymphoma\" in name_lower\n",
    "        if has_leukemia and has_lymphoma:\n",
    "            return \"leukemia/lymphoma\"\n",
    "        elif has_leukemia:\n",
    "            return \"leukemia\"\n",
    "        elif has_lymphoma:\n",
    "            return \"lymphoma\"\n",
    "    return None\n",
    "\n",
    "# Classify each column name\n",
    "def classify_column(col_name):\n",
    "    col_lower = col_name.lower()\n",
    "    # 1. Direct match with cleaned final_parents\n",
    "    if col_lower in final_parents_cleaned:\n",
    "        return (\"final_parent match\", col_lower)\n",
    "    # 2. Keyword mapping\n",
    "    for keyword, replacement in keyword_mapping.items():\n",
    "        if re.search(rf\"\\b{re.escape(keyword)}\\b\", col_lower):\n",
    "            return (\"keyword match\", replacement)\n",
    "    # 3. Leukemia/Lymphoma classification\n",
    "    leukemia_lymphoma_result = classify_leukemia_lymphoma(col_lower)\n",
    "    if leukemia_lymphoma_result:\n",
    "        return (\"leukemia/lymphoma classification\", leukemia_lymphoma_result)\n",
    "    # 4. No match â€” return original name as value\n",
    "    return (\"not matched\", col_name)\n",
    "\n",
    "# Apply classification\n",
    "match_info = []\n",
    "for col in cancer_cols_in_row0:\n",
    "    match_type, match_val = classify_column(col)\n",
    "    match_info.append({\"column_name\": col, \"match_type\": match_type, \"matched_value\": match_val})\n",
    "print(f\"\\n{'Column Name':<40} | {'Match Type':<30} | Matched Value\")\n",
    "print(\"-\" * 100)\n",
    "for row in match_info:\n",
    "    print(f\"{row['column_name']:<40} | {row['match_type']:<30} | {row['matched_value']}\")\n",
    "\n",
    "# Count unique matched values (rightmost column)\n",
    "unique_matched_values = set(row['matched_value'] for row in match_info)\n",
    "print(f\"\\nTotal unique matched cancer types: {len(unique_matched_values)}\")\n",
    "\n",
    "# Count how many columns belong to each match type\n",
    "match_types = [\"final_parent match\", \"keyword match\", \"leukemia/lymphoma classification\", \"not matched\"]\n",
    "for match_type in match_types:\n",
    "    count = sum(1 for row in match_info if row[\"match_type\"] == match_type)\n",
    "    print(f\"\\nTotal columns with match type '{match_type}': {count}\")\n",
    "\n",
    "# Calculate the total sum of the last values\n",
    "total_columns = sum(1 for match_type in match_types for row in match_info if row[\"match_type\"] == match_type)\n",
    "print(f\"\\nTotal sum of columns: {total_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01857344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect all columns before renaming (including cancer and non-cancer columns)\n",
    "total_columns_before = variant_df.shape[1]\n",
    "print(f\"\\nTotal columns in variant_df (before renaming): {total_columns_before}\")\n",
    "\n",
    "column_rename_map = {\n",
    "    row[\"column_name\"]: row[\"matched_value\"] for row in match_info\n",
    "}\n",
    "\n",
    "# Extract cancer-related columns and rename them based on the classification result\n",
    "cancer_data = variant_df[cancer_cols_in_row0].copy()\n",
    "cancer_data_renamed = cancer_data.rename(columns=column_rename_map)\n",
    "\n",
    "# Calculate how many columns in index 0 have the word 'Cancer' excluding ignored columns\n",
    "columns_with_cancer_in_index0 = [\n",
    "    col for col in variant_df.columns \n",
    "    if 'cancer' in str(variant_df.iloc[0][col]).lower() and col not in ignore_columns\n",
    "]\n",
    "print(f\"\\nNumber of columns in index 0 with 'Cancer' in their names (excluding ignored columns): {len(columns_with_cancer_in_index0)}\")\n",
    "\n",
    "# Merge columns using the logical OR (group by columns) - Fixing deprecation warning\n",
    "cancer_data_cleaned = cancer_data_renamed.drop(index=0).copy()\n",
    "cancer_data_cleaned = cancer_data_cleaned.apply(pd.to_numeric, errors='coerce').fillna(0).astype(int)\n",
    "merged_cancer_binary_df = cancer_data_cleaned.T.groupby(level=0).max().T\n",
    "\n",
    "# Count how many cancer columns were merged\n",
    "total_cancer_columns_after_merge = len(merged_cancer_binary_df.columns)  # Columns after merging\n",
    "total_cancer_columns_before_merge = len(cancer_data_renamed.columns)  # Columns before merging\n",
    "columns_merged = total_cancer_columns_before_merge - total_cancer_columns_after_merge\n",
    "print(f\"\\nTotal cancer columns merged: {columns_merged}\")\n",
    "\n",
    "# Inspect non-cancer columns (keep them separate)\n",
    "non_cancer_columns = [col for col in variant_df.columns if col not in cancer_cols_in_row0 and col not in ignore_columns]\n",
    "print(f\"\\nTotal non-cancer columns: {len(non_cancer_columns)}\")\n",
    "\n",
    "# Merge the final non-cancer columns with the merged cancer columns (preserving column order)\n",
    "ignored_columns_in_final = [col for col in ignore_columns if col in variant_df.columns]\n",
    "ignored_df      = variant_df[ignored_columns_in_final]\n",
    "non_cancer_df   = variant_df[non_cancer_columns]\n",
    "cancer_df       = merged_cancer_binary_df\n",
    "cleaned_df_v1 = pd.concat([ignored_df, non_cancer_df, cancer_df], axis=1)\n",
    "\n",
    "# Add \"Cancer\" to index 0 for the merged cancer columns ONLY\n",
    "cleaned_df_v1.iloc[0] = cleaned_df_v1.iloc[0].astype('object') \n",
    "cleaned_df_v1.iloc[0] = cleaned_df_v1.iloc[0].fillna(\"Cancer\")\n",
    "\n",
    "total_columns_after_merge = cleaned_df_v1.shape[1]\n",
    "print(f\"\\nTotal columns after renaming and merging (final merged DataFrame): {total_columns_after_merge}\")\n",
    "\n",
    "columns_removed_by_merging = total_columns_before - total_columns_after_merge\n",
    "print(f\"\\nTotal columns REMOVED by merging: {columns_removed_by_merging}\")\n",
    "\n",
    "# Verify the math check\n",
    "calculated_total_after_merge = total_columns_before - columns_removed_by_merging\n",
    "print(f\"\\nCalculated Total columns AFTER renaming and merging: {calculated_total_after_merge}\")\n",
    "print(f\"Total columns AFTER renaming and merging (actual): {total_columns_after_merge}\")\n",
    "print(f\"Do the values match? {calculated_total_after_merge == total_columns_after_merge}\")\n",
    "\n",
    "cancer_columns_after_merge = [\n",
    "    col for col in cleaned_df_v1.columns \n",
    "    if 'cancer' in col.lower() and col not in ignore_columns\n",
    "]\n",
    "print(f\"\\nTotal 'Cancer' columns after merging (excluding ignored columns): {len(cancer_columns_after_merge)}\")\n",
    "\n",
    "print(\"\\nFinal Dataset (first 5 rows):\")\n",
    "print(cleaned_df_v1.head())\n",
    "\n",
    "final_dataset_name = \"cleaned_df_v1\"\n",
    "print(f\"\\nThe name of the final dataset after this execution is: {final_dataset_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5849fae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all column names in cleaned_df_v1 as a list\n",
    "print(f\"Length rows: {len(cleaned_df_v1):,}\")\n",
    "print(f\"Length columns: {len(cleaned_df_v1.columns):,}\")\n",
    "\n",
    "print(\"\\nAll column names in cleaned_df_v1:\")\n",
    "print(cleaned_df_v1.columns.tolist())  # List all column names\n",
    "\n",
    "# Print the values in index 0 (first row) as a list\n",
    "print(\"\\nValues in index 0 (first row) of cleaned_df_v1:\")\n",
    "print(cleaned_df_v1.iloc[0].tolist())  # List all values in index 0\n",
    "\n",
    "# Check for empty values in index 0\n",
    "empty_values_in_index0 = cleaned_df_v1.iloc[0].isna().sum()  # Count of NaN values in index 0\n",
    "print(f\"\\nNumber of empty (NaN) values in index 0 (first row): {empty_values_in_index0}\")\n",
    "\n",
    "# Print a list of columns where index 0 has NaN values\n",
    "columns_with_empty_values = cleaned_df_v1.columns[cleaned_df_v1.iloc[0].isna()].tolist()\n",
    "print(f\"\\nColumns with NaN values in index 0: {columns_with_empty_values}\")\n",
    "\n",
    "cleaned_df_v1.to_csv(\"cleaned_df_v1.csv\", index=False)\n",
    "print(\"\\ncleaned_df_v1 has been kindly saved as 'cleaned_df_v1.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a9961f",
   "metadata": {},
   "source": [
    "# Step 2) Treatment normaliaztion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a7010c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print total number of rows and columns in cleaned_df_v1\n",
    "print(f\"Total rows: {cleaned_df_v1.shape[0]:,}\")\n",
    "print(f\"Total columns: {cleaned_df_v1.shape[1]:,}\")\n",
    "\n",
    "# Find all columns where index 0 contains the word 'treatment'\n",
    "columns_with_treatment_in_index0 = [\n",
    "    col for col in cleaned_df_v1.columns\n",
    "    if 'treatment' in str(cleaned_df_v1.iloc[0][col]).lower() and col not in ignore_columns\n",
    "]\n",
    "print(\"\\nColumns with 'treatment' in index 0 (excluding ignored columns):\")\n",
    "for col in columns_with_treatment_in_index0:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6247fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate overall shape\n",
    "print(f\"Total rows: {cleaned_df_v1.shape[0]:,}\")\n",
    "print(f\"Total columns: {cleaned_df_v1.shape[1]:,}\")\n",
    "\n",
    "metadata_row = cleaned_df_v1.iloc[0]\n",
    "metadata_counts = metadata_row.value_counts()\n",
    "\n",
    "print(\"\\nUnique values in index 0 and how many columns have each:\")\n",
    "print(metadata_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abf985b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Copy cleaned_df_v1 into cleaned_df_v2\n",
    "cleaned_df_v2 = cleaned_df_v1.copy()\n",
    "exclude_columns = ['PaperId', 'PaperTitle','Abstract']\n",
    "\n",
    "# Identify columns with metadata \"Initial df\" in row 0, excluding PaperId and PaperTitle\n",
    "columns_to_drop = [col for col in cleaned_df_v2.columns if cleaned_df_v2.iloc[0][col] == 'Initial df' and col not in exclude_columns]\n",
    "\n",
    "# Drop the specified columns in ignore_columns\n",
    "ignore_columns = [\n",
    "    'Citations', 'CoFoS', 'Authors', \n",
    "    'Language', 'PubYear', 'PubDate', 'BioBERT', 'Sum_Gene_Mentions', \n",
    "    'Extracted_Cancer_Terms_old', 'Extracted_Cancer_Terms', 'Cancer_Type_Sum',\n",
    "    'Mapped_Cancer_Terms', 'Unmatched_Cancer_Terms', 'Remapped_Cancer_Terms', \n",
    "    'Final_Mapped_Cancer_Terms', 'Treatment_matches', 'Sum_treatments', \n",
    "    'total_variant_count', 'LLM_Prompt', 'LLM_Response', 'Cleaned_Variant_Gene_Pairs'\n",
    "]\n",
    "\n",
    "# Add the ignore columns to the list of columns to drop, ensuring the columns are present in the DataFrame\n",
    "columns_to_drop += [col for col in ignore_columns if col in cleaned_df_v2.columns]\n",
    "print(\"Columns to be dropped based on metadata (Initial df) and ignore_columns:\")\n",
    "print(columns_to_drop)\n",
    "\n",
    "# Calculate the number of columns before and after dropping\n",
    "columns_before = cleaned_df_v1.shape[1]  \n",
    "columns_after = cleaned_df_v2.shape[1]\n",
    "\n",
    "# Drop the identified columns\n",
    "cleaned_df_v2 = cleaned_df_v2.drop(columns=columns_to_drop)\n",
    "cleaned_df_v2.reset_index(drop=True, inplace=True)\n",
    "columns_dropped = columns_before - cleaned_df_v2.shape[1]\n",
    "\n",
    "print(f\"\\nNumber of columns in cleaned_df_v1: {columns_before}\")\n",
    "print(f\"Number of columns in cleaned_df_v2: {cleaned_df_v2.shape[1]}\")\n",
    "print(f\"Total number of columns dropped: {columns_dropped}\")\n",
    "\n",
    "dropped_columns = [col for col in cleaned_df_v1.columns if col not in cleaned_df_v2.columns]\n",
    "print(\"\\nDropped columns:\")\n",
    "print(dropped_columns)\n",
    "print(\"\\nCleaned DataFrame (cleaned_df_v2) after dropping columns:\")\n",
    "print(cleaned_df_v2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41427634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique study designs\n",
    "unique_study_designs = cleaned_df_v2['Study_design'].unique()\n",
    "\n",
    "# Create a DataFrame to display the study designs with an index\n",
    "study_design_df = pd.DataFrame(unique_study_designs, columns=['Study_Design'])\n",
    "print(\"Study designs within dataset:\")\n",
    "print(study_design_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b332222f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Add Study weights\n",
    "\n",
    "# Create a v3 copy of cleaned_df_v2\n",
    "cleaned_df_v3 = cleaned_df_v2.copy()\n",
    "\n",
    "# Define conditions\n",
    "# From the evidence pyramide\n",
    "study_design_weights = {\n",
    "    'Systematic review study': 1.0,      # Highest weight for Systematic Reviews & Meta-Analyses\n",
    "    'Clinical study': 1.0,               # High-quality clinical studies (RCTs, Cohorts)\n",
    "    'Observational/RWE study': 0.9,      # Real-world evidence, Cohort studies, Case-control studies\n",
    "    'Case report study': 0.9,           # Case reports and case series\n",
    "    'In vivo/Animal study': 0.8,         # Animal studies - mechanistic but not clinical\n",
    "    'In vitro study': 0.7,               # Lab-based studies - valuable but less direct relevance\n",
    "    'In silico study': 0.7,             # Computational predictions - hypothesis-generating but unverified\n",
    "    'Undefined': 0.5                     # Lowest weight for poorly categorized studies\n",
    "}\n",
    "\n",
    "\n",
    "# Create a new 'Study_weight' column with default value 'Study' for the metadata row (index 0)\n",
    "cleaned_df_v3['Study_weight'] = cleaned_df_v3['Study_design'].map(study_design_weights).fillna(0.0)\n",
    "\n",
    "cleaned_df_v3.loc[0, 'Study_weight'] = 'Study'\n",
    "columns = cleaned_df_v3.columns.tolist()\n",
    "study_design_index = columns.index('Study_design')\n",
    "\n",
    "columns.insert(study_design_index + 1, columns.pop(columns.index('Study_weight')))\n",
    "cleaned_df_v3 = cleaned_df_v3[columns]\n",
    "\n",
    "# Generate output\n",
    "print(cleaned_df_v3.head(10))\n",
    "output_file = 'cleaned_df_v3.csv'\n",
    "cleaned_df_v3.to_csv(output_file, index=False)\n",
    "print(f\"\\n\\nNumber of columns in dataset v2: {len(cleaned_df_v2.columns):,}\")\n",
    "print(f\"Number of rows in dataset v2: {len(cleaned_df_v2):,}\")\n",
    "print(f\"Number of columns in dataset v3: {len(cleaned_df_v3.columns):,}\")\n",
    "print(f\"Number of rows in dataset v3: {len(cleaned_df_v3):,}\")\n",
    "print(f\"\\n7nFile successfully saved as: '{output_file}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52fb66a",
   "metadata": {},
   "source": [
    "# =============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbac13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract metadata from row 0 and store it in a mapping DataFrame\n",
    "cleaned_df_v4 = cleaned_df_v3.copy()\n",
    "metadata_row = cleaned_df_v4.iloc[0]\n",
    "metadata_mapping = pd.DataFrame(metadata_row).T\n",
    "\n",
    "# Drop the metadata row (index 0) from cleaned_df_v4\n",
    "cleaned_df_v4 = cleaned_df_v4.drop(0)\n",
    "cleaned_df_v4.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Harmonize binary matrix to integers (0 and 1) for all columns except text columns\n",
    "text_columns = ['PaperId', 'PaperTitle', 'Abstract', 'Study_design',\"Study_weight\"]\n",
    "\n",
    "for col in cleaned_df_v4.columns:\n",
    "    if col not in text_columns:\n",
    "        cleaned_df_v4[col] = pd.to_numeric(cleaned_df_v4[col], errors='coerce')\n",
    "        cleaned_df_v4[col] = cleaned_df_v4[col].apply(lambda x: 1 if x == 1.0 else (0 if x == 0.0 else np.nan))\n",
    "\n",
    "# Generate output\n",
    "cleaned_df_v4.to_csv('cleaned_df_v4.csv', index=False)\n",
    "metadata_mapping.to_csv('metadata_mapping.csv', index=False)\n",
    "print(\"Metadata Mapping DataFrame:\")\n",
    "print(\"\\nCleaned DataFrame (cleaned_df_v4) after dropping metadata row and harmonizing binary matrix:\")\n",
    "print(cleaned_df_v4.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfdf0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting only the columns except 'PaperId', 'PaperTitle', 'Abstract', and 'Study_design'\n",
    "subset_df = cleaned_df_v4.drop(columns=['PaperId', 'PaperTitle', 'Abstract', 'Study_design','Study_weight'])\n",
    "\n",
    "# Checking for columns that contain only 1 and 0\n",
    "binary_columns = [col for col in subset_df.columns if subset_df[col].dropna().isin([0, 1]).all()]\n",
    "\n",
    "print(\"Columns containing only 1 and 0:\")\n",
    "for col in binary_columns:\n",
    "    print(col)\n",
    "\n",
    "sums = subset_df[binary_columns].sum()\n",
    "sorted_sums = sums.sort_values(ascending=False)\n",
    "print(\"Sorted list of binary columns by count of 1s (from highest to lowest):\")\n",
    "for col, count in sorted_sums.items():\n",
    "    print(f\"{col}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145f2db4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Validation 1: Check binary columns for consistency (should only contain 0 and 1)\n",
    "binary_columns = [col for col in cleaned_df_v4.columns if col not in ['PaperId', 'PaperTitle', 'Abstract','Study_design','Study_weight']]\n",
    "\n",
    "# Check if all values in these columns are either 0 or 1\n",
    "binary_valid = True\n",
    "for col in binary_columns:\n",
    "    if not cleaned_df_v4[col].isin([0, 1]).all():\n",
    "        print(f\"Warning: Column '{col}' contains values other than 0 and 1.\")\n",
    "        binary_valid = False\n",
    "if binary_valid:\n",
    "    print(\"\\nAll binary columns are valid (only contain 0 and 1).\")\n",
    "\n",
    "# Validation 2: Check text columns to ensure they don't contain numeric values\n",
    "text_columns = ['PaperId', 'PaperTitle', 'Abstract', 'Study_design']\n",
    "text_valid = True\n",
    "for col in text_columns:\n",
    "    if cleaned_df_v4[col].apply(lambda x: isinstance(x, (int, float))).any():\n",
    "        print(f\"Warning: Column '{col}' contains numeric values.\")\n",
    "        text_valid = False\n",
    "\n",
    "if text_valid:\n",
    "    print(\"\\nAll text columns contain valid string values.\")\n",
    "\n",
    "# Validation 3: Check for missing values\n",
    "missing_values = cleaned_df_v4.isnull().sum()\n",
    "if missing_values.any():\n",
    "    print(\"\\nMissing values found in the following columns:\")\n",
    "    print(missing_values[missing_values > 0])\n",
    "else:\n",
    "    print(\"\\nNo missing values found.\")\n",
    "    \n",
    "    \n",
    "# Checking for columns that only contain 0s (sum = 0)\n",
    "zero_only_columns = [col for col in binary_columns if cleaned_df_v4[col].sum() == 0]\n",
    "\n",
    "if zero_only_columns:\n",
    "    print(\"\\nColumns that contain only 0s (sum = 0):\")\n",
    "    for col in zero_only_columns:\n",
    "        print(f\" - {col}\")\n",
    "else:\n",
    "    print(\"\\nNo columns found that only contain 0s (sum = 0).\")\n",
    "\n",
    "print(\"\\nValidation Summary:\")\n",
    "print(f\"Binary columns valid: {binary_valid}\")\n",
    "print(f\"Text columns valid: {text_valid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69748dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final df and csv\n",
    "#cleaned_df_v4.to_csv('cleaned_df_v4.csv', index=False)\n",
    "#metadata_mapping.to_csv('metadata_mapping.csv', index=False)\n",
    "print(cleaned_df_v4.shape)\n",
    "print(cleaned_df_v4.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:llms] *",
   "language": "python",
   "name": "conda-env-llms-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
