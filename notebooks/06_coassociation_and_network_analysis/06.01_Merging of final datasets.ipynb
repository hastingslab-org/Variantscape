{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5f95fb2",
   "metadata": {},
   "source": [
    "# Merging of all datasets based on 'PaperId' columns\n",
    "\n",
    "## Overview\n",
    "Processing and merging of four different datasets based on the common identifier 'PaperId'. \n",
    "The goal is to consolidate information while avoiding duplication of key columns!\n",
    "\n",
    "---\n",
    "\n",
    "## Datasets Used:\n",
    "- **'Initial_df'** → The primary dataset with extracted gene names (starting point for merging).\n",
    "- **'Cancer_df'** → Contains additional cancer-related information.\n",
    "- **'Treatment_df'** → Contains treatment-related information.\n",
    "- **'Study_df'** → Contains study-related information.\n",
    "- **'Variant_df'** → Contains extarcted variant information.\n",
    "---\n",
    "\n",
    "## Steps for merging\n",
    "1. **Load the datasets** from CSV files.\n",
    "2. **Identify and ignore redundant columns** in `cancer_df`, `treatment_df`, and `study_df`:\n",
    "   - Columns like `PaperTitle`, `Author`, `Abstract`, `Citations`, `PubDate`, `PubYear`, `CoFoS`, or `ID` (already in `Initial_df`) are dropped.\n",
    "3. **Merge all datasets** on `PaperId` using a left join (keeping all records from `Initial_df`).\n",
    "4. **Check for duplicate column names** after merging to avoid redundancy.\n",
    "5. **Print a summary** of the final merged dataset, including its shape and column names."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c191c05",
   "metadata": {},
   "source": [
    "# 1) Set up libraries and datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7fbf24",
   "metadata": {},
   "source": [
    "## 1.1) Import libraries and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d33868",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Success!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156ff2e3",
   "metadata": {},
   "source": [
    "## 1.2) Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075d91e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directories\n",
    "base_dir = \"/data/BASE_DIRECTORY\"\n",
    "input_dir = os.path.join(base_dir, \"Input\")\n",
    "output_dir = os.path.join(base_dir, \"Output\")\n",
    "LLM_variant_dir = os.path.join(output_dir, \"LLM_variant_analysis\")\n",
    "coassociation_dir = os.path.join(output_dir, \"coassociation_analysis\")\n",
    "classifier_dir = os.path.join(output_dir, \"gc_batch_files\")\n",
    "variantscape_dir = os.path.join(output_dir, \"Variantscape_analysis\")\n",
    "\n",
    "# Confirm current working directory\n",
    "os.chdir(output_dir)\n",
    "print(\"Current directory:\", os.getcwd())\n",
    "\n",
    "# Load datasets\n",
    "# Input directory files\n",
    "ESCAT_genes = pd.read_csv(os.path.join(input_dir, \"ESCAT_pc_genes.csv\"), header=None)\n",
    "oncomine_genes = pd.read_csv(os.path.join(input_dir, \"oncomine_ngs_panel.csv\"), header=None)\n",
    "\n",
    "# Output directory files\n",
    "initial_df = pd.read_csv(os.path.join(output_dir, \"cleaned_BioBERT_data.csv\"))\n",
    "cancer_df = pd.read_csv(os.path.join(output_dir, \"binary_cancer_matrix_filtered.csv\"))\n",
    "treatment_df = pd.read_csv(os.path.join(output_dir, \"filtered_treatment_mapping_with_matches.csv\"))\n",
    "\n",
    "# Classifier directory file\n",
    "study_df = pd.read_csv(os.path.join(classifier_dir, \"final_gc_classificaton_output_199726.csv\"))\n",
    "\n",
    "# Variant directory file\n",
    "variant_df = pd.read_csv(os.path.join(LLM_variant_dir, \"normalized_merged_variant_matrix_v4.csv\"))\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22083c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate datasets\n",
    "os.chdir(output_directory)\n",
    "print(\"\\nDataset Lengths:\")\n",
    "print(f\"-Initial dataset: {len(initial_df):,}\")\n",
    "print(f\"-Cancer type dataset: {len(cancer_df):,}\")\n",
    "print(f\"-Treatment dataset: {len(treatment_df):,}\")\n",
    "print(f\"-Study design dataset: {len(study_df):,}\")\n",
    "print(f\"-Variant dataset: {len(variant_df):,}\")\n",
    "print(\"\\nNumber of ESCAT genes:\", len(ESCAT_genes))\n",
    "print(\"Number of oncomine genes:\", len(oncomine_genes))\n",
    "\n",
    "# Check if all datasets have the same length\n",
    "dataset_lengths = [len(initial_df), len(cancer_df), len(treatment_df), len(study_df),len(variant_df)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded32f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate cancer of unkonwn primary origin\n",
    "column_name = \"cancer of unknown primary origin\"\n",
    "if column_name in cancer_df.columns:\n",
    "    paper_ids_with_unknown_primary = cancer_df[cancer_df[column_name] == 1]['PaperId'].tolist()\n",
    "    print(f\"Found {len(paper_ids_with_unknown_primary)} PaperIds where '{column_name}' is 1.\")\n",
    "    print(f\"PaperIds: {paper_ids_with_unknown_primary}\")\n",
    "    matching_treatment_rows = treatment_df[treatment_df['PaperId'].isin(paper_ids_with_unknown_primary)]\n",
    "    if not matching_treatment_rows.empty:\n",
    "        print(\"\\nRows in treatment_df with matching PaperIds:\")\n",
    "        print(matching_treatment_rows)\n",
    "\n",
    "        for column in matching_treatment_rows.columns:\n",
    "            if column != 'PaperId': \n",
    "                if matching_treatment_rows[column].dtype == 'object':\n",
    "                    count_ones = (matching_treatment_rows[column] == '1').sum()\n",
    "                else:\n",
    "                    count_ones = (matching_treatment_rows[column] == 1).sum()\n",
    "                if count_ones > 0:\n",
    "                    print(f\"Column '{column}' has {count_ones} '1's in matching PaperIds\")\n",
    "        matching_variant_rows = variant_df[variant_df['PaperId'].isin(paper_ids_with_unknown_primary)]\n",
    "\n",
    "        if not matching_variant_rows.empty:\n",
    "            print(\"\\nRows in variant_df with matching PaperIds:\")\n",
    "            print(matching_variant_rows)\n",
    "        else:\n",
    "            print(\"\\nNo matching PaperIds found in variant_df.\")\n",
    "    else:\n",
    "        print(\"\\nNo matching PaperIds found in treatment_df.\")\n",
    "else:\n",
    "    print(f\"The column '{column_name}' does not exist in 'cancer_df'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abc7c66",
   "metadata": {},
   "source": [
    "# 2) Merging of datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97582f8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ensure all 'PaperId' columns are strings\n",
    "initial_df[\"PaperId\"] = initial_df[\"PaperId\"].astype(str)\n",
    "cancer_df[\"PaperId\"] = cancer_df[\"PaperId\"].astype(str)\n",
    "treatment_df[\"PaperId\"] = treatment_df[\"PaperId\"].astype(str)\n",
    "study_df[\"PaperId\"] = study_df[\"PaperId\"].astype(str)\n",
    "variant_df[\"PaperId\"] = variant_df[\"PaperId\"].astype(str)\n",
    "\n",
    "# Get the set of valid PaperIds from the variant_df\n",
    "valid_paper_ids = set(variant_df['PaperId'])\n",
    "\n",
    "# Function to filter a dataset by valid PaperIds\n",
    "def filter_by_variant_ids(df, df_name):\n",
    "    if \"PaperId\" not in df.columns:\n",
    "        print(f\"'PaperId' column not found in {df_name}. Skipping filtering.\")\n",
    "        return df\n",
    "\n",
    "    initial_count = len(df)\n",
    "    df_subset = df[df['PaperId'].isin(valid_paper_ids)]\n",
    "    final_count = len(df_subset)\n",
    "    \n",
    "    print(f\"{df_name}: Filtered from {initial_count:,} rows to {final_count:,} rows.\")\n",
    "    return df_subset\n",
    "\n",
    "# Apply filtering to each dataset\n",
    "initial_df_subset = filter_by_variant_ids(initial_df, \"Initial_df\")\n",
    "cancer_df_subset = filter_by_variant_ids(cancer_df, \"Cancer_df\")\n",
    "treatment_df_subset = filter_by_variant_ids(treatment_df, \"Treatment_df\")\n",
    "study_df_subset = filter_by_variant_ids(study_df, \"Study_df\")\n",
    "\n",
    "print(\"\\nFiltered Dataset Lengths:\")\n",
    "print(f\"-Initial dataset subset: {len(initial_df_subset):,}\")\n",
    "print(f\"-Cancer type dataset subset: {len(cancer_df_subset):,}\")\n",
    "print(f\"-Treatment dataset subset: {len(treatment_df_subset):,}\")\n",
    "print(f\"-Study design dataset subset: {len(study_df_subset):,}\")\n",
    "print(f\"-Variant dataset (reference): {len(variant_df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f579cbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert oncomine_genes to a set for fast lookups\n",
    "oncomine_genes_set = set(oncomine_genes[0])\n",
    "columns_to_remove = oncomine_genes_set.union({\"Sum_Gene_Mentions\"})\n",
    "cancer_cols_to_drop = [col for col in cancer_df_subset.columns if col in columns_to_remove or \"BioBERT\" in col]\n",
    "treatment_cols_to_drop = [col for col in treatment_df_subset.columns if col in columns_to_remove or \"BioBERT\" in col]\n",
    "\n",
    "# Drop columns from cancer_df_subset\n",
    "cancer_df_subset = cancer_df_subset.drop(columns=cancer_cols_to_drop, errors='ignore')\n",
    "\n",
    "# Drop columns from treatment_df_subset\n",
    "treatment_df_subset = treatment_df_subset.drop(columns=treatment_cols_to_drop, errors='ignore')\n",
    "print(f\"\\nRemoved from cancer_df_subset: {len(cancer_cols_to_drop)} columns\")\n",
    "print(f\"Removed from treatment_df_subset: {len(treatment_cols_to_drop)} columns\")\n",
    "print(\"Columns removed due to:\")\n",
    "print(\"- Oncomine Genes\")\n",
    "print(\"- Sum_Gene_Mentions\")\n",
    "print(\"- Columns containing 'BioBERT'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277a01b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.chdir(variantscape_directory)\n",
    "output_file = \"merged_detected_variant_df.csv\"\n",
    "initial_row_counts = {\n",
    "    \"initial_df\": len(initial_df_subset),\n",
    "    \"cancer_df_rem\": len(cancer_df_rem),\n",
    "    \"treatment_df_rem\": len(treatment_df_rem),\n",
    "    \"study_df_rem\": len(study_df_rem),\n",
    "    \"variant_df_rem\": len(variant_df_rem)\n",
    "}\n",
    "\n",
    "# Initialize the merged DataFrame with the initial dataset\n",
    "merged_df = initial_df_subset.copy()\n",
    "column_sources = {col: \"Initial df\" for col in initial_df_subset.columns}\n",
    "\n",
    "datasets_to_merge = [\n",
    "    (cancer_df_rem, \"Cancer\"),\n",
    "    (treatment_df_rem, \"Treatment\"),\n",
    "    (study_df_rem, \"Study\"),\n",
    "    (variant_df_rem, \"Variant\")\n",
    "]\n",
    "\n",
    "# Merge datasets with progress bar\n",
    "print(\"\\nStarting the merging process...\")\n",
    "for df, name in tqdm(datasets_to_merge, desc=\"Merging datasets\", unit=\"dataset\"):\n",
    "    if \"PaperId\" in df.columns:\n",
    "        for col in df.columns:\n",
    "            if col != \"PaperId\" and col not in column_sources:\n",
    "                column_sources[col] = name  \n",
    "        merged_df = pd.merge(merged_df, df, on=\"PaperId\", how=\"outer\", suffixes=(\"\", f\"_{name}\"))\n",
    "    else:\n",
    "        print(f\"'PaperId' column missing in {name}, skipping merge.\")\n",
    "\n",
    "# Ensure consistent formatting for numeric columns\n",
    "print(\"\\nConverting numeric columns...\")\n",
    "numeric_columns = [col for col in merged_df.columns if pd.api.types.is_numeric_dtype(merged_df[col])]\n",
    "\n",
    "for col in tqdm(numeric_columns, desc=\"Converting numeric columns\", unit=\"column\"):\n",
    "    merged_df[col] = pd.to_numeric(merged_df[col], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "# Add the source row at the top *after* merging\n",
    "source_row = pd.Series(column_sources, name='Source_Row')\n",
    "merged_df = pd.concat([pd.DataFrame([source_row]), merged_df], ignore_index=True)\n",
    "print(\"\\nFirst 5 rows of merged dataset:\")\n",
    "print(merged_df.head())\n",
    "\n",
    "# Check for Duplicate Column Names\n",
    "print(\"\\nChecking for duplicate column names...\")\n",
    "duplicate_columns = merged_df.columns[merged_df.columns.duplicated()].unique()\n",
    "\n",
    "if len(duplicate_columns) > 0:\n",
    "    print(f\"\\nDuplicate column names found ({len(duplicate_columns):,}):\")\n",
    "    for col in tqdm(duplicate_columns, desc=\"Reporting duplicates\", unit=\"column\"):\n",
    "        print(col)\n",
    "else:\n",
    "    print(\"No duplicate column names found.\")\n",
    "\n",
    "print(\"\\nMerging completed successfully.\")\n",
    "print(f\"Final dataset shape: {merged_df.shape[0]:,} rows, {merged_df.shape[1]:,} columns\")\n",
    "print(\"\\nFirst 10 rows of merged dataset:\")\n",
    "print(merged_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc9b326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print final results\n",
    "print(\"\\nMerging completed. Final dataset shape:\", f\"{merged_df.shape[0]:,} rows, {merged_df.shape[1]:,} columns\")\n",
    "print(\"Columns in final dataset:\", merged_df.columns.tolist())\n",
    "print(\"\\nFirst 10 rows of merged dataset:\")\n",
    "print(merged_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80ca115",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the columns relevant to all_conditions\n",
    "columns_of_interest = [\n",
    "    \"Sum_Gene_Mentions\", \"Cancer_Type_Sum\", \"Sum_treatments\", \"total_variant_count\", \"Study_design\"\n",
    "]\n",
    "print(\"\\nFirst 5 rows of merged_df with relevant columns:\")\n",
    "print(merged_df[columns_of_interest].head())\n",
    "print(merged_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b7379c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'Cancer_of_Primary_Unknown' with the actual column name\n",
    "column_name = \"cancer of unknown primary origin\"\n",
    "if column_name in merged_df.columns:\n",
    "    count_1 = (merged_df[column_name] == 1).sum() \n",
    "    count_0 = (merged_df[column_name] == 0).sum() \n",
    "\n",
    "    print(f\"Count of '1' in '{column_name}': {count_1}\")\n",
    "    print(f\"Count of '0' in '{column_name}': {count_0}\")\n",
    "else:\n",
    "    print(f\"The column '{column_name}' does not exist in the DataFrame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b26f1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset as CSV with progress bar (as it is huge, this takes about 1h)\n",
    "os.chdir(variantscape_directory)\n",
    "file_name = \"final_merged_df.csv\"\n",
    "chunk_size = 5_000  \n",
    "total_rows = len(merged_df)\n",
    "\n",
    "with open(file_name, \"w\", encoding=\"utf-8\", newline='') as f:\n",
    "    merged_df.head(0).to_csv(f, index=False) \n",
    "    \n",
    "    # Progress bar for writing rows in chunks\n",
    "    with tqdm(total=total_rows, desc=\"Saving CSV\", unit=\" rows\", bar_format=\"{l_bar}{bar} {n_fmt}/{total_fmt} [{elapsed}<{remaining}]\") as pbar:\n",
    "        for i in range(0, total_rows, chunk_size):\n",
    "            chunk = merged_df.iloc[i:i + chunk_size]\n",
    "            chunk.to_csv(f, index=False, header=False, mode='a')\n",
    "            pbar.update(len(chunk))\n",
    "\n",
    "print(\"\\nMerged dataframe successfully saved.\")\n",
    "print(f\"Total Rows: {total_rows:,}\")\n",
    "print(f\"Total Columns: {len(merged_df.columns):,}\")\n",
    "print(f\"File Saved: {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f30cd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure numeric types (without modifying anything)\n",
    "cols_to_check = [\"Sum_Gene_Mentions\", \"Cancer_Type_Sum\", \"Sum_treatments\", \"total_variant_count\"]\n",
    "\n",
    "# Count rows where value is >= 1 for each column, ignoring index 0\n",
    "counts_ge1 = (merged_df.iloc[1:][cols_to_check] >= 1).sum()\n",
    "\n",
    "# Count non-empty 'Study_design' values (not null, not '0', not numeric 0), excluding index 0\n",
    "study_design_non_empty = merged_df.iloc[1:][\"Study_design\"].notna() & (merged_df.iloc[1:][\"Study_design\"] != \"0\") & (merged_df.iloc[1:][\"Study_design\"] != 0)\n",
    "count_study_design = study_design_non_empty.sum()\n",
    "all_conditions = (\n",
    "    (merged_df.iloc[1:][\"Sum_Gene_Mentions\"] >= 1) &\n",
    "    (merged_df.iloc[1:][\"Cancer_Type_Sum\"] >= 1) &\n",
    "    (merged_df.iloc[1:][\"Sum_treatments\"] >= 1) &\n",
    "    (merged_df.iloc[1:][\"total_variant_count\"] >= 1) &\n",
    "    study_design_non_empty\n",
    ")\n",
    "count_all_conditions = all_conditions.sum()\n",
    "print(\"\\nNumber of rows with values ≥ 1 (ignoring index 0):\")\n",
    "for col, count in counts_ge1.items():\n",
    "    print(f\"- {col}: {count:,}\")\n",
    "\n",
    "print(f\"- Study_design (not empty or 0): {count_study_design:,}\")\n",
    "print(f\"- All conditions TRUE (ignoring index 0): {count_all_conditions:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad6d894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all conditions (ignoring index 0)\n",
    "all_conditions = (\n",
    "    (merged_df.iloc[1:][\"Sum_Gene_Mentions\"] >= 1) &\n",
    "    (merged_df.iloc[1:][\"Cancer_Type_Sum\"] >= 1) &\n",
    "    (merged_df.iloc[1:][\"Sum_treatments\"] >= 1) &\n",
    "    (merged_df.iloc[1:][\"total_variant_count\"] >= 1)\n",
    ")\n",
    "\n",
    "# Get the subset of rows where all conditions are true, ignoring index 0\n",
    "filtered_subset = merged_df.iloc[1:][all_conditions].copy()\n",
    "if 'PaperId' in filtered_subset.columns:\n",
    "    print(\"The column 'PaperId' is present in the filtered DataFrame.\")\n",
    "else:\n",
    "    print(\"The column 'PaperId' is not present in the filtered DataFrame.\")\n",
    "filtered_subset_with_index_0 = pd.concat([merged_df.iloc[[0]], filtered_subset])\n",
    "print(f\"\\nShape of the filtered DataFrame (with index 0): {filtered_subset_with_index_0.shape}\")\n",
    "print(filtered_subset_with_index_0.head())\n",
    "print(f\"Final shape of the filtered DataFrame: {filtered_subset_with_index_0.shape}\")\n",
    "filtered_subset_with_index_0.to_csv('filtered_subset_with_index_0.csv', index=False)\n",
    "print(\"Filtered DataFrame has been saved as 'filtered_subset_with_index_0.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2626784c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original dataset lengths for reference\n",
    "original_lengths = {\n",
    "    \"Initial Dataset\": len(initial_df),\n",
    "    \"Cancer Type Dataset\": len(cancer_df),\n",
    "    \"Study Design Dataset\": len(study_df),\n",
    "    \"Treatment Dataset\": len(treatment_df),\n",
    "    \"Variant Dataset\": len(variant_df)\n",
    "}\n",
    "\n",
    "print(\"\\nOriginal Dataset Lengths (with percentage of initial dataset):\")\n",
    "for name, length in original_lengths.items():\n",
    "    percentage = (length / original_lengths[\"Initial Dataset\"]) * 100\n",
    "    print(f\"- {name}: {length:,} ({percentage:.2f}%)\")\n",
    "\n",
    "percentage_all_conditions = (count_all_conditions / original_lengths[\"Initial Dataset\"]) * 100\n",
    "print(f\"\\n- All conditions TRUE: {count_all_conditions:,} ({percentage_all_conditions:.2f}%)\")\n",
    "\n",
    "labels = [\n",
    "    \"Initial Dataset\", \"Cancer Type Dataset\", \"Study Design Dataset\", \n",
    "    \"Treatment Dataset\", \"Variant Dataset\", \"All conditions TRUE\"\n",
    "]\n",
    "\n",
    "counts = list(original_lengths.values()) + [count_all_conditions]\n",
    "percentages = [(count / original_lengths[\"Initial Dataset\"]) * 100 for count in counts]\n",
    "dataset_lengths_df = pd.DataFrame({\n",
    "    \"Category\": labels,\n",
    "    \"Count\": counts,\n",
    "    \"Percentage\": percentages\n",
    "})\n",
    "\n",
    "csv_file_path = \"conditional_filterting_of_final_datasets_for_coassciaitions_analysis.csv\"\n",
    "dataset_lengths_df.to_csv(csv_file_path, index=False)\n",
    "print(f\"\\nData successfully saved to: {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33aaa971",
   "metadata": {},
   "source": [
    "## Create figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e191f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original dataset lengths for reference\n",
    "original_lengths = {\n",
    "    \"Initial Dataset\": len(initial_df),\n",
    "    \"Cancer Type Dataset\": len(cancer_df),\n",
    "    \"Treatment Dataset\": len(treatment_df),\n",
    "    \"Study Design Dataset\": len(study_df),\n",
    "    \"Variant Dataset\": len(variant_df)\n",
    "}\n",
    "\n",
    "# Counts from original datasets\n",
    "count_1 = original_lengths[\"Initial Dataset\"]\n",
    "count_2 = original_lengths[\"Cancer Type Dataset\"]\n",
    "count_3 = original_lengths[\"Study Design Dataset\"]\n",
    "count_4 = original_lengths[\"Treatment Dataset\"]\n",
    "count_5 = original_lengths[\"Variant Dataset\"]\n",
    "count_all_conditions = count_all_conditions\n",
    "\n",
    "labels = [\n",
    "    \"Extracted genes\",\n",
    "    \"Extracted cancer types\",\n",
    "    \"Classified study design\", \n",
    "    \"Extracted treatments\",   \n",
    "    \"Identified variants\",\n",
    "    \"All conditions TRUE\"\n",
    "]\n",
    "\n",
    "counts = [count_1, count_2, count_3, count_4, count_5, count_all_conditions]\n",
    "percentages = [(count / count_1 * 100) if count_1 > 0 else 0 for count in counts]\n",
    "\n",
    "# Plotting the bar chart\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bars = ax.bar(labels, counts, color=\"#0073e6\", edgecolor='black', alpha=0.95) \n",
    "\n",
    "for bar, count, pct in zip(bars, counts, percentages):\n",
    "    label = f\"{count:,} ({pct:.1f}%)\"\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2,\n",
    "            bar.get_height() + max(counts) * 0.01,\n",
    "            label,\n",
    "            ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "ax.set_title(\"Dataset Sizes and Condition Filtering (With Percentages)\", fontsize=16, fontweight='bold', pad=20)\n",
    "ax.set_ylabel(\"Number of Rows\", fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(range(len(labels)))\n",
    "ax.set_xticklabels(labels, fontsize=12, ha=\"right\", rotation=15)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbc79f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data inspection and export of filtered dataset\n",
    "\n",
    "first_row = filtered_subset_with_index_0.iloc[0]\n",
    "unique_values = first_row.unique()\n",
    "print(\"Unique values in the first row:\")\n",
    "print(unique_values)\n",
    "\n",
    "# Find columns where the value is NaN\n",
    "nan_columns = first_row[first_row.isna()].index.tolist()\n",
    "print(\"\\n\\nColumns with NaN in the first row:\")\n",
    "for col in nan_columns:\n",
    "    print(f\"- {col}\")\n",
    "\n",
    "columns_of_interest = [\"Sum_Gene_Mentions\", \"Cancer_Type_Sum\", \"Sum_treatments\", \"total_variant_count\"]\n",
    "print(\"Rows from selected columns:\")\n",
    "print(filtered_subset_with_index_0[columns_of_interest].head(10))\n",
    "\n",
    "# Save the updated and filtered dataset to CSV\n",
    "merged_detected_variant_df=filtered_subset_with_index_0.copy()\n",
    "output_file = \"merged_detected_variant_df.csv\"\n",
    "merged_detected_variant_df.to_csv(output_file, index=False)\n",
    "print(f\"Filtered dataset saved as '{output_file}'\")\n",
    "\n",
    "rows_before_col_drop=len(merged_detected_variant_df)\n",
    "col_before_col_drop=len(merged_detected_variant_df.columns)\n",
    "print(f\"Total rows: {rows_before_col_drop:,}\")\n",
    "print(f\"Total columns: {col_before_col_drop:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d021879",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop the COLUMNS as well where the variant sum is ZERO!\n",
    "\n",
    "print(merged_detected_variant_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55891b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load or use existing dataframe\n",
    "if 'merged_detected_variant_df' not in globals():\n",
    "    merged_detected_variant_df = pd.read_csv(\"merged_detected_variant_df.csv\")\n",
    "\n",
    "# Drop 'Unnamed: 0' if it exists — this was likely the index when saved\n",
    "if \"Unnamed: 0\" in merged_detected_variant_df.columns:\n",
    "    print(\"Dropping 'Unnamed: 0' — it was likely saved as index.\")\n",
    "    merged_detected_variant_df = merged_detected_variant_df.drop(columns=[\"Unnamed: 0\"])\n",
    "\n",
    "# Copy for processing\n",
    "rem_col_variant_df = merged_detected_variant_df.copy()\n",
    "\n",
    "# Manually defined columns to ignore\n",
    "ignore_columns = {\n",
    "    'PaperId', 'PaperTitle', 'Citations', 'CoFoS', 'Authors', 'Abstract', \n",
    "    'Language', 'PubYear', 'PubDate', 'BioBERT', 'Sum_Gene_Mentions', \n",
    "    'Extracted_Cancer_Terms_old', 'Extracted_Cancer_Terms','Cancer_Type_Sum',\n",
    "    'Mapped_Cancer_Terms', 'Unmatched_Cancer_Terms', 'Remapped_Cancer_Terms', \n",
    "    'Final_Mapped_Cancer_Terms', 'Treatment_matches', 'Sum_treatments', 'Study_design',\n",
    "    'total_variant_count','LLM_Prompt', 'LLM_Response', 'Cleaned_Variant_Gene_Pairs'\n",
    "}\n",
    "\n",
    "# Additional prefixes to ignore\n",
    "ignore_prefixes = ('Gene_', 'Variant_', 'LLM_Prompt', 'LLM_Response')\n",
    "\n",
    "# Add any column starting with those prefixes to the ignore list\n",
    "for col in rem_col_variant_df.columns:\n",
    "    if col.startswith(ignore_prefixes):\n",
    "        ignore_columns.add(col)\n",
    "\n",
    "# Identify candidate binary columns (not ignored)\n",
    "candidate_cols = [col for col in rem_col_variant_df.columns if col not in ignore_columns]\n",
    "\n",
    "# Exclude row 0 (metadata row)\n",
    "data_for_analysis = rem_col_variant_df.iloc[1:].copy()\n",
    "\n",
    "# Convert to numeric and normalize values\n",
    "data_for_analysis[candidate_cols] = data_for_analysis[candidate_cols].apply(pd.to_numeric, errors='coerce')\n",
    "data_for_analysis[candidate_cols] = data_for_analysis[candidate_cols].astype('Int64')\n",
    "non_binary_columns = []\n",
    "print(\"Checking for non-binary values (0/1 only)...\")\n",
    "for col in tqdm(candidate_cols, desc=\"Validating columns\", unit=\"col\"):\n",
    "    unique_vals = data_for_analysis[col].dropna().unique()\n",
    "    if not all(val in [0, 1] for val in unique_vals):\n",
    "        non_binary_columns.append((col, unique_vals.tolist()))\n",
    "if non_binary_columns:\n",
    "    print(f\"{len(non_binary_columns)} column(s) (excluding ignored and row 0) contain values other than 0 or 1.\")\n",
    "    print(\"First 5:\")\n",
    "    for col, vals in non_binary_columns[:5]:\n",
    "        print(f\"- {col}: {vals}\")\n",
    "else:\n",
    "    print(\"All candidate columns contain only 0s and 1s (excluding row 0).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cde633d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all binary-valid columns by removing the ones with non-binary values\n",
    "binary_columns = [col for col in candidate_cols if col not in [c[0] for c in non_binary_columns]]\n",
    "print(f\"\\nBinary columns (total: {len(binary_columns)}):\\n\")\n",
    "for col in binary_columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4c9722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 0 columns\n",
    "# Only proceed if all binary columns are valid\n",
    "if not non_binary_columns:\n",
    "    steps = [\n",
    "        \"Check which binary columns are all-zero (excluding row 0)\",\n",
    "        \"Print summary\",\n",
    "        \"Drop all-zero binary columns\",\n",
    "        \"Save cleaned DataFrame\"\n",
    "    ]\n",
    "    pbar = tqdm(total=len(steps), desc=\"Processing\", bar_format='{l_bar}{bar} | {n_fmt}/{total_fmt} [{elapsed}]')\n",
    "    col_sums = data_for_analysis[candidate_cols].sum()\n",
    "    cols_all_zero = col_sums[col_sums == 0].index.tolist()\n",
    "    cols_with_ones = col_sums[col_sums > 0].index.tolist()\n",
    "    pbar.update(1)\n",
    "    total = len(candidate_cols)\n",
    "    num_zero = len(cols_all_zero)\n",
    "    num_one = len(cols_with_ones)\n",
    "    pct_one = (num_one / total * 100) if total > 0 else 0\n",
    "    pct_zero = (num_zero / total * 100) if total > 0 else 0\n",
    "\n",
    "    print(\"\\nColumn Summary (excluding metadata and row 0):\")\n",
    "    print(f\"- Columns with at least one '1': {num_one:,} ({pct_one:.1f}%)\")\n",
    "    print(f\"- Columns with only '0's:         {num_zero:,} ({pct_zero:.1f}%)\")\n",
    "    print(f\"- Dropped columns: {num_zero:,}\")\n",
    "    print(\"  First 10 dropped columns:\", cols_all_zero[:10])\n",
    "    with open(\"dropped_zero_columns.txt\", \"w\") as f:\n",
    "        for col in cols_all_zero:\n",
    "            f.write(col + \"\\n\")\n",
    "    print(\"Full list of dropped columns saved to 'dropped_zero_columns.txt'\")\n",
    "    pbar.update(1)\n",
    "\n",
    "    small_rem_col_variant_df = rem_col_variant_df.drop(columns=cols_all_zero)\n",
    "    pbar.update(1)\n",
    "    output_file = \"small_rem_col_variant_df.csv\"\n",
    "    small_rem_col_variant_df.to_csv(output_file, index=False)\n",
    "    print(f\"\\nFiltered dataset saved as '{output_file}'\")\n",
    "    pbar.update(1)\n",
    "\n",
    "    pbar.close()\n",
    "else:\n",
    "    print(\"Non-binary columns detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfa9b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get shape before column dropping\n",
    "rows_before_col_drop, col_before_col_drop = rem_col_variant_df.shape\n",
    "# Get shape after column dropping\n",
    "rows_after_col_drop, col_after_col_drop = small_rem_col_variant_df.shape\n",
    "print(\"\\n=== Final Dataset Summary ===\")\n",
    "print(f\"{'Metric':<30} {'Before':>15} {'After':>15}\")\n",
    "print(f\"{'Total rows':<30} {rows_before_col_drop:>15,} {rows_after_col_drop:>15,}\")\n",
    "print(f\"{'Total columns':<30} {col_before_col_drop:>15,} {col_after_col_drop:>15,}\")\n",
    "print(f\"{'Total columns dropped':<30} {'':>15} {col_before_col_drop - col_after_col_drop:>15,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc753afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns starting with Variant_ or Gene_\n",
    "variant_gene_cols = [col for col in small_rem_col_variant_df.columns if col.startswith((\"Variant_\", \"Gene_\"))]\n",
    "final_variant_df_for_analysis = small_rem_col_variant_df.drop(columns=variant_gene_cols)\n",
    "\n",
    "ordered_front = [col for col in final_variant_df_for_analysis.columns if col in ignore_columns]\n",
    "remaining_cols = [col for col in final_variant_df_for_analysis.columns if col not in ordered_front]\n",
    "final_variant_df_for_analysis = final_variant_df_for_analysis[ordered_front + remaining_cols]\n",
    "final_variant_df_for_analysis.to_csv(\"final_variant_df_for_analysis.csv\", index=False)\n",
    "original_col_count = small_rem_col_variant_df.shape[1]\n",
    "dropped_col_count = len(variant_gene_cols)\n",
    "remaining_col_count = final_variant_df_for_analysis.shape[1]\n",
    "\n",
    "print(\"\\n=== Final Variant Dataset Shape ===\")\n",
    "print(f\"{'Metric':<30} {'Value':>20}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Total rows':<30} {final_variant_df_for_analysis.shape[0]:>20,}\")\n",
    "print(f\"{'Original columns':<30} {original_col_count:>20,}\")\n",
    "print(f\"{'Columns dropped':<30} {dropped_col_count:>20,}\")\n",
    "print(f\"{'Remaining columns':<30} {remaining_col_count:>20,}\")\n",
    "\n",
    "if original_col_count - dropped_col_count == remaining_col_count:\n",
    "    print(\"Column count check: MATCH\")\n",
    "else:\n",
    "    print(\"Column count check: MISMATCH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6241aec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(small_rem_col_variant_df)\n",
    "# print(final_variant_df_for_analysis.columns.tolist())\n",
    "print(\"\\n=== Columns in final_variant_df_for_analysis ===\")\n",
    "for col in final_variant_df_for_analysis.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8cd198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset to use: final_variant_df_for_analysis.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
