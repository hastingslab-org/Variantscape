{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cf9ecd9",
   "metadata": {},
   "source": [
    "# Create network graph and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24329d27",
   "metadata": {},
   "source": [
    "# 1) Set up libraries and work directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70822ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## UPDATED\n",
    "\n",
    "# Set up libraries\n",
    "import os\n",
    "import ast\n",
    "import time \n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "from tqdm import tqdm\n",
    "from rich.text import Text\n",
    "from rich.console import Console\n",
    "from pyvis.network import Network\n",
    "\n",
    "# Work directory\n",
    "input_directory = \"INPUT_DIRECTORY\"\n",
    "output_directory = \"OUTPUT DIRECTORY\"\n",
    "LLM_variant_directoy = \"LLM_VARIANT_DIRECTORY\"\n",
    "variantscape_directory = \"VARIANTSCAPE_DIRECTORY\"\n",
    "variantscape_llm_coas_directory = \"VARIANTSCAPE_LLM_COAS_DIRECTORY\"\n",
    "\n",
    "# Load the metadata and variant dataset\n",
    "os.chdir(variantscape_directory)\n",
    "metadata_mapping = pd.read_csv(os.path.join(variantscape_directory, \"metadata_mapping_transposed.csv\"), low_memory=False)\n",
    "variant_analysis_df = pd.read_csv(os.path.join(variantscape_directory, \"cleaned_df_v4.csv\"), low_memory=False)\n",
    "print(metadata_mapping.head(5))\n",
    "\n",
    "# Cancer synonyms\n",
    "CIVIC_cancer_synonyms_df = pd.read_csv(output_directory + \"/Network_cancer_synonyms.csv\")\n",
    "print(\"\\n\\n\\nCancer synonym dataset loaded successfully!\")\n",
    "print(\"Length of dataset\", len(CIVIC_cancer_synonyms_df))\n",
    "#print(CIVIC_cancer_synonyms_df)\n",
    "\n",
    "os.chdir(variantscape_LLM_coas_directory)\n",
    "df_consensus = pd.read_csv(\"final_variant_treatment_consensus.csv\")\n",
    "os.chdir(variantscape_directory)\n",
    "print(\"\\n\\n\")\n",
    "#print(df_consensus.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b6854f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Explore variant dictionary\n",
    "# Filter metadata_mapping for entities where Category == 'Variant'\n",
    "variant_entities = metadata_mapping[metadata_mapping['Category'] == 'Variant']['Entity'].tolist()\n",
    "print(f\"\\nTotal unique entities with Category == 'Variant': {len(variant_entities)}\")\n",
    "print(\"ScoreLabel List for Entity = Variant:\\n\")\n",
    "for idx, entity in enumerate(variant_entities, 1):\n",
    "    print(f\"{idx}. {entity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51997f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cancer dictionary\n",
    "# Filter metadata_mapping for entities where Category == 'Cancer'\n",
    "cancer_entities = metadata_mapping[metadata_mapping['Category'] == 'Cancer']['Entity'].tolist()\n",
    "print(f\"\\nTotal unique entities with Category == 'Cancer': {len(cancer_entities)}\")\n",
    "print(\"ScoreLabel List for Entity = Cancer:\\n\")\n",
    "for idx, entity in enumerate(cancer_entities, 1):\n",
    "    print(f\"{idx}. {entity.capitalize()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7696e83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treatment dictionary\n",
    "# Filter for Treatment entities\n",
    "treatment_entities = metadata_mapping[metadata_mapping['Category'] == 'Treatment']['Entity'].tolist()\n",
    "print(f\"\\n\\033[1mTotal unique entities with Category == 'Treatment': {len(treatment_entities)}\\033[0m\")\n",
    "print(\"\\n\\033[1mScoreLabel List for Entity = Treatment:\\033[0m\")\n",
    "for idx, entity in enumerate(treatment_entities, 1):\n",
    "    print(f\"{idx}. {entity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f90e6bd",
   "metadata": {},
   "source": [
    "# ========================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440afa8b",
   "metadata": {},
   "source": [
    "# 2) Network graph analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5fbc47",
   "metadata": {},
   "source": [
    "## Create network graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080c7503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean variant_analysis_df by removing non-entity columns\n",
    "# Ingnore non binary columns\n",
    "entity_columns = [col for col in variant_analysis_df.columns if col not in ['PaperId', 'Study_design', 'Abstract', 'Study_weight', 'PaperTitle']]\n",
    "\n",
    "# Create network graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes for each entity based on metadata_mapping dictionary\n",
    "for col in entity_columns:\n",
    "    category = metadata_mapping.loc[metadata_mapping['Entity'] == col, 'Category'].values[0]\n",
    "    G.add_node(col, category=category)\n",
    "\n",
    "# Add edges based on co-occurrence\n",
    "for idx, row in tqdm(variant_analysis_df.iterrows(), total=variant_analysis_df.shape[0], desc=\"Adding edges\"):\n",
    "    present_entities = row[entity_columns] == 1 \n",
    "    present_columns = present_entities[present_entities].index\n",
    "    for col1 in present_columns:\n",
    "        for col2 in present_columns:\n",
    "            if col1 != col2:\n",
    "                if G.has_edge(col1, col2):\n",
    "                    G[col1][col2]['weight'] += 1 \n",
    "                else:\n",
    "                    G.add_edge(col1, col2, weight=1)\n",
    "\n",
    "# Exmplore network analysis\n",
    "print(\"Number of nodes:\", len(G.nodes))\n",
    "print(\"Number of edges:\", len(G.edges))\n",
    "print(\"Network successfully created!\")\n",
    "\n",
    "# Save the network\n",
    "nx.write_gml(G, 'network_graph.gml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4c88b9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Verify the network analysis\n",
    "G = nx.read_gml('network_graph.gml')\n",
    "\n",
    "# Example query: Find the neighbors of a particular variant of interest\n",
    "variant_of_interest = \"s768i_EGFR\"\n",
    "variant_neighbors = set(G.neighbors(variant_of_interest))\n",
    "\n",
    "# Find treatments and cancers associated with the variant\n",
    "treatments = []\n",
    "cancers = []\n",
    "\n",
    "for node in variant_neighbors:\n",
    "    if G.nodes[node]['category'] == 'Treatment':\n",
    "        treatments.append(node)\n",
    "    elif G.nodes[node]['category'] == 'Cancer':\n",
    "        cancers.append(node)\n",
    "        \n",
    "print(f\"Treatments associated with variant '{variant_of_interest}':\")\n",
    "print(treatments)\n",
    "print(f\"\\nCancers associated with variant '{variant_of_interest}':\")\n",
    "print(cancers)\n",
    "\n",
    "# Find the top 5 most connected nodes (by degree centrality)\n",
    "centrality = nx.degree_centrality(G)\n",
    "sorted_centrality = sorted(centrality.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nTop 5 most connected nodes (based on degree centrality):\")\n",
    "for node, score in sorted_centrality[:5]:\n",
    "    print(f\"{node}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2a4371",
   "metadata": {},
   "source": [
    "## Create weighted network graph based on study design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39590bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create weighted network graph based on study design\n",
    "\n",
    "# Study‑design weights\n",
    "study_design_weights = {\n",
    "    'Systematic review study':       1.0,\n",
    "    'Clinical study':                1.0,\n",
    "    'Observational/RWE study':       0.9,\n",
    "    'Case report study':             0.9,\n",
    "    'In vivo/Animal study':          0.8,\n",
    "    'In vitro study':                0.7,\n",
    "    'In silico study':               0.6,\n",
    "    'Undefined':                     0.1,\n",
    "    'Other':                         0.1,\n",
    "}\n",
    "\n",
    "# Entity columns\n",
    "non_entity_cols = ['PaperId', 'Study_design', 'Abstract', 'Study_weight', 'PaperTitle']\n",
    "entity_columns = [c for c in variant_analysis_df.columns if c not in non_entity_cols]\n",
    "\n",
    "# Initialize two graphs\n",
    "G   = nx.Graph()  # unweighted (increments by 1)\n",
    "G_w = nx.Graph()  # weighted by evidence level\n",
    "\n",
    "# 1) Add nodes to both\n",
    "for col in entity_columns:\n",
    "    cat = metadata_mapping.loc[\n",
    "        metadata_mapping['Entity'] == col, 'Category'\n",
    "    ].values[0]\n",
    "    G.add_node(col,   category=cat)\n",
    "    G_w.add_node(col, category=cat)\n",
    "\n",
    "# 2) Add edges\n",
    "for _, row in tqdm(variant_analysis_df.iterrows(), total=len(variant_analysis_df), desc=\"Building graphs\"):\n",
    "    ents = row[entity_columns][row[entity_columns] == 1].index.tolist()\n",
    "    design = str(row['Study_design']).strip()\n",
    "    w      = study_design_weights.get(design, 0.5)\n",
    "\n",
    "    # iterate unique pairs\n",
    "    for i, e1 in enumerate(ents):\n",
    "        for e2 in ents[i+1:]:\n",
    "            # --- unweighted: +1 per co‑occurrence ---\n",
    "            if G.has_edge(e1, e2):\n",
    "                G[e1][e2]['weight'] += 1\n",
    "            else:\n",
    "                G.add_edge(e1, e2, weight=1)\n",
    "\n",
    "            # --- weighted: +w per co‑occurrence ---\n",
    "            if G_w.has_edge(e1, e2):\n",
    "                G_w[e1][e2]['weight'] += w\n",
    "            else:\n",
    "                G_w.add_edge(e1, e2, weight=w)\n",
    "\n",
    "# 3) Compare basic metrics\n",
    "print(\"=== Unweighted graph\")\n",
    "print(\" Nodes:\", G.number_of_nodes(), \"Edges:\", G.number_of_edges())\n",
    "deg_unw = nx.degree_centrality(G)\n",
    "top_unw = sorted(deg_unw.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "print(\" Top 5 nodes by degree centrality:\", top_unw)\n",
    "print(\"\\n=== Weighted graph\")\n",
    "print(\" Nodes:\", G_w.number_of_nodes(), \"Edges:\", G_w.number_of_edges())\n",
    "deg_w = nx.degree_centrality(G_w)\n",
    "top_w = sorted(deg_w.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "print(\" Top 5 nodes by degree centrality:\", top_w)\n",
    "\n",
    "# Compare edge‐weight distributions\n",
    "weights_unw = np.array([d['weight'] for _, _, d in G.edges(data=True)])\n",
    "weights_w   = np.array([d['weight'] for _, _, d in G_w.edges(data=True)])\n",
    "\n",
    "print(\"Edge weights (unweighted): mean=%.2f, std=%.2f\" % (weights_unw.mean(), weights_unw.std()))\n",
    "print(\"Edge weights (weighted):   mean=%.2f, std=%.2f\" % (weights_w.mean(),   weights_w.std()))\n",
    "print(\"Number of edges re‑weighted:\", np.sum(weights_w != weights_unw))\n",
    "\n",
    "# Correlate the two weight vectors\n",
    "corr = np.corrcoef(weights_unw, weights_w)[0,1]\n",
    "print(\"Pearson corr between unweighted/weighted edge‐weights: %.3f\" % corr)\n",
    "\n",
    "# Compare degree centrality shifts per node\n",
    "dc_unw = nx.degree_centrality(G)\n",
    "dc_w   = nx.degree_centrality(G_w)\n",
    "delta  = {n: dc_w[n] - dc_unw[n] for n in G.nodes()}\n",
    "nx.write_gml(G_w, 'network_graph_weighted.gml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b2f542",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### NEW CANCER SYNONYM MATCHING\n",
    "\n",
    "# Copy the graph\n",
    "G = G_w.copy()\n",
    "\n",
    "CIVIC_cancer_synonyms_df = pd.read_csv(output_directory + \"/Network_cancer_synonyms.csv\")\n",
    "print(\"\\n\\n\\nCancer synonym dataset loaded successfully!\")\n",
    "print(\"Length of dataset\", len(CIVIC_cancer_synonyms_df))\n",
    "\n",
    "# Load and normalize the synonym dataframe\n",
    "df_syn = CIVIC_cancer_synonyms_df.copy()\n",
    "df_syn[\"name\"] = df_syn[\"name\"].str.strip().str.lower()\n",
    "\n",
    "def safe_parse_synonyms(x):\n",
    "    if isinstance(x, str):\n",
    "        try:\n",
    "            parsed = ast.literal_eval(x)\n",
    "            if isinstance(parsed, list):\n",
    "                return [s.strip().lower() for s in parsed if isinstance(s, str)]\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: could not parse synonym entry '{x}': {e}\")\n",
    "    return []\n",
    "\n",
    "df_syn[\"synonyms\"] = df_syn[\"synonyms\"].apply(safe_parse_synonyms)\n",
    "\n",
    "# Build the alias map: synonym → canonical cancer name\n",
    "cancer_alias_map = {}\n",
    "for _, row in df_syn.iterrows():\n",
    "    canonical = row[\"name\"]\n",
    "    for synonym in row[\"synonyms\"]:\n",
    "        cancer_alias_map[synonym] = canonical\n",
    "\n",
    "print(\"Synonym map contains\", len(cancer_alias_map), \"entries.\")\n",
    "\n",
    "# Extract cancer-type nodes from the graph\n",
    "cancer_nodes_in_graph = set()\n",
    "for node in G.nodes():\n",
    "    if G.nodes[node].get(\"category\", \"\").lower() == \"cancer\":\n",
    "        cancer_nodes_in_graph.add(node.lower().strip())\n",
    "\n",
    "print(\"Found\", len(cancer_nodes_in_graph), \"cancer-type nodes in the graph.\")\n",
    "\n",
    "# Report unmatched cancer names\n",
    "canonical_names_in_synonym_file = set(df_syn[\"name\"])\n",
    "unmatched_cancers = sorted([\n",
    "    c for c in cancer_nodes_in_graph\n",
    "    if c not in cancer_alias_map and c not in canonical_names_in_synonym_file\n",
    "])\n",
    "\n",
    "print(len(unmatched_cancers), \"cancer names in the graph are not found in the synonym map or canonical names.\")\n",
    "if unmatched_cancers:\n",
    "    print(\"Examples of unmatched cancer names:\")\n",
    "    for c in unmatched_cancers[:10]:\n",
    "        print(\" -\", c)\n",
    "\n",
    "# Print synonym to canonical name mappings\n",
    "# print(\"\\nFull synonym → canonical cancer name list:\")\n",
    "# for synonym, canonical in sorted(cancer_alias_map.items()):\n",
    "#    print(f\" - {synonym} becomes {canonical}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eadf415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load and clean the variant synonym file ===\n",
    "variant_syn_file = os.path.join(LLM_variant_directoy, \"CIVIC_ClinVar_merged.csv\")\n",
    "relevant_cols = [\n",
    "    'Variant Name', 'Gene Name', 'rsID', 'ClinVar_ID', 'HGVS_Notation', 'Variant_Description'\n",
    "]\n",
    "variant_syn_df = pd.read_csv(variant_syn_file, usecols=relevant_cols)\n",
    "\n",
    "# Normalize base columns\n",
    "for col in relevant_cols:\n",
    "    variant_syn_df[col] = variant_syn_df[col].astype(str).str.strip()\n",
    "\n",
    "# Helper to combine fields into format \"value_GENE\"\n",
    "def combine_fields(lower_val, upper_gene):\n",
    "    if lower_val.lower() != \"nan\" and upper_gene.upper() != \"NAN\" and lower_val != \"\" and upper_gene != \"\":\n",
    "        return f\"{lower_val.lower()}_{upper_gene.upper()}\"\n",
    "    else:\n",
    "        return \"NaN\"\n",
    "\n",
    "# Create new identifier columns\n",
    "variant_syn_df[\"variant_gene\"] = variant_syn_df.apply(\n",
    "    lambda row: combine_fields(row[\"Variant Name\"], row[\"Gene Name\"]), axis=1\n",
    ")\n",
    "variant_syn_df[\"rsid_gene\"] = variant_syn_df.apply(\n",
    "    lambda row: combine_fields(row[\"rsID\"], row[\"Gene Name\"]), axis=1\n",
    ")\n",
    "variant_syn_df[\"clinvarid_gene\"] = variant_syn_df.apply(\n",
    "    lambda row: combine_fields(row[\"ClinVar_ID\"], row[\"Gene Name\"]), axis=1\n",
    ")\n",
    "variant_syn_df[\"hgvs_gene\"] = variant_syn_df.apply(\n",
    "    lambda row: combine_fields(row[\"HGVS_Notation\"], row[\"Gene Name\"]), axis=1\n",
    ")\n",
    "variant_syn_df[\"vardesc_gene\"] = variant_syn_df.apply(\n",
    "    lambda row: combine_fields(row[\"Variant_Description\"], row[\"Gene Name\"]), axis=1\n",
    ")\n",
    "\n",
    "# Convert DataFrame to list-of-dicts for compatibility\n",
    "variant_lookup_report = variant_syn_df.to_dict(orient=\"records\")\n",
    "\n",
    "# === Build variant_alias_map ===\n",
    "variant_alias_map = {}\n",
    "lookup_cols = [\"variant_gene\", \"rsid_gene\", \"clinvarid_gene\", \"hgvs_gene\", \"vardesc_gene\"]\n",
    "\n",
    "def normalize_variant_gene(entity):\n",
    "    if \"_\" in entity:\n",
    "        variant, gene = entity.split(\"_\", 1)\n",
    "        return f\"{variant.lower()}_{gene.upper()}\"\n",
    "    return entity.lower()\n",
    "\n",
    "# Collect all unique identifiers for matching\n",
    "variant_entities = set()\n",
    "for row in variant_lookup_report:\n",
    "    for col in lookup_cols:\n",
    "        val = row[col]\n",
    "        if isinstance(val, str) and val.lower() != \"nan\":\n",
    "            variant_entities.add(val)\n",
    "\n",
    "# Build the alias map\n",
    "for raw_entity in tqdm(variant_entities, desc=\"Building variant synonym map\"):\n",
    "    entity = normalize_variant_gene(raw_entity)\n",
    "    matching_rows = variant_syn_df[\n",
    "        variant_syn_df[lookup_cols].apply(lambda row: entity in row.values, axis=1)\n",
    "    ]\n",
    "    synonyms = set()\n",
    "    for _, row in matching_rows.iterrows():\n",
    "        for col in lookup_cols:\n",
    "            val = row[col]\n",
    "            if isinstance(val, str) and val != entity and val.lower() != \"nan\":\n",
    "                synonyms.add(val)\n",
    "    if synonyms:\n",
    "        variant_alias_map[entity] = synonyms\n",
    "\n",
    "print(f\"\\nBuilt variant alias map with {len(variant_alias_map)} entries.\")\n",
    "\n",
    "# Preview examples\n",
    "print(\"\\nExample entries (first 10):\")\n",
    "for canonical, syns in list(variant_alias_map.items())[:10]:\n",
    "    print(f\"{canonical} --> {sorted(syns)}\")\n",
    "\n",
    "# Show unmatched entities\n",
    "all_canonical = {normalize_variant_gene(e) for e in variant_entities}\n",
    "matched_canonical = set(variant_alias_map.keys())\n",
    "unmatched_canonical = sorted(all_canonical - matched_canonical)\n",
    "\n",
    "print(f\"\\n{len(unmatched_canonical)} canonical variants had no synonyms found.\")\n",
    "if unmatched_canonical:\n",
    "    print(\"Examples:\")\n",
    "    for e in unmatched_canonical[:10]:\n",
    "        print(e)\n",
    "\n",
    "# === Example lookup ===\n",
    "test_input = \"rs121913227_BRAF\"\n",
    "canonical = next((k for k, v in variant_alias_map.items() if test_input in v), None)\n",
    "print(\"\\nCanonical variant for test input:\", canonical)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0181b013",
   "metadata": {},
   "source": [
    "# ================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c6b1f8",
   "metadata": {},
   "source": [
    "# 3) Query the network to find associations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90aaf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## UPDATED\n",
    "\n",
    "\n",
    "# === Load the weighted network graph ===\n",
    "try:\n",
    "    G_w\n",
    "    print(\"Weighted graph already loaded. Proceeding with analysis.\")\n",
    "except NameError:\n",
    "    print(\"Loading the weighted network graph...\")\n",
    "    path = os.path.join(variantscape_directory, \"network_graph_weighted.gml\")\n",
    "    G_w = nx.read_gml(path)\n",
    "\n",
    "G = G_w.copy()\n",
    "\n",
    "# === Load consensus dataframe ===\n",
    "try:\n",
    "    df_consensus\n",
    "    print(\"df_consensus already loaded. Proceeding with analysis.\")\n",
    "except NameError:\n",
    "    print(\"Loading the consensus file...\")\n",
    "    df_consensus_path = os.path.join(variantscape_LLM_coas_directory, 'final_variant_treatment_consensus.csv')\n",
    "    df_consensus = pd.read_csv(df_consensus_path)\n",
    "\n",
    "# === Build variant_alias_map from variant_lookup_report ===\n",
    "variant_alias_map = {}\n",
    "\n",
    "for row in variant_lookup_report:\n",
    "    canonical = row.get(\"variant_gene\", \"NaN\")\n",
    "    if canonical == \"NaN\":\n",
    "        continue\n",
    "\n",
    "    if \"_\" in canonical:\n",
    "        _, gene = canonical.split(\"_\", 1)\n",
    "    else:\n",
    "        gene = None\n",
    "\n",
    "    for key in [\"variant_gene\", \"rsID\", \"ClinVar_ID\", \"HGVS_Notation\", \"Variant_Description\"]:\n",
    "        val = row.get(key)\n",
    "        if pd.notna(val) and val != \"NaN\":\n",
    "            base = str(val).strip().lower()\n",
    "\n",
    "            # 1. base variant becomes canonical variant as in network\n",
    "            if base not in variant_alias_map:\n",
    "                variant_alias_map[base] = canonical\n",
    "\n",
    "            # 2. base_GENE ecomes canonical gene as in network\n",
    "            if gene:\n",
    "                composed = f\"{base}_{gene.upper()}\"\n",
    "                if composed not in variant_alias_map:\n",
    "                    variant_alias_map[composed] = canonical\n",
    "\n",
    "                    \n",
    "################################################################################################ \n",
    "################################################################################################ \n",
    "################################################################################################ \n",
    "                    \n",
    "# === Define user input ===\n",
    "user_input_cancer = \"NSCLC\"\n",
    "\n",
    "\n",
    "#user_input_variant = \"rs121913227_BRAF\" #this is a synonym for v600r_BRAF\n",
    "#user_input_variant = \"rs113488022_BRAF\" #his is a synonym for v600e_BRAF\n",
    "\n",
    "############## Variants of interest for publication ##############\n",
    "user_input_variant = 'l858r_EGFR' #as durggable usecase\n",
    "#user_input_variant = 't790m_EGFR' #as resistant usecase\n",
    "\n",
    "######################## Rare variants #############################\n",
    "#user_input_variant = 'g469v_BRAF'\n",
    "#user_input_variant = 's768i_EGFR'\n",
    "#user_input_variant = 'l861q_EGFR'\n",
    "#user_input_variant = 'l747p_EGFR' # no associations in the network\n",
    "\n",
    "\n",
    "\n",
    "################################################################################################ \n",
    "################################################################################################ \n",
    "################################################################################################ \n",
    "\n",
    "\n",
    "# === Normalize cancer input ===\n",
    "exceptions = {\"cholangiocarcinoma\", \"adenocarcinoma\"}\n",
    "clean_cancer = user_input_cancer.strip().lower()\n",
    "\n",
    "if clean_cancer not in exceptions:\n",
    "    for word in [\"tumor\", \"tumour\", \"carcinoma\", \"neoplasm\"]:\n",
    "        clean_cancer = clean_cancer.replace(word, \"cancer\")\n",
    "\n",
    "cancer_of_interest = cancer_alias_map.get(clean_cancer, clean_cancer)\n",
    "display_cancer_name = user_input_cancer\n",
    "\n",
    "# === Normalize and map variant input ===\n",
    "def normalize_variant_gene(entity):\n",
    "    if \"_\" in entity:\n",
    "        variant, gene = entity.split(\"_\", 1)\n",
    "        return f\"{variant.lower()}_{gene.upper()}\"\n",
    "    return entity.lower()\n",
    "lookup_key = normalize_variant_gene(user_input_variant.strip())\n",
    "variant_of_interest = variant_alias_map.get(lookup_key)\n",
    "\n",
    "if not variant_of_interest:\n",
    "    variant_of_interest = next(\n",
    "        (v for k, v in variant_alias_map.items() if lookup_key == k.lower()), \n",
    "        user_input_variant\n",
    "    )\n",
    "display_variant_name = user_input_variant\n",
    "\n",
    "\n",
    "# === Print resolution ===\n",
    "print(\"\\n=== User Input Resolution ===\")\n",
    "if clean_cancer != cancer_of_interest:\n",
    "    print(f\"Cancer input '{user_input_cancer}' normalized to '{clean_cancer}', mapped to: '{cancer_of_interest}'\")\n",
    "else:\n",
    "    print(f\"Cancer input '{user_input_cancer}' used directly as: '{cancer_of_interest}'\")\n",
    "\n",
    "if lookup_key != variant_of_interest:\n",
    "    print(f\"Variant input '{user_input_variant}' normalized to '{lookup_key}', mapped to: '{variant_of_interest}'\")\n",
    "else:\n",
    "    print(f\"Variant input '{user_input_variant}' used directly as: '{variant_of_interest}'\")\n",
    "\n",
    "print(\"\\n=== Final Entities for Query ===\")\n",
    "print(f\"Cancer of interest:  {display_cancer_name} (canonical: '{cancer_of_interest}')\")\n",
    "print(f\"Variant of interest: {display_variant_name} (canonical: '{variant_of_interest}')\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128d5a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## UPDATED\n",
    "\n",
    "\n",
    "#### Updated weighted network graph with automated threshold, based on qualitative analysis\n",
    "\n",
    "# Adjustable thresholds\n",
    "TREATMENT_THRESHOLD_PERCENTILE = 80    # highlight top X% of treatment weights\n",
    "TREATMENT_MIN_HIGHLIGHT        = 300   # and require ≥X total weight\n",
    "CANCER_THRESHOLD_PERCENTILE    = 80    # highlight top X% of cancer–variant weights\n",
    "CANCER_MIN_HIGHLIGHT           = 80    # and require ≥X total weight\n",
    "\n",
    "# Prepare consensus lookup\n",
    "df_consensus[\"Variant_Treatment_Pair\"] = (\n",
    "    df_consensus[\"Variant_Treatment_Pair\"]\n",
    "    .str.strip()\n",
    "    .str.lower()\n",
    ")\n",
    "consensus_dict = dict(\n",
    "    zip(df_consensus[\"Variant_Treatment_Pair\"], df_consensus[\"Resolved_Prediction\"])\n",
    ")\n",
    "\n",
    "excluded_treatments = {\n",
    "    'chemotherapy', 'tyrosine kinase inhibitor', 'radiotherapy', 'hormone therapy',\n",
    "    'adjuvant chemotherapy', 'immunotherapy', 'immune checkpoint inhibitor', 'adjuvant chemotherapy',\n",
    "    'mrna vaccine', 'mtor inhibitor', 'radiation ionizing radiotherapy', \n",
    "    'braf inhibitor','angiogenesis inhibitor', 'aromatase inhibitor', 'bet inhibitor',\n",
    "    'EGFR tyrosine kinase inhibitor therapy', 'epidermal growth factor receptor tyrosine kinase inhibitor',\n",
    "    'hematopoietic cell transplantation', 'hyperthermic intraperitoneal chemotherapy', 'TRK inhibitor',\n",
    "    'tyrosine kinase inhibitor', 'therapeutic tumor infiltrating lymphocytes'\n",
    "}\n",
    "\n",
    "# Cancer‐only treatments\n",
    "canc_nei = set(G.neighbors(cancer_of_interest))\n",
    "treatments = [\n",
    "    n for n in canc_nei\n",
    "    if G.nodes[n]['category']=='Treatment'\n",
    "    and n.lower() not in excluded_treatments\n",
    "]\n",
    "t_weights = {t: G[cancer_of_interest][t]['weight'] for t in treatments}\n",
    "top_cancer_treats = sorted(t_weights.items(), key=lambda x: x[1], reverse=True)[:6]\n",
    "c_w = list(t_weights.values())\n",
    "treat_pct = np.percentile(c_w, TREATMENT_THRESHOLD_PERCENTILE) if c_w else 0\n",
    "\n",
    "\n",
    "# Variant + cancer associations\n",
    "sensitive, resistant = [], []\n",
    "for t in treatments:\n",
    "    try:\n",
    "        w = G[cancer_of_interest][t]['weight'] + G[variant_of_interest][t]['weight']\n",
    "        pred = consensus_dict.get(f\"{variant_of_interest} + {t}\".lower())\n",
    "        if pred == \"Sensitive\":\n",
    "            sensitive.append((t, w))\n",
    "        elif pred == \"Resistant\":\n",
    "            resistant.append((t, w))\n",
    "    except KeyError:\n",
    "        continue\n",
    "\n",
    "top_sens = sorted(sensitive, key=lambda x: x[1], reverse=True)[:6]\n",
    "top_res  = sorted(resistant, key=lambda x: x[1], reverse=True)[:6]\n",
    "sens_w = [w for _, w in sensitive]\n",
    "res_w  = [w for _, w in resistant]\n",
    "sens_pct = np.percentile(sens_w, TREATMENT_THRESHOLD_PERCENTILE) if sens_w else 0\n",
    "res_pct  = np.percentile(res_w,   TREATMENT_THRESHOLD_PERCENTILE) if res_w else 0\n",
    "\n",
    "print(f\"\\n\\033[1mSensitive treatments for variant '{variant_of_interest}' \"\n",
    "      f\"(≥{TREATMENT_THRESHOLD_PERCENTILE}th pct & ≥{TREATMENT_MIN_HIGHLIGHT}):\\033[0m\")\n",
    "for t, w in top_sens:\n",
    "    if w >= sens_pct and w >= TREATMENT_MIN_HIGHLIGHT:\n",
    "        print(f\"\\033[1;32m{t}: {w:.0f}\\033[0m\")\n",
    "    else:\n",
    "        print(f\"\\033[2;37m{t}: {w:.0f}\\033[0m\")\n",
    "\n",
    "print(f\"\\n\\033[1mResistant treatments for variant '{variant_of_interest}' \"\n",
    "      f\"(≥{TREATMENT_THRESHOLD_PERCENTILE}th pct & ≥{TREATMENT_MIN_HIGHLIGHT}):\\033[0m\")\n",
    "for t, w in top_res:\n",
    "    if w >= res_pct and w >= TREATMENT_MIN_HIGHLIGHT:\n",
    "        print(f\"\\033[1;31m{t}: {w:.0f}\\033[0m\")\n",
    "    else:\n",
    "        print(f\"\\033[2;37m{t}: {w:.0f}\\033[0m\")\n",
    "\n",
    "        \n",
    "# Other cancers for variant\n",
    "var_nei = set(G.neighbors(variant_of_interest))\n",
    "var_cancers = [\n",
    "    n for n in var_nei\n",
    "    if G.nodes[n]['category']=='Cancer'\n",
    "    and n != cancer_of_interest\n",
    "]\n",
    "vc_weights = {}\n",
    "for c in var_cancers:\n",
    "    w_v = G[variant_of_interest][c]['weight']\n",
    "    w_c = G[cancer_of_interest][c]['weight'] if G.has_edge(cancer_of_interest, c) else 0\n",
    "    vc_weights[c] = w_v + w_c\n",
    "\n",
    "top_var_c = sorted(vc_weights.items(), key=lambda x: x[1], reverse=True)[:6]\n",
    "vc_w = list(vc_weights.values())\n",
    "cancer_pct = np.percentile(vc_w, CANCER_THRESHOLD_PERCENTILE) if vc_w else 0\n",
    "\n",
    "print(f\"\\n\\033[1mOther cancers for variant '{variant_of_interest}' \"\n",
    "      f\"(≥{CANCER_THRESHOLD_PERCENTILE}th pct & ≥{CANCER_MIN_HIGHLIGHT}):\\033[0m\")\n",
    "for c, w in top_var_c:\n",
    "    if w >= cancer_pct and w >= CANCER_MIN_HIGHLIGHT:\n",
    "        print(f\"\\033[1;34m{c.capitalize()}: {w:.0f}\\033[0m\")\n",
    "\n",
    "    else:\n",
    "        print(f\"\\033[2;37m{c.capitalize()}: {w:.0f}\\033[0m\")\n",
    "\n",
    "# Assemble\n",
    "results = []\n",
    "for t, w in top_cancer_treats:\n",
    "    results.append({\n",
    "        \"Cancer\": display_cancer_name, \"Variant\": None,\n",
    "        \"Treatment\": t, \"Association_Type\": \"Cancer-Only\",\n",
    "        \"Prediction\": \"NA\", \"Combined_Weight\": w\n",
    "    })\n",
    "for t, w in top_sens:\n",
    "    results.append({\n",
    "        \"Cancer\": display_cancer_name, \"Variant\": variant_of_interest,\n",
    "        \"Treatment\": t, \"Association_Type\": \"Variant-Cancer\",\n",
    "        \"Prediction\": \"Sensitive\", \"Combined_Weight\": w\n",
    "    })\n",
    "for t, w in top_res:\n",
    "    results.append({\n",
    "        \"Cancer\": display_cancer_name, \"Variant\": variant_of_interest,\n",
    "        \"Treatment\": t, \"Association_Type\": \"Variant-Cancer\",\n",
    "        \"Prediction\": \"Resistant\", \"Combined_Weight\": w\n",
    "    })\n",
    "for c, w in top_var_c:\n",
    "    results.append({\n",
    "        \"Cancer\": c, \"Variant\": variant_of_interest,\n",
    "        \"Treatment\": None, \"Association_Type\": \"Cross-Cancer\",\n",
    "        \"Prediction\": \"NA\", \"Combined_Weight\": w\n",
    "    })\n",
    "\n",
    "df_out = pd.DataFrame(results)\n",
    "safe_c = display_cancer_name.replace(\" \", \"_\").lower()\n",
    "safe_v = variant_of_interest.replace(\" \", \"_\").lower()\n",
    "\n",
    "csv_fname   = f\"network_results_{safe_c}_{safe_v}.csv\"\n",
    "excel_fname = f\"network_results_{safe_c}_{safe_v}.xlsx\"\n",
    "\n",
    "df_out.to_csv(csv_fname, index=False)\n",
    "print(f\"\\nCSV saved to: {csv_fname}\")\n",
    "\n",
    "with pd.ExcelWriter(excel_fname, engine='xlsxwriter') as w:\n",
    "    df_out.to_excel(w, sheet_name='Results', index=False)\n",
    "print(f\"Excel saved to: {excel_fname}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fceb139c",
   "metadata": {},
   "source": [
    "# ================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4de00e",
   "metadata": {},
   "source": [
    "# 4) Create network figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b440a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure to display the full network\n",
    "\n",
    "# Load the graph\n",
    "try:\n",
    "    G_w\n",
    "    print(\"Weighted graph already loaded. Proceeding with analysis.\")\n",
    "except NameError:\n",
    "    print(\"Loading the weighted network graph...\")\n",
    "    path = os.path.join(variantscape_directory, \"network_graph_weighted.gml\")\n",
    "    G_w = nx.read_gml(path)\n",
    "# Use G as the graph variable everywhere else\n",
    "G = G_w.copy()\n",
    "\n",
    "\n",
    "# Filter large components only\n",
    "print(\"Filtering graph...\")\n",
    "if nx.is_directed(G):\n",
    "    G = G.to_undirected()\n",
    "components = list(tqdm(nx.connected_components(G), desc=\"Finding Components\", ncols=100, ascii=True))\n",
    "components = [c for c in components if len(c) >= 50]\n",
    "G = G.subgraph(set().union(*components)).copy()\n",
    "print(f\"Filtered to {len(G.nodes())} nodes and {len(G.edges())} edges.\")\n",
    "\n",
    "print(\"Computing spring layout...\")\n",
    "pos = nx.spring_layout(G, seed=42, k=0.15, iterations=50)\n",
    "\n",
    "# Prepare node visuals (category color, degree size)\n",
    "print(\"Processing node visuals...\")\n",
    "degrees = dict(G.degree())\n",
    "node_sizes = [degrees[n] for n in G.nodes()]\n",
    "max_degree = max(node_sizes)\n",
    "scaled_sizes = [5 + (deg / max_degree) * 20 for deg in node_sizes]\n",
    "\n",
    "\n",
    "category_colors = { \n",
    "    \"Variant\": \"#00b0f0\",     \n",
    "    \"Treatment\": \"#32cd32\",  \n",
    "    \"Cancer\": \"#ff4c4c\"      \n",
    "}\n",
    "\n",
    "\n",
    "node_x, node_y, node_text, node_colors = [], [], [], []\n",
    "for node in tqdm(G.nodes(), desc=\"Placing Nodes\", ncols=100, ascii=True):\n",
    "    x, y = pos[node]\n",
    "    node_x.append(x)\n",
    "    node_y.append(y)\n",
    "    category = G.nodes[node].get('category', 'Variant')\n",
    "    node_colors.append(category_colors.get(category, '#888888'))\n",
    "    node_text.append(f\"{node}<br>Degree: {degrees[node]}\")\n",
    "\n",
    "node_trace = go.Scatter(\n",
    "    x=node_x,\n",
    "    y=node_y,\n",
    "    mode='markers',\n",
    "    text=node_text,\n",
    "    hoverinfo='text',\n",
    "    marker=dict(\n",
    "        size=scaled_sizes,\n",
    "        color=node_colors,\n",
    "        opacity=0.85,\n",
    "        line=dict(width=0.3, color='white')\n",
    "    ),\n",
    "    name=\"\",\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "# Draw edges (but do not show in legend)\n",
    "print(\"Processing edge traces...\")\n",
    "edge_x, edge_y = [], []\n",
    "for u, v in tqdm(G.edges(), desc=\"Drawing Edges\", total=G.number_of_edges(), ncols=100, ascii=True):\n",
    "    x0, y0 = pos[u]\n",
    "    x1, y1 = pos[v]\n",
    "    edge_x.extend([x0, x1, None])\n",
    "    edge_y.extend([y0, y1, None])\n",
    "\n",
    "edge_trace = go.Scatter(\n",
    "    x=edge_x,\n",
    "    y=edge_y,\n",
    "    line=dict(width=0.2, color='rgba(200,200,200,0.15)'),\n",
    "    hoverinfo='none',\n",
    "    mode='lines',\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "# Category legend with note on size\n",
    "legend_entries = [\n",
    "    go.Scatter(x=[None], y=[None], mode='markers',\n",
    "               marker=dict(size=12, color=category_colors['Variant']),\n",
    "               name=\"Variant\"),\n",
    "    go.Scatter(x=[None], y=[None], mode='markers',\n",
    "               marker=dict(size=12, color=category_colors['Treatment']),\n",
    "               name=\"Treatment\"),\n",
    "    go.Scatter(x=[None], y=[None], mode='markers',\n",
    "               marker=dict(size=12, color=category_colors['Cancer']),\n",
    "               name=\"Cancer\"),\n",
    "    go.Scatter(x=[None], y=[None], mode='markers',\n",
    "               marker=dict(size=0.01, color='rgba(0,0,0,0)'),\n",
    "               name=\"Size = Node degree\")\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# Create figure\n",
    "print(\"Creating Plotly figure...\")\n",
    "fig = go.Figure(data=[edge_trace, node_trace] + legend_entries,\n",
    "                layout=go.Layout(\n",
    "                    title=dict(\n",
    "                        text=\"Variantscape: Full network graph of molecular variants, treatments and cancer types\",\n",
    "                        font=dict(size=20, color='white'),\n",
    "                        x=0.5\n",
    "                    ),\n",
    "                    showlegend=True,\n",
    "                    legend=dict(\n",
    "                        font=dict(color='white'),\n",
    "                        title=dict(text=\"Legend\", font=dict(size=14, color='white')),\n",
    "                        bgcolor='rgba(0,0,0,0)',\n",
    "                        x=0.01,\n",
    "                        y=0.99\n",
    "                    ),\n",
    "                    hovermode='closest',\n",
    "                    margin=dict(b=10, l=10, r=10, t=80),\n",
    "                    xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "                    yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "                    plot_bgcolor='black',\n",
    "                    paper_bgcolor='black'\n",
    "                ))\n",
    "print(\"Saving to HTML...\")\n",
    "for _ in tqdm(range(100), desc=\"Writing HTML\", ncols=100, ascii=True):\n",
    "    time.sleep(0.002)\n",
    "fig.write_html(\"variantscape_network_graph.html\")\n",
    "print(\"Saved: variantscape_network_graph.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f672f31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if any nodes are disconnected from the network \n",
    "# (should not be the case, as only articles with all 3 entitiy mentions have been included in the analysis)\n",
    "\n",
    "G_full = G.copy()\n",
    "\n",
    "# Filter large components only\n",
    "print(\"Filtering graph...\")\n",
    "if nx.is_directed(G):\n",
    "    G = G.to_undirected()\n",
    "\n",
    "components = list(tqdm(nx.connected_components(G), desc=\"Finding Components\", ncols=100, ascii=True))\n",
    "components = [c for c in components if len(c) >= 50]\n",
    "G = G.subgraph(set().union(*components)).copy()\n",
    "\n",
    "dropped_nodes = len(G_full.nodes()) - len(G.nodes())\n",
    "print(f\"Filtered to {len(G.nodes())} nodes and {len(G.edges())} edges.\")\n",
    "print(f\"Dropped {dropped_nodes} nodes not in components ≥ 50.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:llms] *",
   "language": "python",
   "name": "conda-env-llms-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
