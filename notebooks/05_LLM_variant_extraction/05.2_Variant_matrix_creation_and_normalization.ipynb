{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0b687c5",
   "metadata": {},
   "source": [
    "# Variant normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc3f013",
   "metadata": {},
   "source": [
    "- Load dataset from LLM (\"llama33-70b\": \"meta-llama/Llama-3.3-70B-Instruct\") variant extraction\n",
    "- Normalization in 3 steps:\n",
    "    - 1. Clean and normalize based on rules (e.g., merge protein changes such as p.V600E and V600E)\n",
    "    - 2. Normalize based on CIVIC API and Aliases\n",
    "    - 3. Normalize based on ClinVar and Ensmbl HGVS Notations\n",
    "   \n",
    "   \n",
    "- Create binary matrix (columns have been merged, so dulicated variants are removed)\n",
    "- Check which proportion is related to Oncomine genes and prostate cancer genes \n",
    "- Output figure of extracted variants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83670bcd",
   "metadata": {},
   "source": [
    "# 1) Set up libraries and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e0639c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "import logging\n",
    "import pprint\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"Success!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288360a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the working directory and file paths\n",
    "input_directory = \"INPUT_DIRECTORY\"\n",
    "output_directory = \"OUTPUT_DIRECTORY\"\n",
    "LLM_directory = \"LLM_DIRECTORY\"\n",
    "\n",
    "os.chdir(output_directory)\n",
    "print(\"Current Working Directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de83d8d3",
   "metadata": {},
   "source": [
    "# 2) Rule-based normalization and matrix creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f337bf2f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Copmute total runtime\n",
    "# Define input and output file paths\n",
    "input_file = \"final_runtime_summary_llama33-70b_prompt3.txt\"\n",
    "output_file = \"final_runtime_summary_statistics_variant_extraction_llama33-70b_prompt3.txt\"\n",
    "batch_runtimes = []\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        match = re.search(r'Batch Runtime:\\s*(\\d+)', line)\n",
    "        if match:\n",
    "            batch_runtimes.append(int(match.group(1)))\n",
    "\n",
    "total_seconds = sum(batch_runtimes)\n",
    "minutes, seconds = divmod(total_seconds, 60)\n",
    "hours, minutes = divmod(minutes, 60)\n",
    "days, hours = divmod(hours, 24)\n",
    "\n",
    "if seconds >= 30: minutes += 1\n",
    "if minutes >= 60: hours, minutes = hours + 1, 0\n",
    "if hours >= 24: days, hours = days + 1, 0\n",
    "\n",
    "output_content = [f\"\\n--- Batch execution times ---\"] + \\\n",
    "                 [f\"Batch {i}: {runtime:,} sec\" for i, runtime in enumerate(batch_runtimes, start=1)] + \\\n",
    "                 [f\"\\n--- Summary statistics ---\",\n",
    "                  f\"Total execution time: {total_seconds:,} sec\",\n",
    "                  f\"Total execution time: {math.ceil(total_seconds / 60):,} min\",\n",
    "                  f\"Total execution time: {math.ceil(total_seconds / 3600):,} h\",\n",
    "                  f\"Total execution time: {' '.join(filter(None, [f'{days}d' if days else '', f'{hours}h' if hours else '', f'{minutes} min' if minutes else '']))}\"]\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as out_file:\n",
    "    out_file.write(\"\\n\".join(output_content) + \"\\n\")\n",
    "\n",
    "print(\"\\n\".join(output_content))\n",
    "print(f\"\\n Saved in '{output_file}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8057eb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "### 1 Load dataset with LLM-identified variants and extract to pairs ###\n",
    "#####################################################################################\n",
    "\n",
    "# Load dataset\n",
    "os.chdir(LLM_directory)\n",
    "file_path = \"final_LLM_variant_extraction_llama33-70b_prompt3.csv\"\n",
    "initial_variant_df = pd.read_csv(file_path)\n",
    "print(\"Dataset loaded\")\n",
    "os.chdir(LLM_directory)\n",
    "\n",
    "\n",
    "initial_rows= len(initial_variant_df)\n",
    "initial_columns = len(initial_variant_df.columns)\n",
    "\n",
    "print(f\"Length of dataset: {initial_rows:,} rows\")\n",
    "print(f\"Length of columns: {initial_columns:,} columns before variant and gene extraction\")\n",
    "\n",
    "variant_df = initial_variant_df.copy()\n",
    "\n",
    "# Extract gene and variant pairs\n",
    "variant_df[\"PaperId\"] = pd.to_numeric(variant_df[\"PaperId\"], errors='coerce').fillna(0).astype('int64')\n",
    "variant_column = \"LLM_Response\"\n",
    "\n",
    "def extract_variant_gene_pairs(text):\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    matches = re.findall(r\"Variant:\\s*([\\w\\d\\.\\*-]+),\\s*Gene:\\s*([\\w\\d]+)\", text)\n",
    "    return matches if matches else []\n",
    "\n",
    "variant_df[\"Variant_Gene_Pairs\"] = variant_df[variant_column].apply(extract_variant_gene_pairs)\n",
    "\n",
    "max_variants = variant_df[\"Variant_Gene_Pairs\"].apply(lambda x: len(set(x)) if isinstance(x, list) else 0).max()\n",
    "total_tuples = variant_df[\"Variant_Gene_Pairs\"].apply(len).sum()\n",
    "\n",
    "print(f\"\\n\\nAfter variant and gene tuple extraction\")\n",
    "print(f\"Max number of unique variant-gene pairs in a row: {max_variants}\")\n",
    "print(f\"Total variant-gene tuples extracted across all rows: {total_tuples:,}\")\n",
    "print(f\"Length of columns: {len(variant_df.columns):,} columns (+1 added)\")\n",
    "\n",
    "#####################################################################################\n",
    "### 2 Clean and deduplicate variant-gene pairs ###\n",
    "#####################################################################################\n",
    "\n",
    "# --- Helper Functions for Normalization ---\n",
    "def remove_hgvs_prefix(variant):\n",
    "    return re.sub(r\"^[cp]\\.\", \"\", variant, flags=re.IGNORECASE)\n",
    "\n",
    "aa_3_to_1 = {\n",
    "    \"val\": \"V\", \"gly\": \"G\", \"glu\": \"E\", \"asp\": \"D\", \"thr\": \"T\", \"met\": \"M\", \n",
    "    \"ala\": \"A\", \"leu\": \"L\", \"ser\": \"S\", \"pro\": \"P\", \"cys\": \"C\", \"phe\": \"F\",\n",
    "    \"his\": \"H\", \"lys\": \"K\", \"asn\": \"N\", \"tyr\": \"Y\", \"trp\": \"W\", \"ile\": \"I\", \n",
    "    \"arg\": \"R\", \"gln\": \"Q\"\n",
    "}\n",
    "\n",
    "def normalize_amino_acids(variant):\n",
    "    aa3 = aa_3_to_1\n",
    "    pattern = re.compile(rf\"(?i)\\b({'|'.join(aa3.keys())})(\\d+)({'|'.join(aa3.keys())})\\b\")\n",
    "\n",
    "    def replacer(match):\n",
    "        ref = match.group(1).lower()\n",
    "        pos = match.group(2)\n",
    "        alt = match.group(3).lower()\n",
    "        return f\"{aa3[ref]}{pos}{aa3[alt]}\"\n",
    "\n",
    "    variant = pattern.sub(replacer, variant)\n",
    "    return variant\n",
    "\n",
    "\n",
    "exon_map = {\n",
    "    \"exon19del\": [\"19del\", \"del19\", \"ex19del\", \"exon19deletion\"],\n",
    "    \"exon20ins\": [\"ex20ins\", \"ins20\", \"exon20insertion\", \"ex20insertion\"]\n",
    "}\n",
    "\n",
    "def map_exon_variant(variant):\n",
    "    variant_lower = variant.lower()\n",
    "    for canonical, variants in exon_map.items():\n",
    "        if any(v in variant_lower for v in variants):\n",
    "            return canonical\n",
    "    return variant\n",
    "\n",
    "# Cleaning configuration\n",
    "unwanted_variants = {\"vus\", \"c.\", \"loss\", \"indel\"}  \n",
    "remove_if_contains = {\"mutation\", \"mutated\", \"mut\", \"mutant\", \"sensitive\", \n",
    "                      \"deficient\", \"null\", \"amplified\", \"deficiency\", \"deletion\",\n",
    "                      \"loxp\", \"loss\", \"knockdown\", \"flox\"}  \n",
    "max_variant_length = 40  \n",
    "max_gene_length = 10  \n",
    "min_variant_length = 2\n",
    "min_gene_length = 2\n",
    "specific_variant_gene_removals = {(\"p53\", \"TP53\")}\n",
    "\n",
    "# Define gene alias map\n",
    "gene_alias_map.update({\n",
    "    \"CASPASE\": \"CASP1\",\n",
    "    \"CBRAF\": \"BRAF\",\n",
    "    \"CHECK2\": \"CHEK2\",\n",
    "    \"CKIT\": \"KIT\",\n",
    "    \"CMET\": \"MET\",\n",
    "    \"CMYC\": \"MYC\",\n",
    "    \"CTNNB\": \"CTNNB1\",\n",
    "    \"CYP19\": \"CYP19A1\",\n",
    "    \"EGFR2\": \"ERBB2\",\n",
    "    \"ER\": \"ESR1\",\n",
    "    \"ERB\": \"ERBB2\",\n",
    "    \"ERB2\": \"ERBB2\",\n",
    "    \"ERBB\": \"ERBB2\",\n",
    "    \"ERRB2\": \"ERBB2\",\n",
    "    \"ERRB3\": \"ERBB3\",\n",
    "    \"ERRB4\": \"ERBB4\",\n",
    "    \"ESR\": \"ESR1\",\n",
    "    \"FACND2\": \"FANCD2\",\n",
    "    \"FCR\": \"FCGR3A\",\n",
    "    \"FCRIIIA\": \"FCGR3A\",\n",
    "    \"FP\": \"PTGFR\",\n",
    "    \"GAL\": \"GAL1\",\n",
    "    \"GALECTIN\": \"LGALS1\",\n",
    "    \"GCSF\": \"CSF3\",\n",
    "    \"GLUT3\": \"SLC2A3\",\n",
    "    \"GNB2L1\": \"RACK1\",\n",
    "    \"GNAS1\": \"GNAS\",\n",
    "    \"GP130\": \"IL6ST\",\n",
    "    \"GPX\": \"GPX1\",\n",
    "    \"GQ\": \"GNAQ\",\n",
    "    \"HER-2\": \"ERBB2\",\n",
    "    \"HER-3\": \"ERBB3\",\n",
    "    \"HER-4\": \"ERBB4\",\n",
    "    \"HER2\": \"ERBB2\",\n",
    "    \"HER2-NEU\": \"ERBB2\",\n",
    "    \"HER2/NEU\": \"ERBB2\",\n",
    "    \"HER2NEU\": \"ERBB2\",\n",
    "    \"HER3\": \"ERBB3\",\n",
    "    \"HER4\": \"ERBB4\",\n",
    "    \"TELOMERASE\": \"TERT\"\n",
    "})\n",
    "\n",
    "# Set of known \"non-genes\" or invalid gene terms\n",
    "invalid_gene_set = {\n",
    "    \"ANDROGEN\", \"CYP450\", \"CATENIN\", \"CSF\", \"14\",\"2Q35\", \"6Q25\", \"8Q24\", \"9Q31\", \"VARIANT\",\n",
    "    \"FCR\", \"FCRIIIA\", \"GCSF\",     # cytokine name, not symbol\n",
    "    \"GAL\",      # ambiguous (Galectin or Galanin?)\n",
    "    \"GALECTIN\", # not a gene symbol\n",
    "    \"GQ\",       # G protein alpha-q, map or remove\n",
    "    \"ANDROGEN\", \"CYP450\",\"CATENIN\",\n",
    "    \"2Q35\", \"6Q25\", \"8Q24\", \"9Q31\", \"FNACA\", \"FOPNL\",\n",
    "    \"FP\", \"GM\", \"G11\",\n",
    "    \"VITAMIN\",\n",
    "    \"ISOCITRATE\",\n",
    "    \"MICRORNA\",\n",
    "    \"LOC146880\", \"LOC643714\", \"LOC730100\",\n",
    "    \"HISTONE3\",\n",
    "    \"CIRCHIBADH\",\n",
    "    \"SURVIVIN\",\n",
    "    \"LINC00951\", \"LINC01614\", \"LINC02183\", \"LINC02869\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Define main cleaning function\n",
    "def clean_variant_gene_pairs(pairs):\n",
    "    cleaned_pairs = []\n",
    "    seen_pairs = set()\n",
    "\n",
    "    for variant, gene in pairs:\n",
    "        gene_clean = gene.strip().upper()\n",
    "        gene_clean = re.sub(r\"[-_\\s]\", \"\", gene_clean)\n",
    "        gene_clean = gene_alias_map.get(gene_clean, gene_clean)\n",
    "        if gene_clean in invalid_gene_set:\n",
    "            continue\n",
    "        variant_clean = variant.strip()\n",
    "\n",
    "        # Remove p53 prefix if gene is TP53\n",
    "        if gene_clean == \"TP53\" and variant_clean.lower().startswith(\"p53\"):\n",
    "            variant_clean = variant_clean[3:]\n",
    "\n",
    "        # Normalize amino acids (e.g., Val600 → V600)\n",
    "        variant_clean = normalize_amino_acids(variant_clean)\n",
    "\n",
    "        # Map exon patterns (e.g., ex20ins → exon20ins)\n",
    "        variant_clean = map_exon_variant(variant_clean)\n",
    "\n",
    "        # Remove HGVS prefixes like \"c.\", \"p.\"\n",
    "        variant_clean = re.sub(r\"^[cp]\\.\", \"\", variant_clean, flags=re.IGNORECASE)\n",
    "\n",
    "        # Remove symbols and lowercase it\n",
    "        variant_clean = re.sub(r\"[-_\\s\\.]\", \"\", variant_clean).lower()\n",
    "\n",
    "        # Skip if variant == gene\n",
    "        if variant_clean == gene_clean.lower():\n",
    "            continue\n",
    "\n",
    "        # Filter rules\n",
    "        if len(variant_clean) < min_variant_length:\n",
    "            continue\n",
    "        if len(gene_clean) < min_gene_length:\n",
    "            continue\n",
    "        if (variant_clean, gene_clean) in specific_variant_gene_removals:\n",
    "            continue\n",
    "        if variant_clean in unwanted_variants:\n",
    "            continue\n",
    "        if any(term in variant_clean for term in remove_if_contains):\n",
    "            continue\n",
    "        if gene_clean in {\"NONE\", \"NO\", \"NOT\", \"UNKNOWN\", \"\"}:\n",
    "            continue\n",
    "        if len(variant_clean) > max_variant_length or len(gene_clean) > max_gene_length:\n",
    "            continue\n",
    "        if variant_clean == \"none\":\n",
    "            continue\n",
    "\n",
    "        # Deduplicate\n",
    "        pair_key = (variant_clean, gene_clean)\n",
    "        if pair_key not in seen_pairs:\n",
    "            seen_pairs.add(pair_key)\n",
    "            cleaned_pairs.append(pair_key)\n",
    "\n",
    "    return cleaned_pairs\n",
    "\n",
    "variant_df[\"Cleaned_Variant_Gene_Pairs\"] = variant_df[\"Variant_Gene_Pairs\"].progress_apply(clean_variant_gene_pairs)\n",
    "cleaned_rows, cleaned_cols = variant_df.shape\n",
    "print(f\"After initial rule-based cleaning: {cleaned_rows:,} rows, {cleaned_cols:,} columns.\")\n",
    "total_cleaned_tuples = variant_df[\"Cleaned_Variant_Gene_Pairs\"].apply(len).sum()\n",
    "print(f\"Total variant-gene tuples after cleaning: {total_cleaned_tuples:,}\")\n",
    "\n",
    "#####################################################################################\n",
    "### 3 Expand variant and gene pairs into separate columns for matrix creation ###\n",
    "#####################################################################################\n",
    "variant_columns = [f\"Variant_{i+1}\" for i in range(max_variants)]\n",
    "gene_columns = [f\"Gene_{i+1}\" for i in range(max_variants)]\n",
    "all_columns = [col for pair in zip(variant_columns, gene_columns) for col in pair]\n",
    "\n",
    "expanded_data = []\n",
    "for pairs in tqdm(variant_df[\"Cleaned_Variant_Gene_Pairs\"], desc=\"Expanding variant-gene pairs\"):\n",
    "    row_values = sum(([v, g] for v, g in pairs), [])  \n",
    "    row_values += [np.nan] * (len(all_columns) - len(row_values))  \n",
    "    expanded_data.append(row_values)\n",
    "\n",
    "expanded_df = pd.DataFrame(expanded_data, columns=all_columns)\n",
    "expanded_df.index = variant_df.index\n",
    "variant_df = pd.concat([variant_df, expanded_df], axis=1)\n",
    "\n",
    "#####################################################################################\n",
    "### Variant Name Cleaning (After Expansion) for Multiple Genes ###\n",
    "#####################################################################################\n",
    "def standardize_variant_gene_names(row):\n",
    "    for i in range(max_variants):\n",
    "        variant_col = f\"Variant_{i+1}\"\n",
    "        gene_col = f\"Gene_{i+1}\"\n",
    "\n",
    "        variant_value = row.get(variant_col)\n",
    "        gene_value = row.get(gene_col)\n",
    "\n",
    "        if pd.notna(variant_value):\n",
    "            variant = str(variant_value).strip().lower()\n",
    "            variant = re.sub(r\"[-_\\s]\", \"\", variant)\n",
    "            row[variant_col] = variant\n",
    "\n",
    "        if pd.notna(gene_value):\n",
    "            gene = str(gene_value).strip().upper()\n",
    "            row[gene_col] = gene\n",
    "    return row\n",
    "\n",
    "def remove_gene_from_variant(row, max_variants):\n",
    "    for i in range(max_variants):\n",
    "        variant_col = f\"Variant_{i+1}\"\n",
    "        gene_col = f\"Gene_{i+1}\"\n",
    "\n",
    "        variant_value = row.get(variant_col)\n",
    "        gene_value = row.get(gene_col)\n",
    "\n",
    "        if pd.notna(gene_value) and pd.notna(variant_value):\n",
    "            gene = str(gene_value).strip().upper()\n",
    "            variant = str(variant_value).strip()\n",
    "\n",
    "            if variant.upper().startswith(f\"{gene}-\") and not re.match(rf\"^{gene}[-][*\\.]\", variant, re.IGNORECASE):\n",
    "                row[variant_col] = variant[len(gene) + 1:]\n",
    "            elif variant.upper().startswith(gene) and not re.match(rf\"^{gene}[*\\.]\", variant, re.IGNORECASE):\n",
    "                row[variant_col] = variant[len(gene):]\n",
    "    return row\n",
    "\n",
    "def remove_alias_from_variant(row, gene_alias_map, max_variants):\n",
    "    alias_reverse_map = defaultdict(list)\n",
    "    for alias, canonical in gene_alias_map.items():\n",
    "        alias_reverse_map[canonical].append(alias.lower())\n",
    "    for i in range(max_variants):\n",
    "        variant_col = f\"Variant_{i+1}\"\n",
    "        gene_col = f\"Gene_{i+1}\"\n",
    "        variant = row.get(variant_col)\n",
    "        gene = row.get(gene_col)\n",
    "        if pd.notna(variant) and pd.notna(gene):\n",
    "            gene = gene.strip().upper()\n",
    "            variant = variant.strip().lower()\n",
    "            aliases = alias_reverse_map.get(gene, [])\n",
    "            for alias in aliases:\n",
    "                if variant.startswith(alias):\n",
    "                    row[variant_col] = variant[len(alias):]\n",
    "                    break\n",
    "    return row\n",
    "\n",
    "variant_df = variant_df.progress_apply(standardize_variant_gene_names, axis=1)\n",
    "variant_df = variant_df.progress_apply(remove_gene_from_variant, axis=1, args=(max_variants,))\n",
    "print(\"Applied variant name standardization and gene name removal after expansion for BRAF, EGFR, TP53, and others.\")\n",
    "variant_df = variant_df.progress_apply(remove_alias_from_variant, axis=1, args=(gene_alias_map, max_variants))\n",
    "print(\"Removed alias prefixes (e.g., HER2) from variants based on mapped gene.\")\n",
    "\n",
    "#####################################################################################\n",
    "### Remove short genes and variants again ###\n",
    "#####################################################################################\n",
    "def final_remove_short_entries(row):\n",
    "    for i in range(max_variants):\n",
    "        variant_col = f\"Variant_{i+1}\"\n",
    "        gene_col = f\"Gene_{i+1}\"\n",
    "        if variant_col in row and pd.notna(row[variant_col]) and len(row[variant_col]) < min_variant_length:\n",
    "            row[variant_col] = np.nan\n",
    "        if gene_col in row and pd.notna(row[gene_col]) and len(row[gene_col]) < min_gene_length:\n",
    "            row[gene_col] = np.nan\n",
    "    return row\n",
    "variant_df = variant_df.progress_apply(final_remove_short_entries, axis=1)\n",
    "print(\"Final cleaning: Removed short variants and genes after all processing.\")\n",
    "\n",
    "#####################################################################################\n",
    "### 4 Create binary matrix in the next cell! ###\n",
    "#####################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40ae2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect and print all unique Variant_n / Gene_n combinations\n",
    "\n",
    "variant_cols = [col for col in variant_df.columns if col.startswith(\"Variant_\")]\n",
    "gene_cols = [col for col in variant_df.columns if col.startswith(\"Gene_\")]\n",
    "variant_gene_set = set()\n",
    "for variant_col, gene_col in zip(variant_cols, gene_cols):\n",
    "    pairs = variant_df[[variant_col, gene_col]].dropna()\n",
    "    for variant, gene in zip(pairs[variant_col], pairs[gene_col]):\n",
    "        if isinstance(variant, str) and isinstance(gene, str):\n",
    "            cleaned_variant = variant.strip().lower()\n",
    "            cleaned_gene = gene.strip().upper()\n",
    "            variant_gene_set.add((cleaned_variant, cleaned_gene))\n",
    "unique_variant_gene_pairs = sorted(variant_gene_set)\n",
    "print(\"\\n Unique (Variant, Gene) pairs found in Variant_n / Gene_n columns:\")\n",
    "for variant, gene in unique_variant_gene_pairs:\n",
    "    print(f\"{variant} — {gene}\")\n",
    "print(f\"\\n Total unique Variant-Gene pairs: {len(unique_variant_gene_pairs):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37faa3c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Collect and print all unique Genes from Gene_n columns\n",
    "gene_cols = [col for col in variant_df.columns if col.startswith(\"Gene_\")]\n",
    "unique_genes = set()\n",
    "\n",
    "for gene_col in gene_cols:\n",
    "    genes = variant_df[gene_col].dropna()\n",
    "    for gene in genes:\n",
    "        if isinstance(gene, str):\n",
    "            cleaned_gene = gene.strip().upper()\n",
    "            unique_genes.add(cleaned_gene)\n",
    "unique_genes_sorted = sorted(unique_genes, key=lambda x: (len(x), x))\n",
    "print(\"\\n Unique gene names found across Gene_n columns (sorted by length):\\n\")\n",
    "for gene in unique_genes_sorted:\n",
    "    print(gene)\n",
    "print(f\"\\n Total unique genes: {len(unique_genes_sorted):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7712f7f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "### 4 Create binary matrix ###\n",
    "#####################################################################################\n",
    "# Initialize the binary matrix columns with pre-allocated values\n",
    "variant_gene_pairs = []\n",
    "for index, row in tqdm(variant_df.iterrows(), total=len(variant_df), desc=\"Collecting variant-gene IDs\"):\n",
    "    for variant_col, gene_col in zip(variant_columns, gene_columns):\n",
    "        if pd.notna(row[variant_col]) and pd.notna(row[gene_col]):\n",
    "            variant = str(row[variant_col]).strip().lower()\n",
    "            gene = str(row[gene_col]).strip().upper()\n",
    "            variant_gene_id = f\"{variant}_{gene}\"\n",
    "            if variant_gene_id not in variant_gene_pairs:\n",
    "                variant_gene_pairs.append(variant_gene_id)\n",
    "print(f\"Total unique variant-gene columns: {len(variant_gene_pairs):,}\")\n",
    "\n",
    "# Pre-allocate the binary matrix with the correct columns\n",
    "binary_matrix = pd.DataFrame(0, index=variant_df.index, columns=variant_gene_pairs)\n",
    "\n",
    "# Iterate through each row and mark the presence of variant-gene pairs\n",
    "for index, row in tqdm(variant_df.iterrows(), total=len(variant_df), desc=\"Building binary matrix\"):\n",
    "    seen_pairs = set()\n",
    "    \n",
    "    for variant_col, gene_col in zip(variant_columns, gene_columns):\n",
    "        if pd.notna(row[variant_col]) and pd.notna(row[gene_col]):\n",
    "            variant = str(row[variant_col]).strip().lower()\n",
    "            gene = str(row[gene_col]).strip().upper()\n",
    "            variant_gene_id = f\"{variant}_{gene}\"\n",
    "\n",
    "            if variant_gene_id not in seen_pairs:\n",
    "                binary_matrix.at[index, variant_gene_id] = 1\n",
    "                seen_pairs.add(variant_gene_id)\n",
    "\n",
    "binary_matrix[\"PaperId\"] = variant_df[\"PaperId\"]\n",
    "print(\"Binary matrix created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88fb6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investiagte bianry matrix\n",
    "cols = binary_matrix.columns.tolist()\n",
    "if len(cols) == len(set(cols)):\n",
    "    print(\"All column names are unique.\")\n",
    "else:\n",
    "    print(\"Duplicate column names found!\")\n",
    "    # Print duplicates\n",
    "    duplicates = [col for col, count in Counter(cols).items() if count > 1]\n",
    "    print(f\"Duplicate columns ({len(duplicates)}):\")\n",
    "    for col in duplicates:\n",
    "        print(f\"  {col}\")    \n",
    "binary_cols = [col for col in binary_matrix.columns if col != \"PaperId\"]\n",
    "non_zero_columns = (binary_matrix[binary_cols].sum(axis=0) >= 1)\n",
    "active_count = non_zero_columns.sum()\n",
    "inactive_count = len(binary_cols) - active_count\n",
    "print(f\"Total binary variant-gene columns: {len(binary_cols):,}\")\n",
    "print(f\"Columns with at least one 1:       {active_count:,}\")\n",
    "print(f\"Columns with all zeros:            {inactive_count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460964cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "### 5 Merge binary matrix with dataset and add total_variant_count ###\n",
    "#####################################################################################\n",
    "tqdm.pandas(desc=\"Merging and calculating total variant count\")\n",
    "\n",
    "final_variant_matrix_df = variant_df.merge(binary_matrix, on=\"PaperId\", how=\"left\")\n",
    "binary_columns = [col for col in binary_matrix.columns if col != \"PaperId\"]\n",
    "final_variant_matrix_df[\"total_variant_count\"] = final_variant_matrix_df[binary_columns].progress_apply(\n",
    "    lambda row: row.sum(), axis=1\n",
    ")\n",
    "print(\"Binary matrix created and merged with PaperId!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6daf58ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "################################ 6 FINAL SUMMARY ####################################\n",
    "#####################################################################################\n",
    "# Count statistics using 'total_variant_count'\n",
    "rows_with_variants = (final_variant_matrix_df[\"total_variant_count\"] >= 1).sum()\n",
    "rows_without_variants = (final_variant_matrix_df[\"total_variant_count\"] == 0).sum()\n",
    "percentage_with_variants = (rows_with_variants / initial_rows) * 100\n",
    "percentage_without_variants = (rows_without_variants / initial_rows) * 100\n",
    "\n",
    "matrix_output_path = os.path.join(LLM_directory, \"full_variant_dataframe_matrix.csv\")\n",
    "\n",
    "chunk_size = 10000\n",
    "num_chunks = math.ceil(len(final_variant_matrix_df) / chunk_size)\n",
    "\n",
    "with open(matrix_output_path, mode='w', newline='', encoding='utf-8') as f:\n",
    "    for i, chunk_start in enumerate(tqdm(range(0, len(final_variant_matrix_df), chunk_size), desc=\"Saving CSV in chunks\")):\n",
    "        chunk_end = chunk_start + chunk_size\n",
    "        chunk = final_variant_matrix_df.iloc[chunk_start:chunk_end]\n",
    "        header = (i == 0) \n",
    "        chunk.to_csv(f, index=False, header=header)\n",
    "        \n",
    "print(\"\\n##################################### FINAL SUMMARY ######################################\")\n",
    "print(f\" Initial dataset: {initial_rows:,} rows, {initial_columns:,} columns.\")\n",
    "print(f\" After binary matrix creation: {len(final_variant_matrix_df):,} rows, {len(final_variant_matrix_df.columns):,} columns.\")\n",
    "\n",
    "print(\"\\n----------------- Variant Count Statistics -----------------\")\n",
    "print(f\" Rows with at least 1 variant: {rows_with_variants:,} ({percentage_with_variants:.2f}%)\")\n",
    "print(f\" Rows with 0 variants: {rows_without_variants:,} ({percentage_without_variants:.2f}%)\")\n",
    "print(f\" Total variants across all rows: {final_variant_matrix_df['total_variant_count'].sum():,}\")\n",
    "\n",
    "print(f\"\\n\\nLength of rows:      {len(final_variant_matrix_df):,}\")\n",
    "print(f\"Length of columns:    {len(final_variant_matrix_df.columns):,}\")\n",
    "print(final_variant_matrix_df)\n",
    "\n",
    "print(\"\\nProcessing completed successfully. Final dataset saved as: full_variant_dataframe_matrix.csv\")\n",
    "print(\"############################################################################################\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63955150",
   "metadata": {},
   "source": [
    "# Variant normalzation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec561247",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_variant_matrix_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4557c8ee",
   "metadata": {},
   "source": [
    "## 1) Variant normalzation with CIVIC Aliases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4e53cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(LLM_directory)\n",
    "non_binary_fields = {\n",
    "    \"PaperId\",\n",
    "    \"PaperTitle\",\n",
    "    \"Abstract\",\n",
    "    \"LLM_Prompt\",\n",
    "    \"LLM_Response\",\n",
    "    \"Variant_Gene_Pairs\",\n",
    "    \"Cleaned_Variant_Gene_Pairs\",\n",
    "    \"total_variant_count\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b35f88d",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "- 1) Fetch CIVIC \"Variant\" information (Variand ID, Variant Name, Aliases) --> common comparator: \"Variant ID\"\n",
    "- 2) Fetch CIVIC \"Molecular profiles\" information (ID, Name, Descirption, Scroe, Variants, Assertion)  --> common comparator: \"ID\"\n",
    "- 3) Fetch CIVIC \"Gene\" information (Molecular Profile ID but doudplcates!) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684540de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load matrix only if not already in memory\n",
    "if \"final_variant_matrix_df\" not in globals():\n",
    "    print(\"Loading binary matrix from CSV...\")\n",
    "    final_variant_matrix_df = pd.read_csv(\"full_variant_dataframe_matrix.csv\", index_col=0, low_memory=False)\n",
    "else:\n",
    "    print(\"Using existing 'final_variant_matrix_df' from memory.\")\n",
    "    \n",
    "# Filter the DataFrame to include only rows where 'total_variant_count' is >= 1\n",
    "filtered_df = final_variant_matrix_df[final_variant_matrix_df[\"total_variant_count\"] >= 1]\n",
    "num_rows = len(filtered_df)\n",
    "print(f\"Number of all rows: {len(final_variant_matrix_df)}\")\n",
    "print(f\"Number of rows where 'total_variant_count' >= 1: {num_rows:,}\")\n",
    "\n",
    "# Save the filtered DataFrame as a new variable and export to CSV\n",
    "filtered_variant_matrix_df = filtered_df.copy()  # Save as a new variable\n",
    "filtered_variant_matrix_df.to_csv(\"filtered_variant_matrix_df.csv\")\n",
    "print(\"Filtered DataFrame saved successfully as 'filtered_variant_matrix_df.csv'.\")\n",
    "\n",
    "# Detect true binary matrix columns (variant_gene format + binary values)\n",
    "non_binary_fields = [\"PaperId\", \"PaperTitle\", \"Abstract\", \"total_variant_count\"]\n",
    "\n",
    "binary_columns = [\n",
    "    col for col in filtered_variant_matrix_df.columns\n",
    "    if col not in non_binary_fields\n",
    "    and isinstance(col, str)\n",
    "    and re.match(r\"^[a-z0-9\\*\\.\\-]+_[A-Z0-9]+$\", col)\n",
    "    and filtered_variant_matrix_df[col].dropna().isin([0, 1]).all()\n",
    "]\n",
    "\n",
    "# Sum variant-gene mentions and group by variant name (before \"_\")\n",
    "binary_matrix_sums = filtered_variant_matrix_df[binary_columns].sum()\n",
    "variant_names = binary_matrix_sums.index.to_series().apply(lambda x: x.rsplit(\"_\", 1)[0])\n",
    "variant_counts = binary_matrix_sums.groupby(variant_names).sum().sort_values(ascending=False)\n",
    "variant_counts_df = variant_counts.to_frame(name=\"Binary Matrix Mentions\")\n",
    "\n",
    "\n",
    "variant_counts_df.to_csv(\"variant_counts_summary.csv\")\n",
    "print(\"Variant counts summary saved successfully as 'variant_counts_summary.csv'.\")\n",
    "print(\"\\n--------- Variant Mention Summary ---------\")\n",
    "print(f\"Unique variants detected: {len(variant_counts):,}\")\n",
    "print(f\"Total variant-gene mentions: {binary_matrix_sums.sum():,}\")\n",
    "print(\"\\nTop 20 variants:\")\n",
    "print(variant_counts_df.head(20).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ad0daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fetch Variant information\n",
    "# Helper method to run the API query\n",
    "def run_query(query, variables):\n",
    "    request = requests.post('https://civicdb.org/api/graphql', json={'query': query, 'variables': variables})\n",
    "    if request.status_code == 200:\n",
    "        return request.json()\n",
    "    else:\n",
    "        raise Exception(f\"Query failed with code {request.status_code}. {query}\")\n",
    "\n",
    "# The GraphQL query to fetch variants and their aliases\n",
    "query = \"\"\"\n",
    "query variants($after: String) {\n",
    "  variants(after: $after) {\n",
    "    pageInfo {\n",
    "      hasNextPage\n",
    "      endCursor\n",
    "    }\n",
    "    nodes {\n",
    "      id\n",
    "      name\n",
    "      variantAliases\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Pagination settings\n",
    "hasNextPage = True\n",
    "previousPageEnd = None\n",
    "variant_data = []\n",
    "total_variants = 10000 \n",
    "\n",
    "# Progress Bar\n",
    "with tqdm(total=total_variants, desc=\"Fetching Variants\", unit=\"variants\") as pbar:\n",
    "    while hasNextPage:\n",
    "        variables = {\"after\": previousPageEnd}\n",
    "        resp = run_query(query, variables)\n",
    "        data = resp['data']['variants']\n",
    "        \n",
    "        # Collect variant names and aliases\n",
    "        for variant in data['nodes']:\n",
    "            aliases = variant.get('variantAliases', [])\n",
    "            variant_data.append({\n",
    "                'Variant ID': variant['id'],\n",
    "                'Variant Name': variant['name'],\n",
    "                'Aliases': \", \".join(aliases) if aliases else \"None\"\n",
    "            })\n",
    "\n",
    "        # Update progress bar\n",
    "        pbar.update(len(data['nodes']))\n",
    "\n",
    "        # Check if there are more pages\n",
    "        hasNextPage = data['pageInfo']['hasNextPage']\n",
    "        previousPageEnd = data['pageInfo']['endCursor']\n",
    "\n",
    "# Convert collected data to DataFrame\n",
    "CIVIC_variants_df = pd.DataFrame(variant_data)\n",
    "pprint.pprint(CIVIC_variants_df.head())\n",
    "csv_filename = \"variants_and_aliases_CIVIC.csv\"\n",
    "CIVIC_variants_df.to_csv(csv_filename, index=False)\n",
    "print(f\"\\nCSV saved as {csv_filename}\")\n",
    "print(f\"\\nTotal variants collected: {len(CIVIC_variants_df)}\")\n",
    "print(\"\\nTable of variants and aliases:\")\n",
    "print(CIVIC_variants_df.head()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b40030e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch molecular profiles\n",
    "\n",
    "# API Endpoint and Headers\n",
    "url = \"https://civicdb.org/api/graphql\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "# GraphQL Query\n",
    "query = \"\"\"\n",
    "query browseMolecularProfiles($after: String) {\n",
    "  molecularProfiles(first: 300, after: $after) {\n",
    "    edges {\n",
    "      node {\n",
    "        id\n",
    "        name\n",
    "        description\n",
    "        molecularProfileScore\n",
    "        variants {\n",
    "          id\n",
    "          name\n",
    "        }\n",
    "        assertions {\n",
    "          nodes{\n",
    "            id\n",
    "            name\n",
    "            description\n",
    "            disease{\n",
    "              id\n",
    "              name\n",
    "            } \n",
    "          }\n",
    "        } \n",
    "      }\n",
    "    }\n",
    "    pageInfo {\n",
    "      endCursor\n",
    "      hasNextPage\n",
    "    }\n",
    "    totalCount\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Fetch total count first for progress bar\n",
    "response = requests.post(url, json={'query': query, 'variables': {\"after\": None}}, headers=headers)\n",
    "response_json = response.json()\n",
    "total_profiles = response_json.get(\"data\", {}).get(\"molecularProfiles\", {}).get(\"totalCount\", 0)\n",
    "\n",
    "# Initialize variables for fetching data\n",
    "all_molecular_profiles = []\n",
    "variables = {\"after\": None}\n",
    "\n",
    "# Progress Bar\n",
    "with tqdm(total=total_profiles, desc=\"Fetching Molecular Profiles\", unit=\"profiles\") as pbar:\n",
    "    while True:\n",
    "        response = requests.post(url, json={'query': query, 'variables': variables}, headers=headers)\n",
    "        response_json = response.json()\n",
    "        \n",
    "        if 'data' in response_json:\n",
    "            molecular_profiles = response_json[\"data\"][\"molecularProfiles\"][\"edges\"]\n",
    "            all_molecular_profiles.extend(molecular_profiles)\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.update(len(molecular_profiles))\n",
    "\n",
    "            # Check if more pages exist\n",
    "            page_info = response_json[\"data\"][\"molecularProfiles\"][\"pageInfo\"]\n",
    "            if not page_info[\"hasNextPage\"]:\n",
    "                break\n",
    "            variables[\"after\"] = page_info[\"endCursor\"]\n",
    "        else:\n",
    "            print(\"Error in response:\", response_json.get('errors'))\n",
    "            break\n",
    "\n",
    "print(f\"\\nTotal profiles fetched: {len(all_molecular_profiles)}\")\n",
    "\n",
    "# Convert Data to DataFrame\n",
    "data = []\n",
    "for profile in all_molecular_profiles:\n",
    "    node = profile[\"node\"]\n",
    "    profile_id = node[\"id\"]\n",
    "    name = node[\"name\"]\n",
    "    description = node[\"description\"]\n",
    "    score = node[\"molecularProfileScore\"]\n",
    "    \n",
    "    variants = \"; \".join([v[\"name\"] for v in node.get(\"variants\", [])])\n",
    "    \n",
    "    assertions = []\n",
    "    for assertion in node.get(\"assertions\", {}).get(\"nodes\", []):\n",
    "        assertion_id = assertion[\"id\"]\n",
    "        assertion_name = assertion[\"name\"]\n",
    "        assertion_desc = assertion[\"description\"]\n",
    "        disease_name = assertion[\"disease\"][\"name\"] if assertion[\"disease\"] else \"N/A\"\n",
    "        assertions.append(f\"{assertion_id}: {assertion_name} ({disease_name})\")\n",
    "    \n",
    "    assertions_text = \" | \".join(assertions)\n",
    "    \n",
    "    data.append([profile_id, name, description, score, variants, assertions_text])\n",
    "\n",
    "# Create DataFrame with a specific variable name\n",
    "molecular_profiles_df = pd.DataFrame(\n",
    "    data, columns=[\"ID\", \"Name\", \"Description\", \"Score\", \"Variants\", \"Assertions\"]\n",
    ")\n",
    "\n",
    "csv_filename = \"molecular_profiles_CIVIC.csv\"\n",
    "molecular_profiles_df.to_csv(csv_filename, index=False)\n",
    "print(f\"CSV saved as {csv_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0c3025",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extract genes as CSV\n",
    "\n",
    "# API Endpoint and Headers\n",
    "url = \"https://civicdb.org/api/graphql\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "# GraphQL Query for Genes\n",
    "query = \"\"\"\n",
    "query browseGenes($after: String) {\n",
    "    genes(first: 300, after: $after) {\n",
    "        nodes {\n",
    "            id\n",
    "            name\n",
    "            description\n",
    "            variants {\n",
    "                nodes {\n",
    "                    id\n",
    "                    name\n",
    "                    molecularProfiles {\n",
    "                        nodes {\n",
    "                            id\n",
    "                            name\n",
    "                            description\n",
    "                            assertions {\n",
    "                                nodes {\n",
    "                                    id\n",
    "                                    name\n",
    "                                    description\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        pageInfo {\n",
    "            endCursor\n",
    "            hasNextPage\n",
    "        }\n",
    "        totalCount\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Fetch total count first for progress bar\n",
    "response = requests.post(url, json={'query': query, 'variables': {\"after\": None}}, headers=headers)\n",
    "response_json = response.json()\n",
    "total_genes = response_json.get(\"data\", {}).get(\"genes\", {}).get(\"totalCount\", 0)\n",
    "\n",
    "# Initialize variables for fetching data\n",
    "all_genes = []\n",
    "variables = {\"after\": None}\n",
    "\n",
    "# Progress Bar\n",
    "with tqdm(total=total_genes, desc=\"Fetching Genes\", unit=\"genes\") as pbar:\n",
    "    while True:\n",
    "        response = requests.post(url, json={'query': query, 'variables': variables}, headers=headers)\n",
    "        response_json = response.json()\n",
    "        \n",
    "        if 'data' in response_json:\n",
    "            genes = response_json[\"data\"][\"genes\"][\"nodes\"]\n",
    "            all_genes.extend(genes)\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.update(len(genes))\n",
    "\n",
    "            # Check if more pages exist\n",
    "            page_info = response_json[\"data\"][\"genes\"][\"pageInfo\"]\n",
    "            if not page_info[\"hasNextPage\"]:\n",
    "                break\n",
    "            variables[\"after\"] = page_info[\"endCursor\"]\n",
    "        else:\n",
    "            print(\"Error in response:\", response_json.get('errors'))\n",
    "            break\n",
    "\n",
    "print(f\"\\nTotal genes fetched: {len(all_genes)}\")\n",
    "\n",
    "# Convert Data to DataFrame\n",
    "data = []\n",
    "for gene in all_genes:\n",
    "    gene_id = gene[\"id\"]\n",
    "    gene_name = gene[\"name\"]\n",
    "    gene_description = gene[\"description\"]\n",
    "    \n",
    "    # Handle nested variants\n",
    "    for variant in gene.get(\"variants\", {}).get(\"nodes\", []):\n",
    "        variant_id = variant[\"id\"]\n",
    "        variant_name = variant[\"name\"]\n",
    "        \n",
    "        # Handle nested molecular profiles\n",
    "        for profile in variant.get(\"molecularProfiles\", {}).get(\"nodes\", []):\n",
    "            profile_id = profile[\"id\"]\n",
    "            profile_name = profile[\"name\"]\n",
    "            profile_desc = profile[\"description\"]\n",
    "\n",
    "            # Handle assertions\n",
    "            assertions = []\n",
    "            for assertion in profile.get(\"assertions\", {}).get(\"nodes\", []):\n",
    "                assertion_id = assertion[\"id\"]\n",
    "                assertion_name = assertion[\"name\"]\n",
    "                assertion_desc = assertion[\"description\"]\n",
    "                assertions.append(f\"{assertion_id}: {assertion_name} ({assertion_desc})\")\n",
    "\n",
    "            assertions_text = \" | \".join(assertions) if assertions else \"None\"\n",
    "\n",
    "            # Append row to dataset\n",
    "            data.append([\n",
    "                gene_id, gene_name, gene_description,\n",
    "                variant_id, variant_name,\n",
    "                profile_id, profile_name, profile_desc,\n",
    "                assertions_text\n",
    "            ])\n",
    "\n",
    "# Create DataFrame with a meaningful name\n",
    "genes_df = pd.DataFrame(data, columns=[\n",
    "    \"Gene ID\", \"Gene Name\", \"Gene Description\",\n",
    "    \"Variant ID\", \"Variant Name\",\n",
    "    \"Molecular Profile ID\", \"Molecular Profile Name\", \"Molecular Profile Description\",\n",
    "    \"Assertions\"\n",
    "])\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "csv_filename = \"genes_CIVIC.csv\"\n",
    "genes_df.to_csv(csv_filename, index=False)\n",
    "print(f\"CSV saved as {csv_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c02ba6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging datasets of variants and molecular profiles\n",
    "\n",
    "variants_df = pd.read_csv(\"variants_and_aliases_CIVIC.csv\")\n",
    "print(f\"Length of variants_df: {len(variants_df):,}\")\n",
    "\n",
    "molecular_profiles_df = pd.read_csv(\"molecular_profiles_CIVIC.csv\")\n",
    "print(f\"Length of molecular_profiles_df: {len(molecular_profiles_df):,}\")\n",
    "\n",
    "molecular_profiles_df.rename(columns={\"ID\": \"Variant ID\"}, inplace=True)\n",
    "CIVIC_variant_merged_df = variants_df.merge(molecular_profiles_df, on=\"Variant ID\", how=\"outer\")\n",
    "merged_csv_filename = \"CIVIC_merged_variants_molprofiles.csv\"\n",
    "CIVIC_variant_merged_df.to_csv(merged_csv_filename, index=False)\n",
    "print(f\"Merged CSV saved as: {merged_csv_filename}\")\n",
    "print(f\"Total rows: {len(CIVIC_variant_merged_df):,}\")\n",
    "\n",
    "selected_columns = [\"Variant Name\", \"Variants\", \"Name\", \"Aliases\"]\n",
    "relevant_columns = [col for col in selected_columns if col in CIVIC_variant_merged_df.columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b16cd38",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Merging with gene dataframe\n",
    "\n",
    "CIVIC_variant_merged_df = pd.read_csv(\"CIVIC_merged_variants_molprofiles.csv\")\n",
    "genes_df = pd.read_csv(\"genes_CIVIC.csv\")\n",
    "genes_selected_columns = [\"Gene ID\", \"Gene Name\", \"Gene Description\", \"Molecular Profile Name\", \"Molecular Profile ID\"]\n",
    "genes_df = genes_df[genes_selected_columns]\n",
    "CIVIC_final_merged_df = CIVIC_variant_merged_df.merge(\n",
    "    genes_df, \n",
    "    left_on=[\"Name\", \"Variant ID\"], \n",
    "    right_on=[\"Molecular Profile Name\", \"Molecular Profile ID\"], \n",
    "    how=\"left\"\n",
    ")\n",
    "CIVIC_final_merged_df.drop(columns=[\"Molecular Profile Name\", \"Molecular Profile ID\"], inplace=True)\n",
    "final_merged_csv_filename = \"CIVIC_merged_variants_molprofiles_genes.csv\"\n",
    "CIVIC_final_merged_df.to_csv(final_merged_csv_filename, index=False)\n",
    "\n",
    "print(f\"Final merged CSV saved as: {final_merged_csv_filename}\")\n",
    "print(f\"Total rows: {len(CIVIC_final_merged_df):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c617c65",
   "metadata": {},
   "source": [
    "# Normalize with CIVIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2c5668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing binary matrix and matching with CIVIC data\n",
    "\n",
    "# Load datasets\n",
    "CIVIC_merged_df = pd.read_csv(\"CIVIC_merged_variants_molprofiles_genes.csv\")\n",
    "matrix_file = \"filtered_variant_matrix_df.csv\"\n",
    "final_variant_matrix_df = pd.read_csv(matrix_file, index_col=0, low_memory=False)\n",
    "print(f\"Loaded matrix with {len(final_variant_matrix_df):,} rows and {len(final_variant_matrix_df.columns):,} columns.\")\n",
    "\n",
    "# Remove columns starting with \"Variant_\" or \"Gene_\"\n",
    "columns_to_remove = [col for col in final_variant_matrix_df.columns if col.startswith(\"Variant_\") or col.startswith(\"Gene_\")]\n",
    "if columns_to_remove:\n",
    "    final_variant_matrix_df.drop(columns=columns_to_remove, inplace=True)\n",
    "    print(f\"\\nRemoved {len(columns_to_remove)} columns starting with 'Variant_' or 'Gene_'.\")\n",
    "\n",
    "# Select and detect valid binary columns\n",
    "metadata_columns = [\n",
    "    \"PaperId\", \"PaperTitle\", \"Abstract\", \"LLM_Prompt\", \"LLM_Response\",\n",
    "    \"Variant_Gene_Pairs\", \"Cleaned_Variant_Gene_Pairs\", \"total_variant_count\"\n",
    "]\n",
    "binary_columns = [\n",
    "    col for col in final_variant_matrix_df.columns \n",
    "    if col not in metadata_columns\n",
    "    and isinstance(col, str)\n",
    "    and re.match(r\"^[a-zA-Z0-9\\*\\.\\-]+_[A-Z0-9]+$\", col)\n",
    "]\n",
    "print(f\"\\nDetected {len(binary_columns):,} binary columns for processing.\")\n",
    "\n",
    "# Convert binary columns to numeric (0s and 1s)\n",
    "for col in binary_columns:\n",
    "    final_variant_matrix_df[col] = pd.to_numeric(final_variant_matrix_df[col], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "# Sum the binary columns and total count\n",
    "binary_matrix_sums = final_variant_matrix_df[binary_columns].sum()\n",
    "total_binary_matrix_count = binary_matrix_sums.sum()\n",
    "print(f\"\\nTotal mentions across all variants: {total_binary_matrix_count:,}\")\n",
    "\n",
    "# Extract and process variant names\n",
    "extracted_variants = binary_matrix_sums.index.to_series().apply(lambda x: re.split(r'_(?=[^_]+$)', x)[0])\n",
    "print(\"\\nSample of extracted variant names:\")\n",
    "print(extracted_variants.head(20))\n",
    "\n",
    "# Match with CIVIC dataset\n",
    "extracted_variant_set = {variant.lower() for variant in extracted_variants.tolist()}\n",
    "matches = []\n",
    "for _, row in CIVIC_merged_df.iterrows():\n",
    "    variant_name = str(row[\"Variant Name\"]).strip().lower() if pd.notna(row[\"Variant Name\"]) else \"\"\n",
    "    aliases = row[\"Aliases\"] if pd.notna(row[\"Aliases\"]) else \"\"\n",
    "    alias_list = [alias.strip().lower() for alias in aliases.split(\", \") if alias]\n",
    "    matched_aliases = [alias for alias in alias_list if alias in extracted_variant_set]\n",
    "    if variant_name in extracted_variant_set or matched_aliases:\n",
    "        matches.append({\n",
    "            \"Variant Name\": variant_name,\n",
    "            \"Matched Aliases\": \", \".join(matched_aliases)\n",
    "        })\n",
    "\n",
    "matched_variants_df = pd.DataFrame(matches)\n",
    "print(f\"\\nNumber of matches found: {len(matched_variants_df)}\")\n",
    "print(\"\\nPreview of matched variants:\")\n",
    "print(matched_variants_df.head(50))\n",
    "matched_variants_df.to_csv(\"matched_variants_and_aliases.csv\", index=False)\n",
    "print(\"\\nMatched variants saved to 'matched_variants_and_aliases.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb36474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization and merging of CIViC Synonyms\n",
    "matrix_file = \"filtered_variant_matrix_df.csv\"\n",
    "matches_file = \"matched_variants_and_aliases.csv\"\n",
    "if not os.path.exists(matrix_file):\n",
    "    print(f\"\\nError: The file '{matrix_file}' was not found. Please upload it.\")\n",
    "elif not os.path.exists(matches_file):\n",
    "    print(f\"\\nError: The file '{matches_file}' was not found. Please upload it.\")\n",
    "else:\n",
    "    final_variant_matrix_df = pd.read_csv(matrix_file, index_col=0, low_memory=False)\n",
    "    matched_variants_df = pd.read_csv(matches_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0719448b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New normalization process\n",
    "\n",
    "# Extract prefix and suffix for each column\n",
    "extracted_column_variants = {}\n",
    "column_suffixes = {}\n",
    "for col in final_variant_matrix_df.columns:\n",
    "    parts = col.rsplit('_', 1)\n",
    "    if len(parts) == 2:\n",
    "        extracted_column_variants[col] = parts[0]\n",
    "        column_suffixes[col] = parts[1]\n",
    "    else:\n",
    "        extracted_column_variants[col] = col\n",
    "        column_suffixes[col] = \"\"\n",
    "\n",
    "original_column_count = final_variant_matrix_df.shape[1]\n",
    "\n",
    "# Build merge dictionary and alias dictionary\n",
    "merge_dict = {}\n",
    "alias_dict = []\n",
    "for _, row in matched_variants_df.iterrows():\n",
    "    variant_name = str(row[\"Variant Name\"]).strip() if pd.notna(row[\"Variant Name\"]) else \"\"\n",
    "    matched_aliases = []\n",
    "    if pd.notna(row[\"Matched Aliases\"]):\n",
    "        matched_aliases = [alias.strip() for alias in str(row[\"Matched Aliases\"]).split(\",\") if alias.strip()]\n",
    "\n",
    "    adjusted_variant_name = variant_name.lower()\n",
    "    adjusted_matched_aliases = [alias.lower() for alias in matched_aliases]\n",
    "\n",
    "    actual_variant_col = [\n",
    "        col for col, short_col in extracted_column_variants.items()\n",
    "        if short_col.lower() == adjusted_variant_name\n",
    "    ]\n",
    "    actual_alias_cols = [\n",
    "        col for col, short_col in extracted_column_variants.items()\n",
    "        if short_col.lower() in adjusted_matched_aliases\n",
    "    ]\n",
    "\n",
    "    if actual_variant_col and actual_alias_cols:\n",
    "        keep_col = actual_variant_col[0]\n",
    "        merge_dict.setdefault(keep_col, []).extend(actual_alias_cols)\n",
    "        for alias_col in actual_alias_cols:\n",
    "            alias_dict.append({\"Kept Name\": keep_col, \"Dropped Alias\": alias_col})\n",
    "\n",
    "# Merge candidates before suffix filtering\n",
    "print(\"\\nMerge Candidates Before Suffix Filtering:\")\n",
    "for keep_col, alias_cols in merge_dict.items():\n",
    "    print(f\"{keep_col}: {alias_cols}\")\n",
    "\n",
    "# Track columns to drop after merging\n",
    "merged_columns = set()\n",
    "\n",
    "# Merging alias columns with suffix filtering\n",
    "print(\"\\nMerging alias columns with suffix filtering...\")\n",
    "for keep_col in tqdm(merge_dict, desc=\"Merging columns\"):\n",
    "    merge_cols = [col for col in merge_dict[keep_col] if col in final_variant_matrix_df.columns and col != keep_col]\n",
    "\n",
    "    if not merge_cols:\n",
    "        continue\n",
    "\n",
    "    keep_col_suffix = column_suffixes.get(keep_col, \"\")\n",
    "    valid_merge_cols = [col for col in merge_cols if column_suffixes.get(col, \"\") == keep_col_suffix]\n",
    "\n",
    "    if not valid_merge_cols:\n",
    "        print(f\"Skipping merge for {keep_col}: No alias matches suffix '{keep_col_suffix}'\")\n",
    "        continue\n",
    "\n",
    "    final_variant_matrix_df[keep_col] = final_variant_matrix_df[[keep_col] + valid_merge_cols].max(axis=1)\n",
    "    merged_columns.update(valid_merge_cols)\n",
    "    print(f\"Merged {valid_merge_cols} into {keep_col}\")\n",
    "\n",
    "# Drop merged alias columns\n",
    "final_variant_matrix_df.drop(columns=merged_columns, inplace=True)\n",
    "\n",
    "# Optional: Sort columns alphabetically\n",
    "final_variant_matrix_df = final_variant_matrix_df.reindex(sorted(final_variant_matrix_df.columns), axis=1)\n",
    "\n",
    "new_column_count = final_variant_matrix_df.shape[1]\n",
    "columns_merged = original_column_count - new_column_count\n",
    "\n",
    "# Save outputs\n",
    "output_file = \"normalized_merged_variant_matrix_v2.csv\"\n",
    "final_variant_matrix_df.to_csv(output_file)\n",
    "\n",
    "alias_dict_df = pd.DataFrame(alias_dict)\n",
    "alias_dict_filename = \"alias_dictionary_v2.csv\"\n",
    "alias_dict_df.to_csv(alias_dict_filename, index=False)\n",
    "\n",
    "# Final summary\n",
    "print(f\"\\nMerged binary matrix saved as '{output_file}'\")\n",
    "print(f\"Original column count: {original_column_count}\")\n",
    "print(f\"New column count: {new_column_count}\")\n",
    "print(f\"Total columns merged: {columns_merged}\")\n",
    "print(f\"Alias dictionary saved to: {alias_dict_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9032aa5c",
   "metadata": {},
   "source": [
    "# Normlaization #3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6647fb",
   "metadata": {},
   "source": [
    "## Fetch only once and resume afterwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b5e9fc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Connect to CliVar for HGSV notations\n",
    "file_name = \"CIVIC_merged_variants_molprofiles_genes.csv\"\n",
    "CIVIC_df = pd.read_csv(file_name)\n",
    "\n",
    "# Extract rsID from \"Variant Name\" and \"Aliases\"\n",
    "def extract_rsID(variant_name, aliases):\n",
    "    \"\"\"Extract rsID from 'Variant Name' or 'Aliases'.\"\"\"\n",
    "    rs_pattern = r\"rs\\d+\"  # Pattern to find rsIDs (e.g., rs121434568)\n",
    "\n",
    "    # Search in \"Variant Name\"\n",
    "    if isinstance(variant_name, str):\n",
    "        match = re.search(rs_pattern, variant_name, re.IGNORECASE)\n",
    "        if match:\n",
    "            return match.group(0).lower()  # Convert to lowercase\n",
    "\n",
    "    # Search in \"Aliases\"\n",
    "    if isinstance(aliases, str):\n",
    "        matches = re.findall(rs_pattern, aliases, re.IGNORECASE)\n",
    "        if matches:\n",
    "            return matches[0].lower()  # Convert to lowercase\n",
    "\n",
    "    return \"\"  # Return empty if no rsID found\n",
    "\n",
    "# Apply function to extract rsID\n",
    "tqdm.pandas()\n",
    "CIVIC_df[\"rsID\"] = CIVIC_df.progress_apply(\n",
    "    lambda row: extract_rsID(row[\"Variant Name\"], row[\"Aliases\"]), axis=1\n",
    ")\n",
    "\n",
    "# Save updated file with rsID column\n",
    "updated_file = \"CIVIC_with_rsID.csv\"\n",
    "CIVIC_df.to_csv(updated_file, index=False)\n",
    "\n",
    "print(\"\\n--- Updated Data Sample with rsID (Lowercase) ---\")\n",
    "print(CIVIC_df.head())\n",
    "\n",
    "# Get ClinVar ID\n",
    "def get_clinvar_id(rsid):\n",
    "    \"\"\"Fetch ClinVar ID using rsID from ClinVar API.\"\"\"\n",
    "    if pd.isna(rsid) or rsid.strip() == \"\" or rsid.lower() == \"nan\":\n",
    "        return None, None  # Keep empty\n",
    "\n",
    "    url = f\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=clinvar&term={rsid}&retmode=json\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        if \"esearchresult\" in data and \"idlist\" in data[\"esearchresult\"]:\n",
    "            id_list = data[\"esearchresult\"][\"idlist\"]\n",
    "            if len(id_list) > 0:\n",
    "                return rsid, id_list[0]\n",
    "\n",
    "    return rsid, None\n",
    "\n",
    "# Get HGVS from ClinVar\n",
    "def get_hgvs_from_clinvar_id(clinvar_id):\n",
    "    \"\"\"Retrieve HGVS notation from ClinVar using ClinVar ID.\"\"\"\n",
    "    if not clinvar_id:\n",
    "        return None\n",
    "\n",
    "    url = f\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi?db=clinvar&id={clinvar_id}&retmode=json\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        if \"result\" in data and clinvar_id in data[\"result\"]:\n",
    "            record = data[\"result\"][clinvar_id]\n",
    "\n",
    "            # Extract HGVS notation\n",
    "            hgvs_transcript = record.get(\"title\", \"\").split(\" \")[0] if \"title\" in record else None\n",
    "            hgvs_protein = record.get(\"variation_name\", \"\") if \"variation_name\" in record else None\n",
    "\n",
    "            return f\"{hgvs_transcript} | {hgvs_protein}\" if hgvs_protein else hgvs_transcript\n",
    "\n",
    "    return None\n",
    "\n",
    "# Get HGVS from Ensembl (Backup)\n",
    "def get_hgvs_from_ensembl(rsid):\n",
    "    \"\"\"Fetch HGVS notation from Ensembl if ClinVar is missing.\"\"\"\n",
    "    if pd.isna(rsid) or rsid.strip() == \"\" or rsid.lower() == \"nan\":\n",
    "        return None  # Keep empty\n",
    "\n",
    "    url = f\"https://rest.ensembl.org/variant_recoder/human/{rsid}?content-type=application/json\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        hgvs_list = set()\n",
    "        for variant in data:\n",
    "            if \"hgvsg\" in variant:\n",
    "                hgvs_list.add(variant[\"hgvsg\"])\n",
    "\n",
    "        return \", \".join(hgvs_list) if hgvs_list else None\n",
    "\n",
    "    return None\n",
    "\n",
    "# Fetch HGVS from ClinVar & Ensembl\n",
    "def fetch_hgvs(rsid, clinvar_id):\n",
    "    \"\"\"First tries ClinVar, then Ensembl if ClinVar HGVS is missing.\"\"\"\n",
    "    if pd.isna(rsid) or rsid.strip() == \"\" or rsid.lower() == \"nan\":\n",
    "        return None \n",
    "    clinvar_hgvs = get_hgvs_from_clinvar_id(clinvar_id)\n",
    "    if not clinvar_hgvs:\n",
    "        return get_hgvs_from_ensembl(rsid)\n",
    "    return clinvar_hgvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0f6d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply API calls to dataset\n",
    "print(\"Fetching ClinVar IDs...\")\n",
    "tqdm.pandas()\n",
    "CIVIC_df[[\"rsID_Confirmed\", \"ClinVar_ID\"]] = \\\n",
    "    CIVIC_df[\"rsID\"].progress_apply(lambda r: pd.Series(get_clinvar_id(str(r))))\n",
    "\n",
    "print(\"Fetching HGVS Notation from ClinVar and Ensembl...\")\n",
    "CIVIC_df[\"HGVS_Notation\"] = \\\n",
    "    CIVIC_df.apply(lambda row: fetch_hgvs(str(row[\"rsID\"]), str(row[\"ClinVar_ID\"])), axis=1)\n",
    "\n",
    "# Extract variant description\n",
    "def extract_variant_description(hgvs_notation):\n",
    "    \"\"\"Extracts the variant notation (e.g., c.944C>T) from the HGVS string.\"\"\"\n",
    "    if pd.isna(hgvs_notation):\n",
    "        return None \n",
    "    parts = hgvs_notation.split(\"|\")\n",
    "    if len(parts) > 0:\n",
    "        variant = parts[0].split(\":\")[-1].strip()\n",
    "        return variant\n",
    "    return None\n",
    "CIVIC_df[\"Variant_Description\"] = CIVIC_df[\"HGVS_Notation\"].apply(extract_variant_description)\n",
    "\n",
    "# Clean data\n",
    "CIVIC_df[\"HGVS_Notation\"] = CIVIC_df[\"HGVS_Notation\"].str.replace(\" | None\", \"\", regex=False)\n",
    "CIVIC_df[\"HGVS_Notation\"] = CIVIC_df[\"HGVS_Notation\"].replace({\"None\": \"\", \"No HGVS available\": \"\"})\n",
    "CIVIC_ClinVar_df=CIVIC_df.copy()\n",
    "final_file = \"CIVIC_with_ClinVar_HGVS.csv\"\n",
    "CIVIC_ClinVar_df.to_csv(final_file, index=False)\n",
    "print(\"\\nProcessing Complete! Final File Saved:\", final_file)\n",
    "print(\"\\n--- Final Updated Data Sample with HGVS & Variant Description ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ba799e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Merge aliases, HGVS and variant descirption for matching of columns\n",
    "file_name = \"CIVIC_with_ClinVar_HGVS.csv\"\n",
    "CIVIC_ClinVar_df = pd.read_csv(file_name)\n",
    "required_columns = [\"Aliases\", \"HGVS_Notation\", \"Variant_Description\"]\n",
    "existing_columns = [col for col in required_columns if col in CIVIC_ClinVar_df.columns]\n",
    "\n",
    "if len(existing_columns) == len(required_columns):\n",
    "    CIVIC_ClinVar_df[\"Aliases_Merged\"] = CIVIC_ClinVar_df[[\"Aliases\", \"HGVS_Notation\", \"Variant_Description\"]].apply(\n",
    "        lambda row: \", \".join(row.dropna().astype(str)), axis=1\n",
    "    )\n",
    "\n",
    "    final_file = \"CIVIC_ClinVar_merged.csv\"\n",
    "    CIVIC_ClinVar_df.to_csv(final_file, index=False)\n",
    "    print(\"\\n Processing Complete! Final File Saved:\", final_file)\n",
    "    print(f\"\\n Length of dataset: {len(CIVIC_ClinVar_df):,}\")\n",
    "    print(\"\\n--- Sample of Updated Data with 'Aliases_Merged' ---\")\n",
    "    print(CIVIC_ClinVar_df[[\"Aliases\", \"HGVS_Notation\", \"Variant_Description\", \"Aliases_Merged\"]].head())\n",
    "else:\n",
    "    print(\" Error: Some required columns are missing in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9386f2c1",
   "metadata": {},
   "source": [
    "# Resume here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038eaf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_file = \"normalized_merged_variant_matrix_v2.csv\"\n",
    "matches_file = \"CIVIC_ClinVar_merged.csv\"\n",
    "if not os.path.exists(matrix_file):\n",
    "    print(f\"\\nError: The file '{matrix_file}' was not found.\")\n",
    "elif not os.path.exists(matches_file):\n",
    "    print(f\"\\nError: The file '{matches_file}' was not found.\")\n",
    "else:\n",
    "    print(\"Loading datasets...\")\n",
    "    final_variant_matrix_df = pd.read_csv(matrix_file, index_col=0, low_memory=False)\n",
    "    matched_variants_df = pd.read_csv(matches_file)\n",
    "\n",
    "print(\"Length of dataset variant matrix\",len(final_variant_matrix_df))\n",
    "print(\"Length of datasets with matches\",len(matched_variants_df))\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2549a754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize and merge variant columns based on aliases and suffixes\n",
    "\n",
    "# Extract prefix and suffix\n",
    "extracted_column_variants = {}\n",
    "column_suffixes = {}\n",
    "\n",
    "for col in final_variant_matrix_df.columns:\n",
    "    parts = col.rsplit('_', 1)\n",
    "    if len(parts) == 2:\n",
    "        extracted_column_variants[col] = parts[0]\n",
    "        column_suffixes[col] = parts[1]\n",
    "    else:\n",
    "        extracted_column_variants[col] = col\n",
    "        column_suffixes[col] = \"\"\n",
    "\n",
    "# Prepare for merging aliases\n",
    "merge_dict = {}\n",
    "alias_dict = []\n",
    "\n",
    "for _, row in matched_variants_df.iterrows():\n",
    "    variant_name = str(row[\"Variant Name\"]).strip() if pd.notna(row[\"Variant Name\"]) else \"\"\n",
    "    matched_aliases = [alias.strip() for alias in str(row[\"Aliases_Merged\"]).split(\",\") if pd.notna(row[\"Aliases_Merged\"])]\n",
    "    \n",
    "    adjusted_variant_name = variant_name.lower()\n",
    "    adjusted_matched_aliases = [alias.lower() for alias in matched_aliases]\n",
    "\n",
    "    actual_variant_col = [col for col, short_col in extracted_column_variants.items() if short_col.lower() == adjusted_variant_name]\n",
    "    actual_alias_cols = [col for col, short_col in extracted_column_variants.items() if short_col.lower() in adjusted_matched_aliases]\n",
    "\n",
    "    if actual_variant_col and actual_alias_cols:\n",
    "        keep_col = actual_variant_col[0]\n",
    "        merge_dict.setdefault(keep_col, []).extend(actual_alias_cols)\n",
    "\n",
    "        for alias_col in actual_alias_cols:\n",
    "            alias_dict.append({\"Kept Name\": keep_col, \"Dropped Alias\": alias_col})\n",
    "\n",
    "# Merge alias columns\n",
    "merged_columns = set()\n",
    "\n",
    "for keep_col, merge_cols in merge_dict.items():\n",
    "    merge_cols = [col for col in merge_cols if col in final_variant_matrix_df.columns]\n",
    "\n",
    "    if not merge_cols:\n",
    "        continue\n",
    "\n",
    "    keep_col_suffix = column_suffixes.get(keep_col, \"\")\n",
    "    valid_merge_cols = [col for col in merge_cols if column_suffixes.get(col, \"\") == keep_col_suffix]\n",
    "\n",
    "    if not valid_merge_cols:\n",
    "        print(f\"Skipping merge for {keep_col}: No alias matches suffix '{keep_col_suffix}'\")\n",
    "        continue\n",
    "\n",
    "    final_variant_matrix_df[keep_col] = final_variant_matrix_df[[keep_col] + valid_merge_cols].max(axis=1)\n",
    "    merged_columns.update(valid_merge_cols)\n",
    "    print(f\"Merged {valid_merge_cols} into {keep_col}\")\n",
    "\n",
    "# Finalize the merged dataset\n",
    "final_variant_matrix_df.drop(columns=merged_columns, inplace=True)\n",
    "output_file = \"normalized_merged_variant_matrix_v3.csv\"\n",
    "final_variant_matrix_df.to_csv(output_file)\n",
    "\n",
    "alias_dict_df = pd.DataFrame(alias_dict)\n",
    "alias_dict_filename = \"alias_dictionary_v3.csv\"\n",
    "alias_dict_df.to_csv(alias_dict_filename, index=False)\n",
    "\n",
    "# Print summary\n",
    "print(f\"\\nMerged binary matrix saved as '{output_file}'!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2688f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final output file: normalized_merged_variant_matrix_v3.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7477c53d",
   "metadata": {},
   "source": [
    "# ================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd09ad68",
   "metadata": {},
   "source": [
    "# Look at variant mention statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dc7812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gene mentions statistics\n",
    "os.chdir(output_directory)\n",
    "cancer_df = pd.read_csv(\"binary_cancer_matrix_filtered.csv\")  \n",
    "\n",
    "os.chdir(LLM_directory)\n",
    "normalized_merged_variant_matrix_v3 = pd.read_csv(\"normalized_merged_variant_matrix_v3.csv\", index_col=0)\n",
    "final_variant_matrix_df = normalized_merged_variant_matrix_v3\n",
    "\n",
    "print(\"Length of cancer dataset:\", len(cancer_df))\n",
    "print(\"Columns of cancer dataset:\", len(cancer_df.columns))\n",
    "\n",
    "print(\"Final_variant_matrix_df loaded\")\n",
    "print(\"Length of matrix:\", len(final_variant_matrix_df))\n",
    "print(\"Columns of matrix:\", len(final_variant_matrix_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4572b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the set of valid PaperIds from cancer_df\n",
    "valid_paper_ids = set(cancer_df['PaperId'])\n",
    "\n",
    "# Initial length of final_variant_matrix_df before filtering\n",
    "initial_length = len(final_variant_matrix_df)\n",
    "filtered_final_variant_matrix_df = final_variant_matrix_df.loc[final_variant_matrix_df.index.isin(valid_paper_ids)]\n",
    "final_length = len(filtered_final_variant_matrix_df)\n",
    "print(f\"\\nFiltering completed successfully.\")\n",
    "print(f\"Initial number of rows in final_variant_matrix_df: {initial_length:,}\")\n",
    "print(f\"Number of rows after filtering: {final_length:,}\")\n",
    "filtered_final_variant_matrix_df.to_csv(\"normalized_merged_variant_matrix_v4.csv\")\n",
    "print(\"\\nFile 'normalized_merged_variant_matrix_v4.csv' has been successfully saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8c279a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a copy of the original DataFrame to avoid overwriting\n",
    "final_variant_matrix_df=filtered_final_variant_matrix_df.copy()\n",
    "variant_matrix_with_counts = final_variant_matrix_df.copy()\n",
    "\n",
    "if 'total_variant_count' in variant_matrix_with_counts.columns:\n",
    "    variant_matrix_with_counts = variant_matrix_with_counts.drop(columns=['total_variant_count'])\n",
    "    print(\"Existing 'total_variant_count' column removed from the new DataFrame.\")\n",
    "\n",
    "# Convert all columns except 'PaperId' to numeric, forcing errors to NaN\n",
    "variant_matrix_with_counts_numeric = variant_matrix_with_counts.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Calculate the new 'total_variant_count' as the row-wise sum, ignoring non-numeric columns\n",
    "variant_matrix_with_counts['total_variant_count'] = variant_matrix_with_counts_numeric.sum(axis=1, skipna=True)\n",
    "print(\"New 'total_variant_count' column added to the new DataFrame.\")\n",
    "\n",
    "value_counts = variant_matrix_with_counts['total_variant_count'].value_counts().sort_index()\n",
    "\n",
    "print(\"Counts of rows by 'total_variant_count' value:\")\n",
    "for value, count in value_counts.items():\n",
    "    print(f\"Rows with value {int(value)}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e3909d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Total number of screened articles and variant hits percentage\n",
    "screened_articles = len(cancer_df)\n",
    "variant_hits_percentage = (len(final_variant_matrix_df) / screened_articles) * 100\n",
    "\n",
    "print(\"Final_variant_matrix_df loaded\")\n",
    "print(f\"Length of matrix: {len(final_variant_matrix_df):,}\")\n",
    "print(f\"Total screened articles: {screened_articles:,}\")\n",
    "print(f\"Percentage of variant hits: {variant_hits_percentage:.2f}%\")\n",
    "\n",
    "# Prepare data by excluding metadata columns and processing binary columns\n",
    "metadata_columns = [\n",
    "    \"PaperId\", \"PaperTitle\", \"Abstract\", \"LLM_Prompt\", \"LLM_Response\",\n",
    "    \"Variant_Gene_Pairs\", \"Cleaned_Variant_Gene_Pairs\", \"total_variant_count\"\n",
    "]\n",
    "binary_columns = [col for col in final_variant_matrix_df.columns if col not in metadata_columns]\n",
    "\n",
    "final_variant_matrix_df[binary_columns] = final_variant_matrix_df[binary_columns].apply(pd.to_numeric, errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "# Sum occurrences, remove rows with all '0's, and print stats\n",
    "binary_matrix_sums = final_variant_matrix_df[binary_columns].sum()\n",
    "total_binary_matrix_count = binary_matrix_sums.sum()\n",
    "\n",
    "initial_row_count = len(final_variant_matrix_df)\n",
    "final_variant_matrix_df = final_variant_matrix_df.loc[(final_variant_matrix_df[binary_columns].sum(axis=1) > 0)]\n",
    "rows_with_only_zeros = initial_row_count - len(final_variant_matrix_df)\n",
    "\n",
    "print(f\"\\nRows removed (containing only '0's across all columns): {rows_with_only_zeros:,}\")\n",
    "print(f\"Remaining rows after removal: {len(final_variant_matrix_df):,}\")\n",
    "\n",
    "# Sort variants, display statistics, and show top 20 columns\n",
    "sorted_binary_matrix_sums = binary_matrix_sums.sort_index(key=lambda x: x.str.len().astype(str) + x)\n",
    "\n",
    "print(\"\\n----------------- Binary Matrix Statistics -----------------\")\n",
    "print(f\"Total binary matrix columns: {len(binary_columns):,}\")\n",
    "print(f\"Total sum of all binary matrix counts (total 1s): {total_binary_matrix_count:,}\")\n",
    "\n",
    "top_20_columns = binary_matrix_sums.sort_values(ascending=False).head(20)\n",
    "top_20_columns_percentage = (top_20_columns / len(final_variant_matrix_df)) * 100\n",
    "\n",
    "print(\"\\nTop 20 binary matrix columns by count and percentage:\")\n",
    "for column, count, percentage in zip(top_20_columns.index, top_20_columns, top_20_columns_percentage):\n",
    "    print(f\"{column}: {count} mentions, {percentage:.2f}%\")\n",
    "\n",
    "# List top 20 column names and save the data\n",
    "print(\"\\nTop 20 column names with the most counts:\")\n",
    "print(top_20_columns.index.tolist())\n",
    "\n",
    "top_20_csv_path = f\"{output_directory}/Top_20_Variants.csv\"\n",
    "top_20_df.to_csv(top_20_csv_path, index=False)\n",
    "print(f\"\\nTop 20 Variants saved to: {top_20_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7feac14d",
   "metadata": {},
   "source": [
    "# Create figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae5987d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar chart for the top 20 binary matrix variant mentions\n",
    "plt.figure(figsize=(12, 8))\n",
    "ax = top_20_columns.plot(kind='bar', color='#1f20b4')\n",
    "plt.title('Top 20 Most Frequent Variants in the Dataset', fontsize=16)\n",
    "plt.xlabel('Variants', fontsize=14)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.xticks(rotation=45, ha=\"right\", fontsize=12)\n",
    "plt.ylim(0, top_20_columns.max() + 500)\n",
    "for i, v in enumerate(top_20_columns):\n",
    "    ax.text(i, v + 5, f\"{v:,}\", color='#505050', ha='center', rotation=45, fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef1d2af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot top 20 most frequent variant-associated genes in the dataset\n",
    "extracted_genes = binary_matrix_sums.index.to_series().apply(lambda x: re.split(r'_(?=[^_]+$)', x)[-1])\n",
    "extracted_gene_counts = binary_matrix_sums.groupby(extracted_genes).sum().sort_values(ascending=False)\n",
    "top_20_genes = extracted_gene_counts.head(20)\n",
    "plt.figure(figsize=(12, 8))\n",
    "ax = top_20_genes.plot(kind='bar', color='#1f77b4')\n",
    "plt.title('Top 20 Most Frequent Variant-Associated Genes in the Dataset', fontsize=16)\n",
    "plt.xlabel('Genes', fontsize=14)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.xticks(rotation=45, ha=\"right\", fontsize=12)\n",
    "plt.ylim(0, top_20_genes.max() + 500)\n",
    "for i, v in enumerate(top_20_genes):\n",
    "    ax.text(i, v + 100, f\"{v:,}\", color='#505050', ha='center', rotation=45, fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e62b571",
   "metadata": {},
   "source": [
    "# ============================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d348e1b",
   "metadata": {},
   "source": [
    "# Oncomine gene comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8091fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Setting working directory...\")\n",
    "os.chdir(LLM_directory)\n",
    "print(\"Current Working Directory:\", os.getcwd())\n",
    "print(\"Loading CSV data into DataFrame...\")\n",
    "matrix_file = \"normalized_merged_variant_matrix_v4.csv\"\n",
    "final_variant_matrix_df = pd.read_csv(matrix_file, index_col=0, low_memory=False)\n",
    "print(\"All datasets loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecfe7f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### ========= Oncomine matching ========= #####\n",
    "\n",
    "# Identify relevant columns\n",
    "print(\"Identifying relevant columns...\")\n",
    "non_binary_fields = {\n",
    "    \"PaperId\", \"PaperTitle\", \"Abstract\",\n",
    "    \"LLM_Prompt\", \"LLM_Response\",\n",
    "    \"Variant_Gene_Pairs\", \"Cleaned_Variant_Gene_Pairs\",\n",
    "    \"total_variant_count\"\n",
    "}\n",
    "\n",
    "# Get all columns that are NOT in non_binary_fields\n",
    "relevant_columns = [col for col in final_variant_matrix_df.columns if col not in non_binary_fields]\n",
    "\n",
    "print(\"\\n----------------- Relevant Columns -----------------\")\n",
    "print(f\"Total relevant columns: {len(relevant_columns):,}\")\n",
    "print(relevant_columns[:10])\n",
    "\n",
    "# Extract filtered matrix\n",
    "print(\"Extracting relevant matrix columns...\")\n",
    "filtered_matrix = final_variant_matrix_df[relevant_columns]\n",
    "\n",
    "# Extract gene names from column names\n",
    "print(\"Extracting gene names from column names...\")\n",
    "extracted_gene_list = []\n",
    "for col in relevant_columns:\n",
    "    gene = col.split('_')[-1]\n",
    "    extracted_gene_list.append(gene)\n",
    "extracted_genes = pd.Series(extracted_gene_list, index=relevant_columns)\n",
    "\n",
    "print(\" Success: Gene names extracted and columns filtered.\")\n",
    "print(\"Summing extracted gene counts (optimized)...\")\n",
    "\n",
    "gene_counts = defaultdict(float)\n",
    "for col in filtered_matrix.columns:\n",
    "    gene = extracted_genes[col]\n",
    "    try:\n",
    "        col_sum = pd.to_numeric(filtered_matrix[col], errors=\"coerce\").sum()\n",
    "        gene_counts[gene] += col_sum\n",
    "    except Exception as e:\n",
    "        print(f\" Skipping column {col} due to error: {e}\")\n",
    "extracted_gene_counts = pd.Series(gene_counts).fillna(0).sort_values(ascending=False)\n",
    "total_extracted_gene_mentions = extracted_gene_counts.sum()\n",
    "print(\"\\n----------------- Extracted Gene Counts -----------------\")\n",
    "print(f\"Total unique extracted genes: {len(extracted_gene_counts):,}\")\n",
    "print(f\"Total extracted gene mentions: {total_extracted_gene_mentions:,}\\n\")\n",
    "print(extracted_gene_counts.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbce97a1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Gene matching and data integrity check with oncomine list\n",
    "os.chdir(input_directory)\n",
    "genes_file = \"oncomine_ngs_panel.csv\"\n",
    "genes = pd.read_csv(genes_file, header=None)\n",
    "gene_list = set(genes[0].tolist())\n",
    "len_gene_list = len(gene_list)\n",
    "print(\"Genes import successful!\")\n",
    "print(f\"Number of Oncomine genes: {len(gene_list):,}\")\n",
    "os.chdir(LLM_directory)\n",
    "\n",
    "# Compare extracted genes with oncomine\n",
    "matching_genes = extracted_gene_counts[extracted_gene_counts.index.isin(gene_list)]\n",
    "non_matching_genes = extracted_gene_counts[~extracted_gene_counts.index.isin(gene_list)]\n",
    "num_matching_genes = len(matching_genes)\n",
    "num_non_matching_genes = len(non_matching_genes)\n",
    "matching_gene_names = list(matching_genes.index)\n",
    "non_matching_gene_names = list(non_matching_genes.index)\n",
    "total_matching_mentions = matching_genes.sum().sum()\n",
    "total_non_matching_mentions = non_matching_genes.sum().sum()\n",
    "total_extracted_genes = num_matching_genes + num_non_matching_genes\n",
    "total_extracted_mentions = total_matching_mentions + total_non_matching_mentions\n",
    "proportion_matching_genes = (num_matching_genes / total_extracted_genes) * 100 if total_extracted_genes > 0 else 0\n",
    "proportion_non_matching_genes = (num_non_matching_genes / total_extracted_genes) * 100 if total_extracted_genes > 0 else 0\n",
    "proportion_matching_mentions = (total_matching_mentions / total_extracted_mentions) * 100 if total_extracted_mentions > 0 else 0\n",
    "proportion_non_matching_mentions = (total_non_matching_mentions / total_extracted_mentions) * 100 if total_extracted_mentions > 0 else 0\n",
    "\n",
    "# Oncomine gene matching\n",
    "print(\"\\n----------------- Oncomine Gene Matching -----------------\")\n",
    "print(f\"Total extracted genes: {total_extracted_genes:,}\")\n",
    "print(f\" - Matching genes in Oncomine: {num_matching_genes:,} ({proportion_matching_genes:.2f}%)\")\n",
    "print(f\" - Non-matching genes (not in Oncomine): {num_non_matching_genes:,} ({proportion_non_matching_genes:.2f}%)\")\n",
    "print(f\"\\nTotal extracted mentions: {total_extracted_mentions:,}\")\n",
    "print(f\" - Total mentions for Oncomine genes: {total_matching_mentions:,} ({proportion_matching_mentions:.2f}%)\")\n",
    "print(f\" - Total mentions for non-Oncomine genes: {total_non_matching_mentions:,} ({proportion_non_matching_mentions:.2f}%)\")\n",
    "\n",
    "# Data integrity check\n",
    "gene_check = \"Match!\" if total_extracted_genes == (num_matching_genes + num_non_matching_genes) else \"Warning: Mismatch!\"\n",
    "mention_check = \"Match!\" if total_extracted_mentions == (total_matching_mentions + total_non_matching_mentions) else \"Warning: Mismatch!\"\n",
    "print(\"\\n----------------- Data Integrity Check -----------------\")\n",
    "print(f\"Sum of matching + non-matching genes: {num_matching_genes + num_non_matching_genes:,} (should be {total_extracted_genes:,}) {gene_check}\")\n",
    "print(f\"Sum of matching + non-matching mentions: {total_matching_mentions + total_non_matching_mentions:,} (should be {total_extracted_mentions:,}) {mention_check}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38a6269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bar chart: unique genes\n",
    "fig, ax1 = plt.subplots(figsize=(8, 6))\n",
    "categories = [\"Matching (Oncomine)\", \"Non-Matching\"]\n",
    "gene_counts = [num_matching_genes, num_non_matching_genes]\n",
    "\n",
    "ax1.bar(categories, gene_counts, color=[\"green\", \"blue\"])\n",
    "ax1.set_title(f\"Oncomine matching of unique variant-associated genes\\n\"\n",
    "              f\"Total Oncomine genes: {len(gene_list):,}\\n\"\n",
    "              f\"Total extracted genes: {total_extracted_genes:,}\",\n",
    "              fontsize=14, pad=20)\n",
    "\n",
    "ax1.set_ylim(0, max(gene_counts) * 1.2 if max(gene_counts) > 0 else 1)\n",
    "\n",
    "for i, v in enumerate(gene_counts):\n",
    "    if total_extracted_genes > 0:\n",
    "        percentage = (v / total_extracted_genes) * 100\n",
    "    else:\n",
    "        percentage = 0\n",
    "    ax1.text(i, v + (max(gene_counts) * 0.05 if max(gene_counts) > 0 else 0.5),\n",
    "             f\"{v:,} ({percentage:.2f}%)\", ha='center', fontsize=10)\n",
    "\n",
    "plt.show()\n",
    "fig, ax2 = plt.subplots(figsize=(8, 6))\n",
    "mention_counts = [total_matching_mentions, total_non_matching_mentions]\n",
    "ax2.bar(categories, mention_counts, color=[\"green\", \"blue\"])\n",
    "ax2.set_title(f\"Total mentions of variant-associated genes in articles\\n\"\n",
    "              f\"Total extracted mentions: {total_extracted_mentions:,}\", \n",
    "              fontsize=14, pad=20)\n",
    "\n",
    "ax2.set_ylim(0, max(mention_counts) * 1.2 if max(mention_counts) > 0 else 1)\n",
    "for i, v in enumerate(mention_counts):\n",
    "    if total_extracted_mentions > 0:\n",
    "        percentage = (v / total_extracted_mentions) * 100\n",
    "    else:\n",
    "        percentage = 0\n",
    "    ax2.text(i, v + (max(mention_counts) * 0.05 if max(mention_counts) > 0 else 0.5),\n",
    "             f\"{v:,} ({percentage:.2f}%)\", ha='center', fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10e5c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics and CSV creation for waterfall chart.\n",
    "\n",
    "percent_oncomine_extracted = round((num_matching_genes / len_gene_list) * 100, 2) if len_gene_list > 0 else 0\n",
    "percent_oncomine_extracted_of_all_extracted = round((num_matching_genes / total_extracted_genes) * 100, 2) if total_extracted_genes > 0 else 0\n",
    "percent_other_extracted = round((num_non_matching_genes / total_extracted_genes) * 100, 2) if total_extracted_genes > 0 else 0\n",
    "percent_oncomine_mentions = round((total_matching_mentions / total_extracted_mentions) * 100, 2) if total_extracted_mentions > 0 else 0\n",
    "percent_other_mentions = round((total_non_matching_mentions / total_extracted_mentions) * 100, 2) if total_extracted_mentions > 0 else 0\n",
    "\n",
    "sum_extraction_check = round(percent_oncomine_extracted + (100 - percent_oncomine_extracted), 2)\n",
    "sum_extracted_total_check = round(percent_oncomine_extracted_of_all_extracted + percent_other_extracted, 2)\n",
    "sum_mentions_check = round(percent_oncomine_mentions + percent_other_mentions, 2)\n",
    "\n",
    "print(\"\\nValidation Checks:\")\n",
    "print(f\"Oncomine extraction breakdown is {sum_extraction_check:,.2f}%. This should be 100.00%.\")\n",
    "print(f\"Total extracted genes breakdown is {sum_extracted_total_check:,.2f}%. This should be 100.00%.\")\n",
    "print(f\"Total gene mentions breakdown is {sum_mentions_check:,.2f}%. This should be 100.00%.\")\n",
    "\n",
    "assert sum_extraction_check == 100.00, f\"Extraction sum mismatch. Found {sum_extraction_check:,.2f}%.\"\n",
    "assert sum_extracted_total_check == 100.00, f\"Extracted sum mismatch. Found {sum_extracted_total_check:,.2f}%.\"\n",
    "assert sum_mentions_check == 100.00, f\"Mentions sum mismatch. Found {sum_mentions_check:,.2f}%.\"\n",
    "\n",
    "print(\"All percentage calculations are correct.\")\n",
    "\n",
    "gene_list_sorted = sorted(gene_list)\n",
    "matching_genes_sorted = sorted(matching_genes.index)  \n",
    "non_matching_genes_sorted = sorted(non_matching_genes.index) \n",
    "oncomine_not_extracted_sorted = sorted(set(gene_list) - set(matching_genes.index))\n",
    "\n",
    "oncomine_genes_str = \", \".join(gene_list_sorted)  \n",
    "oncomine_not_extracted_str = \", \".join(oncomine_not_extracted_sorted)\n",
    "oncomine_extracted_str = \", \".join(matching_genes_sorted) \n",
    "other_extracted_str = \", \".join(non_matching_genes_sorted) \n",
    "\n",
    "data = {\n",
    "    \"Category\": [\n",
    "        \"Oncomine genes total\", \n",
    "        \"Oncomine genes not extracted\", \n",
    "        \"Oncomine genes extracted\",\n",
    "        \"\",  \n",
    "        \"Other genes extracted\",\n",
    "        \"Oncomine genes extracted\",  \n",
    "        \"Extracted unique genes\",\n",
    "        \"\",  \n",
    "        \"Oncomine gene mentions\",  \n",
    "        \"Other gene mentions\",\n",
    "        \"Total gene mentions\"\n",
    "    ],\n",
    "    \"Count\": [\n",
    "        f\"{len_gene_list:,}\",  \n",
    "        f\"{len_gene_list - num_matching_genes:,}\",  \n",
    "        f\"{num_matching_genes:,}\",  \n",
    "        \"\",  \n",
    "        f\"{num_non_matching_genes:,}\",  \n",
    "        f\"{num_matching_genes:,}\",  \n",
    "        f\"{total_extracted_genes:,}\",  \n",
    "        \"\",  \n",
    "        f\"{total_matching_mentions:,}\",  \n",
    "        f\"{total_non_matching_mentions:,}\",  \n",
    "        f\"{total_extracted_mentions:,}\"  \n",
    "    ],\n",
    "    \"Percentage\": [\n",
    "        \"100.00\",  \n",
    "        f\"{round(100 - percent_oncomine_extracted, 2):,.2f}\",  \n",
    "        f\"{percent_oncomine_extracted:,.2f}\",  \n",
    "        \"\",  \n",
    "        f\"{percent_other_extracted:,.2f}\",  \n",
    "        f\"{percent_oncomine_extracted_of_all_extracted:,.2f}\",  \n",
    "        \"100.00\",  \n",
    "        \"\",  \n",
    "        f\"{percent_oncomine_mentions:,.2f}\",  \n",
    "        f\"{percent_other_mentions:,.2f}\",  \n",
    "        \"100.00\"  \n",
    "    ],\n",
    "    \"Genes\": [\n",
    "        oncomine_genes_str,  \n",
    "        oncomine_not_extracted_str,  \n",
    "        oncomine_extracted_str,  \n",
    "        \"\",  \n",
    "        other_extracted_str,  \n",
    "        oncomine_extracted_str,  \n",
    "        oncomine_genes_str,  \n",
    "        \"\",  \n",
    "        \"\",  \n",
    "        \"\",  \n",
    "        \"\"  \n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "csv_path = \"oncomine_matched_gene_counts_summary.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(\"\\n----------------- Gene Counts Summary -----------------\")\n",
    "for index, row in df.iterrows():\n",
    "    if row['Category'] == \"\":\n",
    "        print(\"\")\n",
    "    else:\n",
    "        print(f\"{row['Category']:>35} {str(row['Count']):>10} {str(row['Percentage']):>10}\")\n",
    "\n",
    "print(f\"\\nCSV file has been saved as {csv_path}.\")\n",
    "print(\"\\n\\n\\n----------- Print sorted gene names per category -------------\")\n",
    "print(\"\\nOncomine genes in total.\")\n",
    "print(oncomine_genes_str)\n",
    "print(\"\\nOncomine genes not extracted.\")\n",
    "print(oncomine_not_extracted_str if oncomine_not_extracted_str else \"None.\")\n",
    "print(\"\\nOncomine genes extracted.\")\n",
    "print(oncomine_extracted_str if oncomine_extracted_str else \"None.\")\n",
    "print(\"\\nOther extracted genes (non-Oncomine).\")\n",
    "print(other_extracted_str if other_extracted_str else \"None.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a509f8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data for the two tables to be saved\n",
    "data_summary = {\n",
    "    \"Category\": [\n",
    "        \"Oncomine genes total\", \n",
    "        \"Oncomine genes not extracted\", \n",
    "        \"Oncomine genes extracted\",\n",
    "        \"Oncomine gene mentions\",\n",
    "        \"Other gene mentions\",\n",
    "        \"Total gene mentions\"\n",
    "    ],\n",
    "    \"Count\": [\n",
    "        len_gene_list,\n",
    "        len_gene_list - num_matching_genes,\n",
    "        num_matching_genes,\n",
    "        total_matching_mentions,\n",
    "        total_non_matching_mentions,\n",
    "        total_extracted_mentions\n",
    "    ],\n",
    "    \"Percentage\": [\n",
    "        100.00, \n",
    "        100 - percent_oncomine_extracted,\n",
    "        percent_oncomine_extracted,\n",
    "        percent_oncomine_mentions,\n",
    "        percent_other_mentions,\n",
    "        100.00  \n",
    "    ]\n",
    "}\n",
    "\n",
    "df_summary = pd.DataFrame(data_summary)\n",
    "csv_summary_path = \"oncomine_gene_summary_stats_forfigure.csv\"\n",
    "df_summary.to_csv(csv_summary_path, index=False)\n",
    "print(f\"CSV file successfully saved as {csv_summary_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b4147a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate final dataset one mroe time\n",
    "df = pd.read_csv(\"normalized_merged_variant_matrix_v4.csv\")\n",
    "total_rows = 199726\n",
    "count_ge_1 = (df[\"total_variant_count\"] >= 1).sum()\n",
    "count_0 = 199726 - count_ge_1\n",
    "percentage_ge_1 = (count_ge_1 / total_rows) * 100\n",
    "percentage_0 = (count_0 / total_rows) * 100\n",
    "print(f\"Total number of rows: {total_rows:,}\")\n",
    "print(f\"Rows where total_variant_count >= 1: {count_ge_1:,} ({percentage_ge_1:.2f}%)\")\n",
    "print(f\"Rows where total_variant_count == 0: {count_0:,} ({percentage_0:.2f}%)\")\n",
    "if total_rows == (count_ge_1 + count_0):\n",
    "    print(\"The counts add up correctly.\")\n",
    "else:\n",
    "    print(\"The counts do NOT add up. There might be missing or NaN values in 'total_variant_count'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
