{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbdd887b",
   "metadata": {},
   "source": [
    "# Use LLM for genetic variant extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c875159",
   "metadata": {},
   "source": [
    "- Set up libraries and datasets\n",
    "- Select LLM\n",
    "- Select performing prompt\n",
    "- Run LLM on dataset\n",
    "- Extract information and create variant matrix and dictionary\n",
    "- Evaluation\n",
    "    - Comparison of LLMs for genetic variant extraction\n",
    "    - Compairson of different prompts to for genetic variant extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83670bcd",
   "metadata": {},
   "source": [
    "# 1) Set up libraries and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e0639c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import sys\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d977552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the working directory and file paths\n",
    "input_directory = \"INPUT_DIRECTORY\"\n",
    "output_directory = \"OUTPUT_DIRECTORY\"\n",
    "LLM_evaluation_directory = \"LLM_EVALUATION_DIRECTORY\"\n",
    "os.chdir(output_directory)\n",
    "print(\"Current Working Directory:\", os.getcwd())\n",
    "\n",
    "# Load datasets\n",
    "os.chdir(output_directory)\n",
    "print(\"\\nCurrent Working Directory:\", os.getcwd())\n",
    "cancer_df = pd.read_csv(\"binary_cancer_matrix_filtered.csv\")\n",
    "len_cancer_df=len(cancer_df)\n",
    "print(f\" --> Total rows in cancer dataset: {len_cancer_df:,}\")\n",
    "cancer_df = cancer_df[['PaperId', 'PaperTitle', 'Abstract']].copy()\n",
    "print(cancer_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97a1e27",
   "metadata": {},
   "source": [
    "# 2) Select and set up LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75357b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a language model to answer the questions\n",
    "!pip install OpenAI\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from openai import OpenAI\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ccd919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models to be tested\n",
    "models = [\"llama31-70b\", \"llama33-70b\", \"deepseek_v3\", \"deepseek_r1\", \"deepseek_r1_distill_llama_70b\"]\n",
    "\n",
    "# Mapping model names to their full Hugging Face or DeepInfra identifiers\n",
    "model_fullnames = {\n",
    "    \"llama31-70b\": \"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n",
    "    \"llama33-70b\": \"meta-llama/Llama-3.3-70B-Instruct\",\n",
    "    \"deepseek_v3\": \"deepseek-ai/DeepSeek-V3\",\n",
    "    \"deepseek_r1\": \"deepseek-ai/DeepSeek-R1\",\n",
    "    \"deepseek_r1_distill_llama_70b\": \"deepseek-ai/DeepSeek-R1-Distill-Llama-70B\"\n",
    "}\n",
    "\n",
    "SYSTEM_MSG = \"You are a helpful medical question answering assistant. Please carefully follow the exact instructions and do not provide explanations.\"\n",
    "modelname = models[1]\n",
    "\n",
    "if modelname in [ \"llama2-3b\" ]:  # Local model\n",
    "    model, tokenizer = load(model_fullnames[modelname])\n",
    "    def generateFromPrompt(prompt):\n",
    "        if hasattr(tokenizer, \"apply_chat_template\") and tokenizer.chat_template is not None:\n",
    "            messages = [{\"role\": \"system\", \"content\": SYSTEM_MSG},\n",
    "                {\"role\": \"user\", \"content\": prompt}]\n",
    "            prompt = tokenizer.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "            response = generate(model, tokenizer, prompt=prompt, verbose=False)\n",
    "            return response\n",
    "elif modelname in [ \"gpt35\", \"gpt4o\" ]: # OpenAI models\n",
    "    client = OpenAI(\n",
    "       api_key='API_KEY1'   \n",
    "    )\n",
    "    def generateFromPrompt(promptStr,maxTokens=100):\n",
    "      messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_MSG},\n",
    "        {\"role\": \"user\", \"content\": promptStr}\n",
    "      ]\n",
    "      completion = client.chat.completions.create(\n",
    "        model=model_fullnames[modelname],\n",
    "        messages=messages)\n",
    "      response=completion.choices[0].message.content\n",
    "      return(response)\n",
    "elif modelname in [ \"llama31-70b\" , \"llama33-70b\" , \"deepseek_v3\" , \"deepseek_r1\" , \"deepseek_r1_distill_llama_70b\"]:  # DeepInfra models\n",
    "    client = OpenAI(\n",
    "        api_key = \"API_KEY2\",\n",
    "        base_url=\"https://api.deepinfra.com/v1/openai\",\n",
    "    )\n",
    "    def generateFromPrompt(promptStr,maxTokens=100):\n",
    "      messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_MSG},\n",
    "        {\"role\": \"user\", \"content\": promptStr}\n",
    "      ]\n",
    "      completion = client.chat.completions.create(\n",
    "        model=model_fullnames[modelname],\n",
    "        messages=messages)\n",
    "      response=completion.choices[0].message.content\n",
    "      return(response)\n",
    "    \n",
    "generateFromPrompt(\"hello!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8977f275",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"All installed models:\",   models)\n",
    "print(\"Current model in use:\",   modelname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cb678e",
   "metadata": {},
   "source": [
    "# 3) Define prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfe876d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompts in a dictionary\n",
    "# All prompts have been tested and #3 is the best performing\n",
    "\n",
    "PROMPTS = {\n",
    "    0: lambda title, abstract: (\n",
    "        f\"You are a helpful and highly detail-oriented research assistant. Carefully review the title and abstract of the given publication to identify any specific genetic variants mentioned.\"\n",
    "        f\"Genetic variants must include specific and concrete names such as variant reference IDs, DNA base changes, protein changes, or precise genetic notations.\"\n",
    "        f\"Your goal is to report only **explicitly mentioned and well-defined genetic variants** and disregard any vague or generic descriptions. \"\n",
    "        f\"Do NOT include generic mentions like simple gene names (e.g., BRCA1, BRCA2, ATM) or unspecific descriptions (e.g., 'allele loss,' 'mutation detected'). \"\n",
    "        f\"Do NOT report terms such as 'reversion mutations,' 'gene mutation,' or 'mutation detected,' unless a specific variant name is explicitly mentioned.\\n\\n\"\n",
    "        f\"Start your response with:\\n\"\n",
    "        f\" 'No genetic variant detected in this publication.' if no genetic variants are found, or if only vague terms or placeholders are mentioned (e.g., 'Not specified,' 'unknown variant').\\n\\n\"\n",
    "        f\" 'Genetic variant detected:' if you identify any genetic variant, followed by detailed information below.\\n\\n\"\n",
    "        f\"For each identified variant, provide the following details:\\n\"\n",
    "        f\"- **Variant name**: e.g., N2875H\\n\"\n",
    "        f\"- **Variant type**: Specify SNP, Indel, missense variant, or splice site variant.\\n\"\n",
    "        f\"- **Notation**: Provide HGVS notation if available (e.g., c.7089+1del).\\n\"\n",
    "        f\"- **Sequence ontology**: e.g., missense variant, synonymous variant, or frameshift.\\n\"\n",
    "        f\"- **Location**: Specify gene name, chromosome, or precise locus (e.g., Gene: ATM, Chromosome: chr11).\\n\"\n",
    "        f\"- **Functional impact**: Describe effects on gene function or expression (e.g., nonsynonymous alteration, base change C > T).\\n\"\n",
    "        f\"- **Clinical relevance**: Mention any associated disease, trait, or phenotype.\\n\"\n",
    "        f\"Important: If a Variant name is **'Not mentioned,' 'Not specified,' or only inferred** (e.g., 'reversion mutation,' 'mutation detected'), respond with: **'No genetic variant detected in this publication.'**\\n\\n\"\n",
    "        f\"Now analyze the following details:\\n\"\n",
    "        f\"Title: {title}\\n\\nAbstract: {abstract}\\n\\n\"\n",
    "        f\"Provide your response in the specified format.\"\n",
    "    ),\n",
    "    \n",
    "    1: lambda title, abstract: (\n",
    "        f\"Analyze the following title and abstract to identify **genetic variants** mentioned,\" \n",
    "        f\"including specific variant names, base changes, and protein alterations.\\n\"\n",
    "        f\"Only report well-defined variants, such as specific reference IDs or exact mutations.\"\n",
    "        f\"Ignore vague terms like 'mutation detected' or generic mentions of genes (e.g., BRCA1, ATM).\"\n",
    "        f\"If no specific variant is mentioned, respond with: 'No variant.'\\n\\n\"\n",
    "        f\"For each identified variant, provide:\\n\"\n",
    "        f\"- **Variant Name**: e.g., c.2138C>G\\n\"\n",
    "        f\"- **Gene Name**: e.g., BRCA1\\n\"\n",
    "        f\"Return results in the following format: 'Variant: Variant Name, Gene Name' or 'No variant.'\\n\\n\"\n",
    "        f\"Title: {title}\\nAbstract: {abstract}\\n\"\n",
    "    ),\n",
    "\n",
    "    2: lambda title, abstract: (\n",
    "        f\"Extract all genetic variants from the following title and abstract. Only return:\\n\"\n",
    "        f\"- **HGVS Notation** (c., p., g.), e.g., c.2138C>G, p.Arg713Trp, g.32389625G>A\\n\"\n",
    "        f\"- **Protein changes** (e.g., V600E, Arg713Trp)\\n\"\n",
    "        f\"- **rsIDs** (e.g., rs121913529)\\n\"\n",
    "        f\"- Ignore vague terms (e.g., 'mutation found').\\n\\n\"\n",
    "        f\"### Format response strictly as:\\n\"\n",
    "        f\"'Variant: <mutation>, Gene: <gene>' per line, or 'No variant' if none found.\\n\\n\"\n",
    "        f\"Title: {title}\\nAbstract: {abstract}\"\n",
    "    ),\n",
    "\n",
    "    3: lambda title, abstract: (\n",
    "        f\"Extract only specific genetic variants from the text. Return strictly:\\n\"\n",
    "        f\"- **HGVS Notation** (c., p., g.) e.g., c.2138C>G, p.Arg713Trp\\n\"\n",
    "        f\"- **Protein changes** (e.g., V600E, Arg713Trp)\\n\"\n",
    "        f\"- **rsIDs** (e.g., rs121913529)\\n\"\n",
    "        f\"- Ignore vague terms (e.g., 'mutation found').\\n\\n\"\n",
    "        f\"### Format:\\n\"\n",
    "        f\"- Variant: 'Variant: <mutation>, Gene: <gene>' per line\\n\"\n",
    "        f\"- If none, return: 'No variant'\\n\"\n",
    "        f\"- No extra text, no explanations.\\n\\n\"\n",
    "        f\"Title: {title}\\nAbstract: {abstract}\"\n",
    "    ),\n",
    "\n",
    "    4: lambda title, abstract: (\n",
    "        f\"Extract only genetic variants from the text in strict format:\\n\"\n",
    "        f\"- HGVS Notation: c., p., g. (e.g., c.2138C>G, p.Arg713Trp)\\n\"\n",
    "        f\"- Protein changes: (e.g., V600E, Arg713Trp, frameshift mutations e.g., p.Asp427Thrfs*3**)\\n\"\n",
    "        f\"- rsIDs: (e.g., rs121913529)\\n\"\n",
    "        f\"- Ignore vague terms ('mutation found').\\n\\n\"\n",
    "        f\"**Format:**\\n\"\n",
    "        f\"- 'Variant: <mutation>, Gene: <gene>' per line\\n\"\n",
    "        f\"- If none, return: 'No variant'\\n\"\n",
    "        f\"- No extra text or explanation.\\n\\n\"\n",
    "        f\"Title: {title}\\nAbstract: {abstract}\"\n",
    "    ),\n",
    "\n",
    "    5: lambda title, abstract: (\n",
    "        f\"Extract **only genetic variants** from the text in strict format:\\n\"\n",
    "        f\"- HGVS Notation: c., p., g. (e.g., c.2138C>G, p.Arg713Trp)\\n\"\n",
    "        f\"- Protein changes: (e.g., V600E, Arg713Trp, frameshift mutations e.g., p.Asp427Thrfs*3**)\\n\"\n",
    "        f\"- rsIDs: (e.g., rs121913529)\\n\"\n",
    "        f\"- Ignore vague terms (e.g., 'mutation found').\\n\"\n",
    "        f\"- **Do not add extra comments, explanations, or summaries.**\\n\\n\"\n",
    "        f\"**Format:**\\n\"\n",
    "        f\"- 'Variant: <mutation>, Gene: <gene>' per line\\n\"\n",
    "        f\"- If none, return exactly: 'No variant'\\n\"\n",
    "        f\"- **No extra text, no summaries, no explanations, no other output.**\\n\\n\"\n",
    "        f\"Title: {title}\\nAbstract: {abstract}\"\n",
    "    ),\n",
    "\n",
    "    6: lambda title, abstract: (\n",
    "        f\"Extract only specific genetic variants from the text. Strictly return:\\n\"\n",
    "        f\"- **HGVS Notation** (c., p., g.) e.g., c.2138C>G, p.Arg713Trp\\n\"\n",
    "        f\"- **Protein changes** (e.g., V600E, Arg713Trp)\\n\"\n",
    "        f\"- **rsIDs** (e.g., rs121913529)\\n\"\n",
    "        f\"- Each variant must be associated with a gene.\\n\"\n",
    "        f\"- **Do NOT return genes alone with no variant.**\\n\"\n",
    "        f\"- **Ignore vague terms** (e.g., 'mutation found', 'gene alteration', or general mentions of genes like TP53).\\n\"\n",
    "        f\"- **Strictly format the output as follows:**\\n\"\n",
    "        f\"\\n### Output format:\\n\"\n",
    "        f\"Variant: <variant>, Gene: <gene>  (one per line)\\n\"\n",
    "        f\"Example: 'Variant: V600E, Gene: BRAF'\\n\"\n",
    "        f\"If no valid variants exist, return only: 'No variant'\\n\"\n",
    "        f\"No explanations, no extra text.\\n\\n\"\n",
    "        f\"Title: {title}\\nAbstract: {abstract}\"\n",
    "    )\n",
    "}\n",
    "print(\"Prompts successfully defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449ca54d",
   "metadata": {},
   "source": [
    "# 4) Run genetic variant extraction with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc298f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================== CONFIGURATION ========================== #\n",
    "# Define output directory\n",
    "os.chdir(output_directory)\n",
    "tqdm.pandas()\n",
    "\n",
    "# Define batch size for processing\n",
    "BATCH_SIZE = 60000\n",
    "\n",
    "# Set model name and prompt selection\n",
    "modelname = modelname\n",
    "selected_prompt_number = 3  # Change dynamically as needed\n",
    "\n",
    "# Get today's date and current time\n",
    "today_date = datetime.today().strftime('%Y-%m-%d')\n",
    "start_time_str = datetime.now().strftime('%Y-%m-%d %H:%M:%S') \n",
    "\n",
    "# Define file paths\n",
    "variant_output_file_path = os.path.join(output_directory, f\"LLM_variant_extraction_{modelname}_prompt{selected_prompt_number}.csv\")\n",
    "runtime_file = os.path.join(output_directory, f\"runtime_summary_{modelname}_prompt{selected_prompt_number}.txt\")\n",
    "progress_log_file = os.path.join(output_directory, f\"progress_log_{modelname}_prompt{selected_prompt_number}.txt\")\n",
    "\n",
    "# Ensure files exist\n",
    "def ensure_file_exists(file_path, header_text=None):\n",
    "    \"\"\"Creates the file if it does not exist and optionally writes a header.\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        with open(file_path, \"w\") as f:\n",
    "            if header_text:\n",
    "                f.write(f\"{header_text}\\n\")\n",
    "                f.write(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "ensure_file_exists(runtime_file, f\"### Runtime Log - {today_date} ###\\nStart Time: {start_time_str}\")\n",
    "ensure_file_exists(progress_log_file, f\"### Progress Log - {today_date} ###\")\n",
    "\n",
    "if not os.path.exists(variant_output_file_path):\n",
    "    with open(variant_output_file_path, \"w\") as f:\n",
    "        f.write(\"PaperId,PaperTitle,Abstract,LLM_Prompt,LLM_Response\\n\")\n",
    "\n",
    "# Define logging setup\n",
    "logging.basicConfig(\n",
    "    filename=progress_log_file,\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "\n",
    "print(\"Success! All necessary files and directories are set up.\")\n",
    "print(\"Script Start Time:\", start_time_str)\n",
    "print(\"Defined prompt number:\", selected_prompt_number)\n",
    "print(f\"Defined batch size: {BATCH_SIZE:,}\")\n",
    "\n",
    "# Define functions\n",
    "def screen_publication_for_variants(row, prompt_number):\n",
    "    \"\"\"Generates a dynamic prompt based on the selected prompt number.\"\"\"\n",
    "    title = row['PaperTitle']\n",
    "    abstract = row['Abstract']\n",
    "    if prompt_number not in PROMPTS:\n",
    "        raise ValueError(f\"Invalid prompt number: {prompt_number}. Choose between 0-5.\")\n",
    "    return PROMPTS[prompt_number](title, abstract)\n",
    "def process_with_llm(prompt):\n",
    "    \"\"\"Process the given prompt using the LLM model.\"\"\"\n",
    "    try:\n",
    "        response = generateFromPrompt(prompt)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        logging.error(f\"LLM processing error: {e}\")\n",
    "        return \"ERROR\"\n",
    "print(\"Functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470b4e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================== RESUME FROM LAST CHECKPOINT ========================== #\n",
    "\n",
    "# Load cancer dataset\n",
    "if 'cancer_df' not in globals():\n",
    "    raise ValueError(\"Dataset `cancer_df` is not loaded in memory. Make sure it's defined before running the script.\")\n",
    "if 'PaperId' not in cancer_df.columns:\n",
    "    raise KeyError(\"Dataset must contain a 'PaperId' column to track progress.\")\n",
    "\n",
    "if os.path.exists(variant_output_file_path):\n",
    "    processed_df = pd.read_csv(variant_output_file_path)\n",
    "    processed_articles = set(processed_df['PaperId']) \n",
    "    total_processed_articles = len(processed_articles) \n",
    "    print(f\"Resuming from last processed row. {total_processed_articles} articles completed so far.\")\n",
    "else:\n",
    "    processed_articles = set()\n",
    "    total_processed_articles = 0\n",
    "    print(\"Starting fresh processing.\")\n",
    "\n",
    "total_batches = (len(cancer_df) // BATCH_SIZE) + (1 if len(cancer_df) % BATCH_SIZE != 0 else 0)\n",
    "if total_processed_articles == len(cancer_df):\n",
    "    print(\"\\nAll batches are complete. No more articles to process.\")\n",
    "    print(\"You have successfully processed the entire dataset.\")\n",
    "    try:\n",
    "        sys.exit(0)\n",
    "    except SystemExit:\n",
    "        pass \n",
    "\n",
    "unprocessed_df = cancer_df[~cancer_df['PaperId'].isin(processed_articles)]\n",
    "total_articles = len(unprocessed_df)    \n",
    "    \n",
    "print(\"Success! All necessary files and directories are set up.\")\n",
    "print(\"Defined prompt number:\", selected_prompt_number)\n",
    "print(f\"Defined batch size to run in chunks: {BATCH_SIZE:,}\")\n",
    "print(f\"Total unprocessed articles: {total_articles:,}\")\n",
    "\n",
    "# ========================== TRACK CUMULATIVE RUNTIME ========================== #\n",
    "if os.path.exists(runtime_file):\n",
    "    with open(runtime_file, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        total_runtime_previous = sum(float(line.split(\":\")[-1].strip().split()[0])\n",
    "                                     for line in lines if \"Total runtime so far\" in line)\n",
    "else:\n",
    "    total_runtime_previous = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c72a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================== BATCH PROCESSING ========================== #\n",
    "start_time = time.time()\n",
    "\n",
    "# Calculate the next batch number\n",
    "batch_number = (total_processed_articles // BATCH_SIZE) + 1\n",
    "\n",
    "for batch_start in range(0, total_articles, BATCH_SIZE):\n",
    "    batch_end = min(batch_start + BATCH_SIZE, total_articles)\n",
    "    batch = unprocessed_df.iloc[batch_start:batch_end].copy()\n",
    "    print(f\"\\nProcessing Batch {batch_number}/{total_batches} ({batch_start + 1} to {batch_end})...\")\n",
    "    batch_start_time = time.time()\n",
    "    batch['LLM_Prompt'] = batch.apply(lambda row: screen_publication_for_variants(row, selected_prompt_number), axis=1)\n",
    "\n",
    "    llm_response_column = f'LLM_Response_{modelname}'\n",
    "    batch[llm_response_column] = batch['LLM_Prompt'].progress_apply(process_with_llm)\n",
    "\n",
    "    batch_runtime = time.time() - batch_start_time\n",
    "    batch_to_save = batch[['PaperId', 'PaperTitle', 'Abstract', 'LLM_Prompt', llm_response_column]]\n",
    "\n",
    "    def generate_progress_bar(percentage, bar_length=20):\n",
    "        filled_length = int(bar_length * percentage / 100)\n",
    "        bar = '|' * filled_length + '-' * (bar_length - filled_length)\n",
    "        return f\"[{bar}] {percentage:.2f}%\"\n",
    "\n",
    "    total_articles = batch_end + len(processed_articles) \n",
    "    total_articles_to_process = len(unprocessed_df) - batch_end \n",
    "    processed_percentage = (total_articles / len(cancer_df)) * 100\n",
    "    to_process_percentage = (total_articles_to_process / len(cancer_df)) * 100\n",
    "\n",
    "    if os.path.exists(variant_output_file_path):\n",
    "        batch_to_save.to_csv(variant_output_file_path, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        batch_to_save.to_csv(variant_output_file_path, mode='w', index=False)\n",
    "\n",
    "    total_runtime_so_far = total_runtime_previous + (time.time() - start_time)\n",
    "\n",
    "    with open(runtime_file, \"a\") as f:\n",
    "        f.write(f\"\\nBatch {batch_number}/{total_batches} started at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Batch Runtime: {batch_runtime:.2f} sec\\n\")\n",
    "        f.write(f\"Total runtime so far (all runs combined): {total_runtime_so_far:.2f} sec\\n\")\n",
    "        f.write(f\"Total articles processed in this batch: {batch_end}\\n\")\n",
    "        f.write(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "    logging.info(f\"Processed batch {batch_number}/{total_batches} in {batch_runtime:.2f} sec.\")\n",
    "\n",
    "    if processed_percentage >= 100:\n",
    "        print(\"\\nAll articles have been successfully processed.\")\n",
    "        print(\"No more articles remaining.\")\n",
    "        break\n",
    "    else:\n",
    "        print(f\"\\nPaused! {batch_end} articles processed in this batch.\")\n",
    "        print(f\"{total_articles} articles processed in total {generate_progress_bar(processed_percentage)}\")\n",
    "        print(f\"{total_articles_to_process} articles to process in total {generate_progress_bar(to_process_percentage)}\")\n",
    "        print(\"Check the CSV and runtime file. When ready, rerun the script to continue processing.\")\n",
    "        break\n",
    "\n",
    "# ========================== FINAL SUMMARY ========================== #\n",
    "total_runtime = total_runtime_so_far\n",
    "total_hours = total_runtime // 3600\n",
    "total_minutes = (total_runtime % 3600) // 60\n",
    "total_seconds = total_runtime % 60\n",
    "\n",
    "summary_text = f\"\"\"\n",
    "### Genetic Variant Extraction Summary ###\n",
    "\n",
    "- Model used: {modelname}\n",
    "- Prompt number: {selected_prompt_number}\n",
    "- Total batches processed: {batch_number:,}/{total_batches:,}\n",
    "- Total articles processed: {total_articles:,}\n",
    "- Batch runtime: {batch_runtime:,}\n",
    "- Cumulative runtime: {total_runtime:.2f} seconds ({total_hours:.0f} hr {total_minutes:.0f} min {total_seconds:.2f} sec)\n",
    "\"\"\"\n",
    "print(summary_text)\n",
    "with open(runtime_file, \"a\") as f:\n",
    "    f.write(\"\\n### Final runtime summary ###\\n\")\n",
    "    f.write(f\"End Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    f.write(summary_text)\n",
    "    f.write(\"\\n\" + \"=\" * 60 + \"\\n\")\n",
    "\n",
    "print(\"Final results saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd4dd3f",
   "metadata": {},
   "source": [
    "# RERUN FROM LAST CHECKPOINT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e62b571",
   "metadata": {},
   "source": [
    "# ============================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c32748a",
   "metadata": {},
   "source": [
    "# 6) Evaluation of LLMs and prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cab4bd",
   "metadata": {},
   "source": [
    "## 6.1) Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495ecb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the working directory and file paths\n",
    "print(\"Current Working Directory:\", os.getcwd())\n",
    "os.chdir(input_directory)\n",
    "print(\"Current Working Directory:\", os.getcwd())\n",
    "llm_ev = \"LLM_evaluation_statistics.csv\"\n",
    "llm_df = pd.read_csv(llm_ev)\n",
    "print(llm_df.head(5))\n",
    "print(len(llm_df))\n",
    "\n",
    "# Investigate column names\n",
    "header = llm_df.columns[1:].tolist()\n",
    "print(header)\n",
    "model_names = ['LLama31-70b', 'LLama33-70b', 'DeepSeek_V3', 'DeepSeek-R1-Distill-Llama-70B']\n",
    "print(\"\\n\\nModels selected for evaluation:\", model_names)\n",
    "\n",
    "# Change the working directory\n",
    "os.chdir(LLM_evaluation_directory)\n",
    "print(\"Current Working Directory:\", os.getcwd())\n",
    "llm_df_bn = llm_df.copy()\n",
    "columns_to_transform = ['Human'] + model_names\n",
    "llm_df_bn[columns_to_transform] = llm_df_bn[columns_to_transform].applymap(lambda x: 0 if x == \"0\" else 1)\n",
    "print(llm_df_bn.head(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1592e3c9",
   "metadata": {},
   "source": [
    "## 6.2) Calculate confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d1f126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the first model from model_names\n",
    "first_model = model_names[0]\n",
    "y_true = llm_df_bn['Human']\n",
    "y_pred = llm_df_bn[first_model]\n",
    "tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "results = {\n",
    "    'Model Name': first_model,\n",
    "    'True Positives (TP)': tp,\n",
    "    'False Positives (FP)': fp,\n",
    "    'False Negatives (FN)': fn,\n",
    "    'True Negatives (TN)': tn,\n",
    "    'F1 Score': f1\n",
    "}\n",
    "results_df = pd.DataFrame([results])\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd770d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure llm_df_bn exists before running analysis\n",
    "if 'llm_df_bn' in globals() or 'llm_df_bn' in locals():\n",
    "    results = {}\n",
    "    y_true = llm_df_bn['Human']\n",
    "    # Loop through each model and compute confusion matrix & metrics\n",
    "    for model in model_names:\n",
    "        y_pred = llm_df_bn[model]\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "        # Calculate performance metrics\n",
    "        accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0  # Sensitivity\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "        f1 = f1_score(y_true, y_pred, zero_division=1)        \n",
    "        results[model] = {\n",
    "            \"True Positives (TP)\": tp,\n",
    "            \"False Positives (FP)\": fp,\n",
    "            \"False Negatives (FN)\": fn,\n",
    "            \"True Negatives (TN)\": tn,\n",
    "            \"F1 Score\": f1,\n",
    "            \"Sensitivity (Recall)\": recall,\n",
    "            \"Specificity\": specificity,\n",
    "            \"Precision\": precision,\n",
    "            \"Accuracy\": accuracy\n",
    "        }\n",
    "    results_df = pd.DataFrame(results).T\n",
    "    print(results_df)\n",
    "\n",
    "results_df.to_csv(\"llm_performance_metrics.csv\", index=True)\n",
    "print(\"\\nResults saved as 'llm_performance_metrics.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a055ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert confusion matrix values to integers to avoid formatting error\n",
    "confusion_matrix_values = results_df[['True Positives (TP)', 'False Positives (FP)', 'False Negatives (FN)', 'True Negatives (TN)']].astype(int)\n",
    "\n",
    "# Heatmap of confusion matrix counts\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(confusion_matrix_values, annot=True, fmt=\"d\", cmap=\"Blues\", linewidths=0.5)\n",
    "plt.title(\"Confusion Matrix Counts for Each Model\")\n",
    "plt.xlabel(\"Metrics\")\n",
    "plt.ylabel(\"LLM Models\")\n",
    "plt.show()\n",
    "\n",
    "# Bar chart for F1 scores\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(x=results_df.index, y=results_df[\"F1 Score\"], palette=\"viridis\")\n",
    "plt.title(\"F1 Scores for Each Model\")\n",
    "plt.xlabel(\"LLM Models\")\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Line plot for Sensitivity, Specificity, Precision, and Accuracy\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(results_df.index, results_df[\"Sensitivity (Recall)\"], marker='o', label=\"Sensitivity (Recall)\")\n",
    "plt.plot(results_df.index, results_df[\"Specificity\"], marker='s', label=\"Specificity\")\n",
    "plt.plot(results_df.index, results_df[\"Precision\"], marker='^', label=\"Precision\")\n",
    "plt.plot(results_df.index, results_df[\"Accuracy\"], marker='d', label=\"Accuracy\")\n",
    "plt.title(\"Model Performance Metrics\")\n",
    "plt.xlabel(\"LLM Models\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
