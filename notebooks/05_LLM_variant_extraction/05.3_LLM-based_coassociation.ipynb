{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbdd887b",
   "metadata": {},
   "source": [
    "# Use LLM for coassociarion analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83670bcd",
   "metadata": {},
   "source": [
    "# 1) Set up libraries and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e0639c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75d49e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the working directory and file paths\n",
    "input_directory = \"INPUT_DIRECTORY\"\n",
    "output_directory = \"OUTPUT_DIRECTORY\"\n",
    "variantscape_directory = \"VARIANTSCAPE_DIRECTORY\"\n",
    "variantscape_LLM_coas_directory = \"VARIANTSCAPE_LLM_COAS_DIRECTORY\"\n",
    "figure_directory = \"FIGURE_DIRECTORY\"\n",
    "\n",
    "os.chdir(variantscape_directory)\n",
    "print(\"Current directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d338907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate dataset\n",
    "variant_analysis_df = pd.read_csv(\"cleaned_df_v4.csv\", low_memory=False)\n",
    "metadata_mapping = pd.read_csv(\"metadata_mapping_transposed.csv\", low_memory=False)\n",
    "variant_cols = set(variant_analysis_df.columns)\n",
    "metadata_mapping[\"Entity\"] = metadata_mapping[\"Entity\"].astype(str).str.strip()\n",
    "metadata_mapping[\"Category\"] = metadata_mapping[\"Category\"].astype(str).str.strip()\n",
    "valid_metadata = metadata_mapping[metadata_mapping[\"Entity\"].isin(variant_cols)].copy()\n",
    "col_to_category = dict(zip(valid_metadata[\"Entity\"], valid_metadata[\"Category\"]))\n",
    "from collections import defaultdict\n",
    "\n",
    "category_to_cols = defaultdict(list)\n",
    "for col, cat in col_to_category.items():\n",
    "    category_to_cols[cat].append(col)\n",
    "\n",
    "for cat, cols in category_to_cols.items():\n",
    "    print(f\"{cat}: {len(cols)} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9aaa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper to extract info per paper\n",
    "def extract_paper_info(row, variant_cols, treatment_cols):\n",
    "    paper_id = row.get(\"PaperId\", \"\")\n",
    "    title = row.get(\"PaperTitle\", \"\")\n",
    "    abstract = row.get(\"Abstract\", \"\")\n",
    "    mentioned_variants = [col for col in variant_cols if row.get(col, 0) == 1]\n",
    "    mentioned_treatments = [col for col in treatment_cols if row.get(col, 0) == 1]\n",
    "    \n",
    "    return {\n",
    "        \"PaperId\": paper_id,\n",
    "        \"PaperTitle\": title,\n",
    "        \"Abstract\": abstract,\n",
    "        \"Variants\": mentioned_variants,\n",
    "        \"Treatments\": mentioned_treatments\n",
    "    }\n",
    "variant_cols = category_to_cols[\"Variant\"]\n",
    "treatment_cols = category_to_cols[\"Treatment\"]\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "print(\"Extracting per-paper treatment and variant mentions...\")\n",
    "paper_data = variant_analysis_df.progress_apply(\n",
    "    lambda row: extract_paper_info(row, variant_cols, treatment_cols), axis=1\n",
    ").tolist()\n",
    "print(f\"Extracted data for {len(paper_data):,} papers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67b1f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the type and size\n",
    "print(\"Type of paper_data:\", type(paper_data))\n",
    "print(\"Number of papers:\", len(paper_data))\n",
    "print(\"\\nKeys in first entry:\", paper_data[0].keys())\n",
    "sample = random.choice(paper_data)\n",
    "print(\"\\nSample extracted entry:\")\n",
    "\n",
    "for k, v in sample.items():\n",
    "    if isinstance(v, str):\n",
    "        print(f\"{k}: {v[:200]}...\")\n",
    "    elif isinstance(v, list):\n",
    "        print(f\"{k}: {v[:5]}...\")   \n",
    "    else:\n",
    "        print(f\"{k}: {v}\")           \n",
    "        \n",
    "# Check how many papers have at least one variant and one treatment\n",
    "num_with_both = sum(1 for entry in paper_data if entry[\"Variants\"] and entry[\"Treatments\"])\n",
    "print(f\"\\nPapers with at least one variant AND one treatment: {num_with_both:,}\")\n",
    "\n",
    "# How many papers have neither\n",
    "num_with_neither = sum(1 for entry in paper_data if not entry[\"Variants\"] and not entry[\"Treatments\"])\n",
    "print(f\"Papers with NO variant and NO treatment: {num_with_neither:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e6f57b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, entry in enumerate(paper_data[:5]):\n",
    "    print(f\"--- Paper {i+1} ---\")\n",
    "    print(\"PaperId:\", entry[\"PaperId\"])\n",
    "    print(\"Title:\", entry[\"PaperTitle\"])\n",
    "    print(\"Abstract:\", entry[\"Abstract\"][:200], \"...\") \n",
    "    print(\"Variants:\", entry[\"Variants\"])\n",
    "    print(\"Treatments:\", entry[\"Treatments\"])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17854fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to dataframe\n",
    "paper_df = pd.DataFrame(paper_data)\n",
    "paper_df[\"Variants\"] = paper_df[\"Variants\"].apply(lambda x: \", \".join(x))\n",
    "paper_df[\"Treatments\"] = paper_df[\"Treatments\"].apply(lambda x: \", \".join(x))\n",
    "paper_df.to_csv(\"filtered_paper_data_for_LLM_coassociation.csv\", index=False)\n",
    "print(f\"Saved {len(paper_df):,} entries to 'filtered_paper_data_for_LLM_coassociation.csv'\")\n",
    "print(paper_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97a1e27",
   "metadata": {},
   "source": [
    "# 2) Select and set up LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75357b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a language model to answer the questions\n",
    "!pip install OpenAI\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from openai import OpenAI\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ccd919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models to be tested\n",
    "models = [\"llama31-70b\", \"llama33-70b\", \"deepseek_v3\", \"deepseek_r1\", \"deepseek_r1_distill_llama_70b\"]\n",
    "\n",
    "# Mapping model names to their full Hugging Face or DeepInfra identifiers\n",
    "model_fullnames = {\n",
    "    \"llama31-70b\": \"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n",
    "    \"llama33-70b\": \"meta-llama/Llama-3.3-70B-Instruct\",\n",
    "    \"deepseek_v3\": \"deepseek-ai/DeepSeek-V3\",\n",
    "    \"deepseek_r1\": \"deepseek-ai/DeepSeek-R1\",\n",
    "    \"deepseek_r1_distill_llama_70b\": \"deepseek-ai/DeepSeek-R1-Distill-Llama-70B\"\n",
    "}\n",
    "\n",
    "SYSTEM_MSG = \"You are a helpful medical question answering assistant. Please carefully follow the exact instructions and do not provide explanations.\"\n",
    "modelname = models[1]\n",
    "\n",
    "if modelname in [ \"llama2-3b\" ]: \n",
    "    model, tokenizer = load(model_fullnames[modelname])\n",
    "    def generateFromPrompt(prompt):\n",
    "        if hasattr(tokenizer, \"apply_chat_template\") and tokenizer.chat_template is not None:\n",
    "            messages = [{\"role\": \"system\", \"content\": SYSTEM_MSG},\n",
    "                {\"role\": \"user\", \"content\": prompt}]\n",
    "            prompt = tokenizer.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "            response = generate(model, tokenizer, prompt=prompt, verbose=False)\n",
    "            return response\n",
    "elif modelname in [ \"gpt35\", \"gpt4o\" ]: # OpenAI models\n",
    "    client = OpenAI(\n",
    "       api_key='API_key1' \n",
    "    )\n",
    "    def generateFromPrompt(promptStr,maxTokens=100):\n",
    "      messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_MSG},\n",
    "        {\"role\": \"user\", \"content\": promptStr}\n",
    "      ]\n",
    "      completion = client.chat.completions.create(\n",
    "        model=model_fullnames[modelname],\n",
    "        messages=messages)\n",
    "      response=completion.choices[0].message.content\n",
    "      return(response)\n",
    "elif modelname in [ \"llama31-70b\" , \"llama33-70b\" , \"deepseek_v3\" , \"deepseek_r1\" , \"deepseek_r1_distill_llama_70b\"]:  # DeepInfra models\n",
    "    client = OpenAI(\n",
    "        api_key = \"API_key2\",\n",
    "        base_url=\"https://api.deepinfra.com/v1/openai\",\n",
    "    )\n",
    "    def generateFromPrompt(promptStr,maxTokens=100):\n",
    "      messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_MSG},\n",
    "        {\"role\": \"user\", \"content\": promptStr}\n",
    "      ]\n",
    "      completion = client.chat.completions.create(\n",
    "        model=model_fullnames[modelname],\n",
    "        messages=messages)\n",
    "      response=completion.choices[0].message.content\n",
    "      return(response)\n",
    "    \n",
    "generateFromPrompt(\"hello!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8977f275",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"All installed models:\",   models)\n",
    "print(\"Current model in use:\",   modelname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cb678e",
   "metadata": {},
   "source": [
    "# 3) Define prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfe876d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define prompts\n",
    "\n",
    "PROMPTS = {\n",
    "    0: lambda title, abstract, pairs: (\n",
    "        f\"You are a biomedical research assistant analyzing the relationship between genetic variants and treatments based on scientific publications.\\n\\n\"\n",
    "        f\"Read the following title and abstract carefully, then evaluate each of the variant-treatment pairs listed.\\n\\n\"\n",
    "        f\"Title: {title}\\n\\nAbstract: {abstract}\\n\\n\"\n",
    "        f\"Variant-Treatment pairs:\\n\" +\n",
    "        \"\\n\".join(f\"- {v} + {t}\" for v, t in pairs) + \"\\n\\n\"\n",
    "        f\"For each pair, classify the relationship described in the abstract using only one of the following labels:\\n\"\n",
    "        f\"- Sensitive\\n- Resistant\\n- Diagnostic\\n- Unrelated\\n- Unknown\\n\\n\"\n",
    "        f\"Respond **only** with the list of pairs and their labels, in this format:\\n\"\n",
    "        f\"<variant> + <treatment> : <label>\\n\"\n",
    "    ),\n",
    "\n",
    "    1: lambda title, abstract, pairs: (\n",
    "        f\"You are analyzing biomedical literature to extract clinical relationships between gene variants and drugs.\\n\"\n",
    "        f\"Using the information from the title and abstract, determine whether each of the following variant-treatment pairs has a meaningful clinical association.\\n\\n\"\n",
    "        f\"Paper title: {title}\\nAbstract: {abstract}\\n\\n\"\n",
    "        f\"Variant-treatment pairs:\\n\" +\n",
    "        \"\\n\".join(f\"- {v} + {t}\" for v, t in pairs) + \"\\n\\n\"\n",
    "        f\"Label each pair using one of the following categories:\\n\"\n",
    "        f\"Sensitive, Resistant, Diagnostic, Unrelated, Unknown.\\n\\n\"\n",
    "        f\"Format your output as follows:\\n\"\n",
    "        f\"<variant> + <treatment> : <label>\\n\"\n",
    "    ),\n",
    "\n",
    "    2: lambda title, abstract, pairs: (\n",
    "        f\"Carefully analyze the title and abstract of the following biomedical paper. Then evaluate the relationship between the listed variant-treatment pairs.\\n\\n\"\n",
    "        f\"Use only these labels:\\nSensitive, Resistant, Diagnostic, Unrelated, Unknown.\\n\\n\"\n",
    "        f\"Title: {title}\\n\\nAbstract: {abstract}\\n\\n\"\n",
    "        f\"Pairs to analyze:\\n\" +\n",
    "        \"\\n\".join(f\"- {v} + {t}\" for v, t in pairs) + \"\\n\\n\"\n",
    "        f\"Respond strictly in this format:\\n<variant> + <treatment> : <label>\"\n",
    "    )\n",
    "}\n",
    "\n",
    "print(\"Prompts successfully defined.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449ca54d",
   "metadata": {},
   "source": [
    "# 4) Run genetic variant extraction with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc298f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "os.chdir(variantscape_LLM_coas_directory)\n",
    "tqdm.pandas()\n",
    "\n",
    "BATCH_SIZE = 7524\n",
    "selected_prompt_number = 1\n",
    "modelname = modelname\n",
    "\n",
    "today_date = datetime.today().strftime('%Y-%m-%d')\n",
    "start_time_str = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Set file paths\n",
    "variant_output_file_path = os.path.join(\n",
    "    variantscape_LLM_coas_directory, f\"LLM_variant_screening_{modelname}_prompt{selected_prompt_number}.csv\"\n",
    ")\n",
    "runtime_file = os.path.join(\n",
    "    variantscape_LLM_coas_directory, f\"runtime_summary_{modelname}_prompt{selected_prompt_number}.txt\"\n",
    ")\n",
    "progress_log_file = os.path.join(\n",
    "    variantscape_LLM_coas_directory, f\"progress_log_{modelname}_prompt{selected_prompt_number}.txt\"\n",
    ")\n",
    "\n",
    "def ensure_file_exists(file_path, header_text=None):\n",
    "    if not os.path.exists(file_path):\n",
    "        with open(file_path, \"w\") as f:\n",
    "            if header_text:\n",
    "                f.write(f\"{header_text}\\n\")\n",
    "                f.write(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "ensure_file_exists(runtime_file, f\"### Runtime Log - {today_date} ###\\nStart Time: {start_time_str}\")\n",
    "ensure_file_exists(progress_log_file, f\"### Progress Log - {today_date} ###\")\n",
    "\n",
    "if not os.path.exists(variant_output_file_path):\n",
    "    with open(variant_output_file_path, \"w\") as f:\n",
    "        f.write(\"PaperId,PaperTitle,Abstract,Variants,Treatments,VariantTreatmentPairs,LLM_Prompt,LLM_Response\\n\")\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    filename=progress_log_file,\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "\n",
    "print(\"Success! All necessary files and directories are set up.\")\n",
    "print(\"Script Start Time:\", start_time_str)\n",
    "print(\"Defined prompt number:\", selected_prompt_number)\n",
    "print(f\"Defined batch size: {BATCH_SIZE:,}\")\n",
    "\n",
    "# Define functions\n",
    "def generate_variant_treatment_pairs(row):\n",
    "    variants = [v.strip() for v in row[\"Variants\"].split(\",\") if v.strip()]\n",
    "    treatments = [t.strip() for t in row[\"Treatments\"].split(\",\") if t.strip()]\n",
    "    return list(product(variants, treatments))\n",
    "\n",
    "def screen_publication_for_variants(row, prompt_number):\n",
    "    title = row[\"PaperTitle\"]\n",
    "    abstract = row[\"Abstract\"]\n",
    "    pairs = generate_variant_treatment_pairs(row)\n",
    "    if prompt_number not in PROMPTS:\n",
    "        raise ValueError(f\"Invalid prompt number: {prompt_number}. Choose from 0, 1, 2.\")\n",
    "    prompt = PROMPTS[prompt_number](title, abstract, pairs)\n",
    "    return prompt, pairs\n",
    "\n",
    "def process_with_llm(prompt):\n",
    "    try:\n",
    "        response = generateFromPrompt(prompt)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        logging.error(f\"LLM processing error: {e}\")\n",
    "        return \"ERROR\"\n",
    "\n",
    "print(\"Functions defined and updated for the current dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470b4e9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ========================== RESUME FROM LAST CHECKPOINT ========================== #\n",
    "\n",
    "# Check if paper_df is already loaded\n",
    "if 'paper_df' not in globals():\n",
    "    raise ValueError(\"Dataset `paper_df` is not loaded in memory. Make sure it's defined before running the script.\")\n",
    "if 'PaperId' not in paper_df.columns:\n",
    "    raise KeyError(\"Dataset must contain a 'PaperId' column to track progress.\")\n",
    "paper_df['PaperId'] = paper_df['PaperId'].astype(str).str.strip()\n",
    "\n",
    "if os.path.exists(variant_output_file_path) and os.path.getsize(variant_output_file_path) > 100:\n",
    "    processed_df = pd.read_csv(variant_output_file_path)\n",
    "    processed_df.columns = processed_df.columns.str.strip().str.replace('\\ufeff', '')\n",
    "    processed_df['PaperId'] = processed_df['PaperId'].astype(str).str.strip()\n",
    "\n",
    "    processed_df = processed_df.drop_duplicates(subset=[\"PaperId\"])\n",
    "    processed_articles = set(processed_df['PaperId'])\n",
    "    total_processed_articles = len(processed_articles)\n",
    "\n",
    "    print(f\"Resuming from last processed row. {total_processed_articles:,} articles already processed.\")\n",
    "else:\n",
    "    processed_articles = set()\n",
    "    total_processed_articles = 0\n",
    "    print(\"Starting fresh processing.\")\n",
    "\n",
    "\n",
    "unprocessed_df = paper_df[~paper_df['PaperId'].isin(processed_articles)].copy()\n",
    "total_articles = len(unprocessed_df)\n",
    "total_batches = (total_articles // BATCH_SIZE) + (1 if total_articles % BATCH_SIZE != 0 else 0)\n",
    "print(f\"Unprocessed articles remaining: {total_articles:,} in {total_batches:,} batches\")\n",
    "\n",
    "if os.path.exists(variant_output_file_path) and os.path.getsize(variant_output_file_path) > 100:\n",
    "    processed_df = pd.read_csv(variant_output_file_path)\n",
    "    processed_df.columns = processed_df.columns.str.strip().str.replace('\\ufeff', '') \n",
    "    if 'PaperId' not in processed_df.columns:\n",
    "        raise KeyError(\"CSV exists but does not contain 'PaperId'. Check batch_to_save column selection.\")\n",
    "    processed_articles = set(processed_df['PaperId'])  \n",
    "    total_processed_articles = len(processed_articles)\n",
    "    print(f\"Resuming from last processed row. {total_processed_articles} articles completed so far.\")\n",
    "else:\n",
    "    processed_articles = set()\n",
    "    total_processed_articles = 0\n",
    "    print(\"Starting fresh processing.\")\n",
    "\n",
    "total_batches = (len(paper_df) // BATCH_SIZE) + (1 if len(paper_df) % BATCH_SIZE != 0 else 0)\n",
    "\n",
    "# If all articles are processed, stop\n",
    "if total_processed_articles == len(paper_df):\n",
    "    print(\"\\nAll batches are complete. No more articles to process.\")\n",
    "    print(\"You have successfully processed the entire dataset.\")\n",
    "    try:\n",
    "        sys.exit(0)\n",
    "    except SystemExit:\n",
    "        pass \n",
    "\n",
    "unprocessed_df = paper_df[~paper_df['PaperId'].isin(processed_articles)]\n",
    "total_articles = len(unprocessed_df)\n",
    "print(\"Success! All necessary files and directories are set up.\")\n",
    "print(\"Defined prompt number:\", selected_prompt_number)\n",
    "print(f\"Defined batch size to run in chunks: {BATCH_SIZE:,}\")\n",
    "print(f\"Total unprocessed articles: {total_articles:,}\")\n",
    "\n",
    "# Track cumulative runtime \n",
    "if os.path.exists(runtime_file):\n",
    "    with open(runtime_file, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        total_runtime_previous = sum(\n",
    "            float(line.split(\":\")[-1].strip().split()[0])\n",
    "            for line in lines if \"Total runtime so far\" in line\n",
    "        )\n",
    "else:\n",
    "    total_runtime_previous = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c72a65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ========================== BATCH PROCESSING ========================== #\n",
    "# Fallbacks in case previous setup blocks were not yet run\n",
    "if 'total_processed_articles' not in globals():\n",
    "    total_processed_articles = 0\n",
    "if 'total_runtime_previous' not in globals():\n",
    "    total_runtime_previous = 0.0\n",
    "if 'unprocessed_df' not in globals():\n",
    "    unprocessed_df = paper_df.copy()\n",
    "if 'total_articles' not in globals():\n",
    "    total_articles = len(unprocessed_df)\n",
    "if 'total_batches' not in globals():\n",
    "    total_batches = (len(unprocessed_df) // BATCH_SIZE) + (1 if len(unprocessed_df) % BATCH_SIZE != 0 else 0)\n",
    "\n",
    "start_time = time.time()\n",
    "batch_number = (total_processed_articles // BATCH_SIZE) + 1\n",
    "for batch_start in range(0, total_articles, BATCH_SIZE):\n",
    "    batch_end = min(batch_start + BATCH_SIZE, total_articles)\n",
    "    batch = unprocessed_df.iloc[batch_start:batch_end].copy()\n",
    "    print(f\"\\nProcessing Batch {batch_number}/{total_batches} ({batch_start + 1} to {batch_end})...\")\n",
    "    batch_start_time = time.time()\n",
    "    batch[\"Prompt_Pair\"] = batch.apply(lambda row: screen_publication_for_variants(row, selected_prompt_number), axis=1)\n",
    "    batch[[\"LLM_Prompt\", \"VariantTreatmentPairs\"]] = pd.DataFrame(batch[\"Prompt_Pair\"].tolist(), index=batch.index)\n",
    "\n",
    "    # Query the LLM\n",
    "    llm_response_column = f'LLM_Response_{modelname}'\n",
    "    batch[llm_response_column] = batch['LLM_Prompt'].progress_apply(process_with_llm)\n",
    "    batch_runtime = time.time() - batch_start_time\n",
    "    batch_to_save = batch[['PaperId', 'PaperTitle', 'Abstract', 'Variants', 'Treatments', 'VariantTreatmentPairs', 'LLM_Prompt', llm_response_column]]\n",
    "    if os.path.exists(variant_output_file_path):\n",
    "        batch_to_save.to_csv(variant_output_file_path, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        batch_to_save.to_csv(variant_output_file_path, mode='w', index=False)\n",
    "    total_runtime_so_far = total_runtime_previous + (time.time() - start_time)\n",
    "    with open(runtime_file, \"a\") as f:\n",
    "        f.write(f\"\\nBatch {batch_number}/{total_batches} started at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Batch Runtime: {batch_runtime:.2f} sec\\n\")\n",
    "        f.write(f\"Total runtime so far (all runs combined): {total_runtime_so_far:.2f} sec\\n\")\n",
    "        f.write(f\"Total articles processed in this batch: {batch_end - batch_start}\\n\")\n",
    "        f.write(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "    with open(progress_log_file, \"a\") as f:\n",
    "        f.write(f\"Completed Batch {batch_number} at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "\n",
    "    logging.info(f\"Processed batch {batch_number}/{total_batches} in {batch_runtime:.2f} sec.\")\n",
    "\n",
    "    articles_in_batch = batch_end - batch_start\n",
    "    total_articles_processed_now = total_processed_articles + articles_in_batch\n",
    "    processed_percentage = (total_articles_processed_now / len(paper_df)) * 100\n",
    "    remaining_articles = len(paper_df) - total_articles_processed_now\n",
    "    remaining_percentage = (remaining_articles / len(paper_df)) * 100\n",
    "\n",
    "    def generate_progress_bar(percentage, bar_length=20):\n",
    "        filled_length = int(bar_length * percentage / 100)\n",
    "        bar = '|' * filled_length + '-' * (bar_length - filled_length)\n",
    "        return f\"[{bar}] {percentage:.2f}%\"\n",
    "    print(f\"\\nPaused! Batch {batch_number} completed.\")\n",
    "    print(f\"Total processed: {total_articles_processed_now:,} {generate_progress_bar(processed_percentage)}\")\n",
    "    print(f\"Remaining: {remaining_articles:,} {generate_progress_bar(remaining_percentage)}\")\n",
    "    print(\"Check the CSV and runtime file. Re-run this cell to continue with the next batch.\")\n",
    "\n",
    "    total_processed_articles = total_articles_processed_now\n",
    "    batch_number += 1\n",
    "    break\n",
    "\n",
    "# ========================== FINAL SUMMARY ========================== #\n",
    "total_runtime = total_runtime_so_far\n",
    "total_hours = total_runtime // 3600\n",
    "total_minutes = (total_runtime % 3600) // 60\n",
    "total_seconds = total_runtime % 60\n",
    "\n",
    "summary_text = f\"\"\"\n",
    "### Genetic Variant-Treatment Screening Summary ###\n",
    "\n",
    "- Model used: {modelname}\n",
    "- Prompt number: {selected_prompt_number}\n",
    "- Total batches processed: {batch_number - 1:,}/{total_batches:,}\n",
    "- Total articles processed: {total_processed_articles:,}\n",
    "- Batch runtime: {batch_runtime:.2f} sec\n",
    "- Cumulative runtime: {total_runtime:.2f} sec ({total_hours:.0f} hr {total_minutes:.0f} min {total_seconds:.2f} sec)\n",
    "\"\"\"\n",
    "print(summary_text)\n",
    "with open(runtime_file, \"a\") as f:\n",
    "    f.write(\"\\n### Final runtime summary ###\\n\")\n",
    "    f.write(f\"End Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    f.write(summary_text)\n",
    "    f.write(\"\\n\" + \"=\" * 60 + \"\\n\")\n",
    "print(\"Final results saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd4dd3f",
   "metadata": {},
   "source": [
    "# RERUN FROM LAST CHECKPOINT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e62b571",
   "metadata": {},
   "source": [
    "# ============================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c32748a",
   "metadata": {},
   "source": [
    "# 6) Evaluation of LLMs and prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cab4bd",
   "metadata": {},
   "source": [
    "## 6.1) Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495ecb62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load CSVs with different prompts for e3valuation\n",
    "df_prompt0 = pd.read_csv(os.path.join(variantscape_LLM_coas_directory, f\"LLM_variant_screening_{modelname}_prompt0.csv\"))\n",
    "df_prompt1 = pd.read_csv(os.path.join(variantscape_LLM_coas_directory, f\"LLM_variant_screening_{modelname}_prompt1.csv\"))\n",
    "df_prompt2 = pd.read_csv(os.path.join(variantscape_LLM_coas_directory, f\"LLM_variant_screening_{modelname}_prompt2.csv\"))\n",
    "\n",
    "print(\"CSV shapes:\")\n",
    "print(\"Prompt 0:\", df_prompt0.shape)\n",
    "print(\"Prompt 1:\", df_prompt1.shape)\n",
    "print(\"Prompt 2:\", df_prompt2.shape)\n",
    "\n",
    "\n",
    "def extract_runtime(prompt_num):\n",
    "    path = os.path.join(variantscape_LLM_coas_directory, f\"runtime_summary_{modelname}_prompt{prompt_num}.txt\")\n",
    "    if not os.path.exists(path):\n",
    "        return f\"Runtime file for prompt {prompt_num} not found.\"\n",
    "    with open(path) as f:\n",
    "        lines = f.readlines()\n",
    "    final_summary = [line for line in lines if \"Cumulative runtime\" in line]\n",
    "    return final_summary[-1].strip() if final_summary else \"No runtime summary found.\"\n",
    "\n",
    "print(\"\\nRuntime summaries:\")\n",
    "print(\"Prompt 0:\", extract_runtime(0))\n",
    "print(\"Prompt 1:\", extract_runtime(1))\n",
    "print(\"Prompt 2:\", extract_runtime(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0b4f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_llm_comparison(df, num_rows=5, maxlen=120):\n",
    "    for i in range(min(num_rows, len(df))):\n",
    "        title = df.iloc[i][\"PaperTitle\"][:80]\n",
    "        p0 = df.iloc[i][\"Prompt0\"][:maxlen].replace(\"\\n\", \" \") + (\"...\" if len(df.iloc[i][\"Prompt0\"]) > maxlen else \"\")\n",
    "        p1 = df.iloc[i][\"Prompt1\"][:maxlen].replace(\"\\n\", \" \") + (\"...\" if len(df.iloc[i][\"Prompt1\"]) > maxlen else \"\")\n",
    "        p2 = df.iloc[i][\"Prompt2\"][:maxlen].replace(\"\\n\", \" \") + (\"...\" if len(df.iloc[i][\"Prompt2\"]) > maxlen else \"\")\n",
    "\n",
    "        print(f\"\\n--- Sample {i+1} ---\")\n",
    "        print(f\"Title: {title}\")\n",
    "        print(f\"Prompt 0: {p0}\")\n",
    "        print(f\"Prompt 1: {p1}\")\n",
    "        print(f\"Prompt 2: {p2}\")\n",
    "\n",
    "display_llm_comparison(df_compare_trunc, num_rows=5, maxlen=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1592e3c9",
   "metadata": {},
   "source": [
    "## 6.2) Calculate confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d1f126",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df_prompt0 = pd.read_csv(f\"{variantscape_LLM_coas_directory}/LLM_variant_screening_llama33-70b_prompt0.csv\")\n",
    "df_prompt1 = pd.read_csv(f\"{variantscape_LLM_coas_directory}/LLM_variant_screening_llama33-70b_prompt1.csv\")\n",
    "df_prompt2 = pd.read_csv(f\"{variantscape_LLM_coas_directory}/LLM_variant_screening_llama33-70b_prompt2.csv\")\n",
    "\n",
    "resp_col_0 = \"LLM_Response\"\n",
    "resp_col_1 = \"LLM_Response\"\n",
    "resp_col_2 = \"LLM_Response\"\n",
    "\n",
    "def split_predictions(text):\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return []\n",
    "    return [s.strip() for s in text.strip().split('\\n') if '+' in s and ':' in s]\n",
    "\n",
    "# Count total predictions\n",
    "df_prompt0_preds = df_prompt0[resp_col_0].apply(split_predictions)\n",
    "df_prompt1_preds = df_prompt1[resp_col_1].apply(split_predictions)\n",
    "df_prompt2_preds = df_prompt2[resp_col_2].apply(split_predictions)\n",
    "\n",
    "flat_0 = [pred for sublist in df_prompt0_preds.tolist() for pred in sublist]\n",
    "flat_1 = [pred for sublist in df_prompt1_preds.tolist() for pred in sublist]\n",
    "flat_2 = [pred for sublist in df_prompt2_preds.tolist() for pred in sublist]\n",
    "\n",
    "print(f\"Prompt 0: {len(flat_0):,} total variant-treatment predictions\")\n",
    "print(f\"Prompt 1: {len(flat_1):,} total variant-treatment predictions\")\n",
    "print(f\"Prompt 2: {len(flat_2):,} total variant-treatment predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c89df5e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define a function to extract variant-treatment pairs from LLM responses\n",
    "def extract_variant_treatment_pairs(response):\n",
    "    \"\"\"Extracts the variant-treatment pairs from LLM responses.\"\"\"\n",
    "    pairs = []\n",
    "    for line in response.split(\"\\n\"):\n",
    "        match = re.match(r\"(\\S.+?)\\s*:\\s*(\\w+)\", line.strip())\n",
    "        if match:\n",
    "            variant_treatment = match.group(1)\n",
    "            prediction = match.group(2)\n",
    "            pairs.append((variant_treatment, prediction))\n",
    "    return pairs\n",
    "\n",
    "# Apply the function to all responses in the three prompts\n",
    "df_prompt0['Predictions'] = df_prompt0['LLM_Response'].apply(extract_variant_treatment_pairs)\n",
    "df_prompt1['Predictions'] = df_prompt1['LLM_Response'].apply(extract_variant_treatment_pairs)\n",
    "df_prompt2['Predictions'] = df_prompt2['LLM_Response'].apply(extract_variant_treatment_pairs)\n",
    "comparison = []\n",
    "\n",
    "for i in range(len(df_prompt0)):\n",
    "    pairs_prompt0 = df_prompt0.iloc[i]['Predictions']\n",
    "    pairs_prompt1 = df_prompt1.iloc[i]['Predictions']\n",
    "    pairs_prompt2 = df_prompt2.iloc[i]['Predictions']\n",
    "    for p0, p1, p2 in zip(pairs_prompt0, pairs_prompt1, pairs_prompt2):\n",
    "        variant_treatment_0, prediction_0 = p0\n",
    "        variant_treatment_1, prediction_1 = p1\n",
    "        variant_treatment_2, prediction_2 = p2\n",
    "        \n",
    "        comparison.append({\n",
    "            'PaperId': df_prompt0.iloc[i]['PaperId'],\n",
    "            'Variant_Treatment': variant_treatment_0,\n",
    "            'Prompt0_Prediction': prediction_0,\n",
    "            'Prompt1_Prediction': prediction_1,\n",
    "            'Prompt2_Prediction': prediction_2,\n",
    "            'Agreement_0_1': prediction_0 == prediction_1,\n",
    "            'Agreement_0_2': prediction_0 == prediction_2,\n",
    "            'Agreement_1_2': prediction_1 == prediction_2\n",
    "        })\n",
    "df_comparison = pd.DataFrame(comparison)\n",
    "\n",
    "def agreement_rate(col1, col2):\n",
    "    return (df_comparison[col1] == True).mean()\n",
    "\n",
    "print(\"Agreement rates:\")\n",
    "print(f\"Prompt 0 vs 1: {agreement_rate('Agreement_0_1', 'Agreement_0_1'):.2%}\")\n",
    "print(f\"Prompt 0 vs 2: {agreement_rate('Agreement_0_2', 'Agreement_0_2'):.2%}\")\n",
    "print(f\"Prompt 1 vs 2: {agreement_rate('Agreement_1_2', 'Agreement_1_2'):.2%}\")\n",
    "\n",
    "\n",
    "df_disagree = df_comparison[\n",
    "    (df_comparison[\"Agreement_0_1\"] == False) |\n",
    "    (df_comparison[\"Agreement_0_2\"] == False) |\n",
    "    (df_comparison[\"Agreement_1_2\"] == False)\n",
    "]\n",
    "\n",
    "print(f\"\\nTotal disagreements: {len(df_disagree)} / {len(df_comparison)}\")\n",
    "print(df_disagree.head(10)) \n",
    "\n",
    "# Get the unique labels (predictions) from all three prompts\n",
    "labels = sorted(set(df_comparison['Prompt0_Prediction'].unique()) |\n",
    "                set(df_comparison['Prompt1_Prediction'].unique()) |\n",
    "                set(df_comparison['Prompt2_Prediction'].unique()))\n",
    "\n",
    "# Generate confusion matrix for Prompt 0 vs Prompt 1\n",
    "cm_0_1 = confusion_matrix(df_comparison['Prompt0_Prediction'], df_comparison['Prompt1_Prediction'], labels=labels)\n",
    "cm_0_1_df = pd.DataFrame(cm_0_1, index=labels, columns=labels)\n",
    "disagreements_0_1 = cm_0_1.sum() - cm_0_1.diagonal().sum()\n",
    "\n",
    "print(f\"\\nConfusion Matrix (Prompt 0 vs Prompt 1) - Disagreements: {disagreements_0_1}\")\n",
    "print(cm_0_1_df)\n",
    "\n",
    "# Generate confusion matrix for Prompt 1 vs Prompt 2\n",
    "cm_1_2 = confusion_matrix(df_comparison['Prompt1_Prediction'], df_comparison['Prompt2_Prediction'], labels=labels)\n",
    "cm_1_2_df = pd.DataFrame(cm_1_2, index=labels, columns=labels)\n",
    "\n",
    "# Count disagreements for Prompt 1 vs Prompt 2\n",
    "disagreements_1_2 = cm_1_2.sum() - cm_1_2.diagonal().sum()\n",
    "\n",
    "print(f\"\\nConfusion Matrix (Prompt 1 vs Prompt 2) - Disagreements: {disagreements_1_2}\")\n",
    "print(cm_1_2_df)\n",
    "\n",
    "# Generate confusion matrix for Prompt 0 vs Prompt 2\n",
    "cm_0_2 = confusion_matrix(df_comparison['Prompt0_Prediction'], df_comparison['Prompt2_Prediction'], labels=labels)\n",
    "cm_0_2_df = pd.DataFrame(cm_0_2, index=labels, columns=labels)\n",
    "\n",
    "# Count disagreements for Prompt 0 vs Prompt 2\n",
    "disagreements_0_2 = cm_0_2.sum() - cm_0_2.diagonal().sum()\n",
    "print(f\"\\nConfusion Matrix (Prompt 0 vs Prompt 2) - Disagreements: {disagreements_0_2}\")\n",
    "print(cm_0_2_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c0cba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display disagreements with the full abstract for each comparison\n",
    "df_disagree_with_abstract = df_comparison[\n",
    "    (df_comparison[\"Agreement_0_1\"] == False) |\n",
    "    (df_comparison[\"Agreement_0_2\"] == False) |\n",
    "    (df_comparison[\"Agreement_1_2\"] == False)\n",
    "]\n",
    "df_disagree_with_abstract = df_disagree_with_abstract.merge(df_prompt0[['PaperId', 'Abstract']], on='PaperId', how='left')\n",
    "\n",
    "print(f\"\\nTotal disagreements: {len(df_disagree_with_abstract)} / {len(df_comparison)}\")\n",
    "print(\"\\nDisagreements with Full Abstract:\")\n",
    "\n",
    "for index, row in df_disagree_with_abstract.iterrows():\n",
    "    print(f\"PaperId: {row['PaperId']}\")\n",
    "    print(f\"Variant-Treatment: {row['Variant_Treatment']}\")\n",
    "    print(f\"Prompt 0 Prediction: {row['Prompt0_Prediction']}\")\n",
    "    print(f\"Prompt 1 Prediction: {row['Prompt1_Prediction']}\")\n",
    "    print(f\"Prompt 2 Prediction: {row['Prompt2_Prediction']}\")\n",
    "    print(f\"Agreement 0 vs 1: {row['Agreement_0_1']}\")\n",
    "    print(f\"Agreement 0 vs 2: {row['Agreement_0_2']}\")\n",
    "    print(f\"Agreement 1 vs 2: {row['Agreement_1_2']}\")\n",
    "    print(f\"Full Abstract: {row['Abstract']}\")\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "os.chdir(variantscape_LLM_coas_directory)\n",
    "df_disagree_with_abstract.to_csv(\"evaluation_of_llm_coassociations_prompts.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2288bb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evalution: Prompt #1 is the most specific one!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1256f3f4",
   "metadata": {},
   "source": [
    "# ============================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5216956",
   "metadata": {},
   "source": [
    "# 7) Investigate dataset and normalize, find consensus of all coassociations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76da9e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate LLM dataset\n",
    "csv_path = \"LLM_variant_screening_llama33-70b_prompt1.csv\"\n",
    "variant_coassociation_LLM_df = pd.read_csv(csv_path)\n",
    "variant_coassociation_LLM_df.columns = variant_coassociation_LLM_df.columns.str.strip()\n",
    "\n",
    "print(f\"Loaded {len(variant_coassociation_LLM_df):,} rows\")\n",
    "print(\"Columns:\", variant_coassociation_LLM_df.columns.tolist())\n",
    "print(\"Shape of the dataset:\", variant_coassociation_LLM_df.shape)\n",
    "\n",
    "print(\"\\nSample rows:\")\n",
    "print(variant_coassociation_LLM_df.head())\n",
    "\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(variant_coassociation_LLM_df.isnull().sum())\n",
    "\n",
    "empty_rows = variant_coassociation_LLM_df[variant_coassociation_LLM_df.isnull().all(axis=1)]\n",
    "print(f\"\\nNumber of completely empty rows: {len(empty_rows)}\")\n",
    "\n",
    "duplicate_paperids = variant_coassociation_LLM_df[variant_coassociation_LLM_df.duplicated(subset='PaperId')]\n",
    "print(f\"Number of duplicated PaperId rows: {len(duplicate_paperids)}\")\n",
    "\n",
    "print(\"\\nDuplicated PaperIds and their counts:\")\n",
    "print(variant_coassociation_LLM_df['PaperId'].value_counts()[variant_coassociation_LLM_df['PaperId'].value_counts() > 1])\n",
    "\n",
    "llm_col = [col for col in variant_coassociation_LLM_df.columns if col.startswith(\"LLM_Response\")]\n",
    "if llm_col:\n",
    "    llm_response_column = llm_col[0]\n",
    "    \n",
    "    print(f\"\\nSample LLM responses from column: {llm_response_column}\")\n",
    "    print(variant_coassociation_LLM_df[llm_response_column].dropna().sample(3, random_state=42).values)\n",
    "\n",
    "    sample = variant_coassociation_LLM_df.sample(3)\n",
    "    for i, row in sample.iterrows():\n",
    "        print(f\"\\n--- Sample {i+1} ---\")\n",
    "        print(f\"Title: {row.get('PaperTitle', '[Missing]')}\")\n",
    "        print(f\"Abstract: {row.get('Abstract', '[Missing]')}\")\n",
    "        print(f\"Variants: {row.get('Variants', '[Missing]')}\")\n",
    "        print(f\"Treatments: {row.get('Treatments', '[Missing]')}\")\n",
    "        print(f\"LLM Response:\\n{row.get(llm_response_column, '[Missing]')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb83818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract LLM resposne\n",
    "def extract_variant_treatment_prediction(response):\n",
    "    \"\"\"Extracts (Variant, Treatment, Prediction) tuples from a raw LLM response.\"\"\"\n",
    "    results = []\n",
    "    if pd.isna(response) or not isinstance(response, str):\n",
    "        return results\n",
    "    for line in response.split(\"\\n\"):\n",
    "        match = re.match(r\"(.+?)\\s*\\+\\s*(.+?)\\s*:\\s*(\\w+)\", line.strip())\n",
    "        if match:\n",
    "            variant = match.group(1).strip()\n",
    "            treatment = match.group(2).strip()\n",
    "            prediction = match.group(3).strip()\n",
    "            results.append((variant, treatment, prediction))\n",
    "    return results\n",
    "\n",
    "variant_coassociation_LLM_df[\"Parsed_Triples\"] = variant_coassociation_LLM_df[\"LLM_Response\"].apply(extract_variant_treatment_prediction)\n",
    "flat_records = []\n",
    "for idx, row in variant_coassociation_LLM_df.iterrows():\n",
    "    paper_id = row.get(\"PaperId\", None)\n",
    "    for variant, treatment, prediction in row[\"Parsed_Triples\"]:\n",
    "        flat_records.append({\n",
    "            \"PaperId\": paper_id,\n",
    "            \"Variant\": variant,\n",
    "            \"Treatment\": treatment,\n",
    "            \"Prediction\": prediction\n",
    "        })\n",
    "\n",
    "df_llm_extracted = pd.DataFrame(flat_records)\n",
    "print(df_llm_extracted.head())\n",
    "print(f\"\\nTotal variant-treatment-prediction entries extracted: {len(df_llm_extracted):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae200399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the extraction function\n",
    "def extract_variant_treatment_prediction(response):\n",
    "    results = []\n",
    "    if pd.isna(response) or not isinstance(response, str):\n",
    "        return results\n",
    "    for line in response.split(\"\\n\"):\n",
    "        match = re.match(r\"(.+?)\\s*\\+\\s*(.+?)\\s*:\\s*(\\w+)\", line.strip())\n",
    "        if match:\n",
    "            variant = match.group(1).strip()\n",
    "            treatment = match.group(2).strip()\n",
    "            prediction = match.group(3).strip()\n",
    "            results.append((variant, treatment, prediction))\n",
    "    return results\n",
    "\n",
    "variant_coassociation_LLM_df[\"Parsed_Triples\"] = variant_coassociation_LLM_df[\"LLM_Response\"].apply(extract_variant_treatment_prediction)\n",
    "flat_records = []\n",
    "for idx, row in variant_coassociation_LLM_df.iterrows():\n",
    "    paper_id = row.get(\"PaperId\", None)\n",
    "    parsed_triples = row.get(\"Parsed_Triples\", [])\n",
    "    for variant, treatment, prediction in parsed_triples:\n",
    "        flat_records.append({\n",
    "            \"PaperId\": paper_id,\n",
    "            \"Variant\": variant,\n",
    "            \"Treatment\": treatment,\n",
    "            \"Prediction\": prediction\n",
    "        })\n",
    "\n",
    "df_llm_extracted = pd.DataFrame(flat_records)\n",
    "print(\" Preview of extracted variant-treatment-prediction entries:\")\n",
    "print(df_llm_extracted.head())\n",
    "print(f\"\\nTotal variant-treatment-prediction entries extracted: {len(df_llm_extracted):,}\")\n",
    "\n",
    "# Clean variant strings\n",
    "def clean_variant(variant):\n",
    "    if pd.isna(variant):\n",
    "        return \"\"\n",
    "    variant = variant.strip().lower()\n",
    "    variant = re.sub(r\"[^\\w\\s]\", \"\", variant)\n",
    "    variant = re.sub(r\"\\s+\", \"\", variant)\n",
    "    variant = variant.replace(\"__\", \"_\")\n",
    "    return variant\n",
    "\n",
    "df_llm_extracted[\"Variant_Clean\"] = df_llm_extracted[\"Variant\"].apply(clean_variant)\n",
    "\n",
    "# Create Variant_Treatment_Pair column\n",
    "df_llm_extracted[\"Variant_Treatment_Pair\"] = (\n",
    "    df_llm_extracted[\"Variant_Clean\"].str.strip() + \" + \" +\n",
    "    df_llm_extracted[\"Treatment\"].str.strip().str.lower()\n",
    ")\n",
    "\n",
    "# Count predictions per pair\n",
    "prediction_counts = (\n",
    "    df_llm_extracted\n",
    "    .groupby([\"Variant_Treatment_Pair\", \"Prediction\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"Count\")\n",
    ")\n",
    "\n",
    "# Total predictions per pair\n",
    "total_counts = (\n",
    "    df_llm_extracted\n",
    "    .groupby(\"Variant_Treatment_Pair\")\n",
    "    .size()\n",
    "    .reset_index(name=\"Total\")\n",
    ")\n",
    "\n",
    "merged = prediction_counts.merge(total_counts, on=\"Variant_Treatment_Pair\")\n",
    "merged[\"Is_Consensus\"] = merged[\"Count\"] == merged[\"Total\"]\n",
    "consensus_only = merged[merged[\"Is_Consensus\"]].copy()\n",
    "print(\"\\n=== Variant + Treatment Pairs with 100% LLM Prediction Consensus ===\")\n",
    "print(consensus_only.sort_values(by=\"Count\", ascending=False).head(20))\n",
    "\n",
    "total_pairs = merged[\"Variant_Treatment_Pair\"].nunique()\n",
    "non_consensus_pairs = merged[~merged[\"Is_Consensus\"]][\"Variant_Treatment_Pair\"].nunique()\n",
    "consensus_pairs = total_pairs - non_consensus_pairs\n",
    "\n",
    "print(f\"\\n Total unique Variant + Treatment pairs: {total_pairs}\")\n",
    "print(f\" Pairs WITHOUT full consensus: {non_consensus_pairs}\")\n",
    "print(f\" Pairs WITH full consensus: {consensus_pairs}\")\n",
    "\n",
    "# Show non-consensus rows\n",
    "no_consensus = merged[~merged[\"Is_Consensus\"]].copy()\n",
    "no_consensus_sorted = no_consensus.sort_values(by=\"Total\", ascending=False)\n",
    "print(\"\\n===  Variant + Treatment Pairs WITHOUT LLM Prediction Consensus ===\")\n",
    "print(no_consensus_sorted.head(20))\n",
    "\n",
    "# Crosscheck 1: Unparsed but non-empty responses\n",
    "unparsed_count = (\n",
    "    variant_coassociation_LLM_df[\"Parsed_Triples\"]\n",
    "    .apply(lambda x: len(x) == 0)\n",
    "    .sum()\n",
    ")\n",
    "print(f\"\\n Number of LLM responses that returned ZERO parsed triples: {unparsed_count}\")\n",
    "\n",
    "# Crosscheck 2: Duplicated variant-treatment pairs\n",
    "dupe_check = df_llm_extracted.duplicated(subset=[\"Variant_Treatment_Pair\", \"Prediction\"], keep=False)\n",
    "if dupe_check.any():\n",
    "    print(\"\\n Duplicate Variant-Treatment-Prediction rows found:\")\n",
    "    print(df_llm_extracted[dupe_check].head())\n",
    "else:\n",
    "    print(\"\\n No duplicate Variant-Treatment-Prediction rows found.\")\n",
    "\n",
    "# Crosscheck 3: Prediction class distribution\n",
    "print(\"\\n Prediction label breakdown:\")\n",
    "print(df_llm_extracted[\"Prediction\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266b8f69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Building consensus labels for variant-treatment predictions\n",
    "valid_preds = [\"Sensitive\", \"Resistant\", \"Diagnostic\"]\n",
    "merged[\"Proportion\"] = merged[\"Count\"] / merged[\"Total\"]\n",
    "merged_sorted = merged.sort_values([\"Variant_Treatment_Pair\", \"Proportion\"], ascending=[True, False])\n",
    "dominant_per_pair = merged_sorted.drop_duplicates(\"Variant_Treatment_Pair\").copy()\n",
    "dominant_per_pair.loc[:, \"Soft_Consensus\"] = (\n",
    "    (dominant_per_pair[\"Proportion\"] >= 0.60) &\n",
    "    (dominant_per_pair[\"Total\"] >= 3) &\n",
    "    (dominant_per_pair[\"Prediction\"].isin(valid_preds))\n",
    ")\n",
    "\n",
    "soft_consensus_total = dominant_per_pair[\"Soft_Consensus\"].sum()\n",
    "soft_consensus_percent = 100 * soft_consensus_total / dominant_per_pair.shape[0]\n",
    "print(f\"Soft consensus pairs (custom rules): {soft_consensus_total}\")\n",
    "print(f\"Percentage of all Variant+Treatment pairs: {soft_consensus_percent:.1f}%\")\n",
    "\n",
    "hard_consensus_pairs = set(merged.loc[merged[\"Is_Consensus\"], \"Variant_Treatment_Pair\"].unique())\n",
    "soft_consensus_pairs = set(dominant_per_pair.loc[dominant_per_pair[\"Soft_Consensus\"], \"Variant_Treatment_Pair\"].unique())\n",
    "total_pairs = df_llm_extracted[\"Variant_Treatment_Pair\"].nunique()\n",
    "all_consensus_pairs = hard_consensus_pairs.union(soft_consensus_pairs)\n",
    "no_consensus_pairs = set(df_llm_extracted[\"Variant_Treatment_Pair\"].unique()) - all_consensus_pairs\n",
    "print(f\"\\nTotal Variant + Treatment pairs: {total_pairs}\")\n",
    "print(f\"Hard consensus pairs: {len(hard_consensus_pairs)}\")\n",
    "print(f\"Soft-only consensus pairs: {len(soft_consensus_pairs - hard_consensus_pairs)}\")\n",
    "print(f\"Total pairs with any consensus (before fallback): {len(all_consensus_pairs)}\")\n",
    "print(f\"Pairs WITHOUT any consensus (before fallback): {len(no_consensus_pairs)}\")\n",
    "\n",
    "# Fallback consensus resolution\n",
    "\n",
    "df_no_consensus = df_llm_extracted[df_llm_extracted[\"Variant_Treatment_Pair\"].isin(no_consensus_pairs)].copy()\n",
    "\n",
    "fallback_results = []\n",
    "for pair, group in df_no_consensus.groupby(\"Variant_Treatment_Pair\"):\n",
    "    label_counts = group[\"Prediction\"].value_counts()\n",
    "    unique_labels = label_counts.index.tolist()\n",
    "\n",
    "    valid_labels = {\"Sensitive\", \"Resistant\", \"Diagnostic\"}\n",
    "    weak_labels = {\"Unknown\", \"Unrelated\"}\n",
    "\n",
    "    present_valid = [label for label in unique_labels if label in valid_labels]\n",
    "    present_weak = [label for label in unique_labels if label in weak_labels]\n",
    "\n",
    "    if set(unique_labels).issubset(weak_labels):\n",
    "        if label_counts.get(\"Unknown\", 0) > label_counts.get(\"Unrelated\", 0):\n",
    "            fallback_results.append((pair, \"Unknown\"))\n",
    "        elif label_counts.get(\"Unrelated\", 0) > label_counts.get(\"Unknown\", 0):\n",
    "            fallback_results.append((pair, \"Unrelated\"))\n",
    "        else:\n",
    "            fallback_results.append((pair, \"Unknown\"))\n",
    "    elif len(unique_labels) == 1:\n",
    "        fallback_results.append((pair, unique_labels[0]))\n",
    "    elif present_valid and present_weak:\n",
    "        top_valid_label = label_counts.loc[present_valid].idxmax()\n",
    "        fallback_results.append((pair, top_valid_label))\n",
    "    else:\n",
    "        fallback_results.append((pair, \"No consensus\"))\n",
    "\n",
    "df_fallback_consensus = pd.DataFrame(fallback_results, columns=[\"Variant_Treatment_Pair\", \"Resolved_Prediction\"])\n",
    "\n",
    "# Final consensus assembly\n",
    "\n",
    "hard_labels = merged[merged[\"Is_Consensus\"]].copy()\n",
    "hard_labels = hard_labels.sort_values([\"Variant_Treatment_Pair\", \"Count\"], ascending=[True, False])\n",
    "hard_labels = hard_labels.drop_duplicates(\"Variant_Treatment_Pair\")[[\"Variant_Treatment_Pair\", \"Prediction\"]]\n",
    "hard_labels = hard_labels.rename(columns={\"Prediction\": \"Resolved_Prediction\"})\n",
    "\n",
    "soft_labels = dominant_per_pair[dominant_per_pair[\"Soft_Consensus\"]][[\"Variant_Treatment_Pair\", \"Prediction\"]].copy()\n",
    "soft_labels = soft_labels.rename(columns={\"Prediction\": \"Resolved_Prediction\"})\n",
    "\n",
    "df_final_consensus = pd.concat([\n",
    "    hard_labels,\n",
    "    soft_labels[~soft_labels[\"Variant_Treatment_Pair\"].isin(hard_labels[\"Variant_Treatment_Pair\"])],\n",
    "    df_fallback_consensus[~df_fallback_consensus[\"Variant_Treatment_Pair\"].isin(hard_labels[\"Variant_Treatment_Pair\"]) &\n",
    "                          ~df_fallback_consensus[\"Variant_Treatment_Pair\"].isin(soft_labels[\"Variant_Treatment_Pair\"])]\n",
    "], ignore_index=True)\n",
    "\n",
    "df_final_consensus = df_final_consensus[[\"Variant_Treatment_Pair\", \"Resolved_Prediction\"]]\n",
    "df_final_consensus.to_csv(\"final_variant_treatment_consensus.csv\", index=False)\n",
    "\n",
    "print(\"Final consensus dataset shape:\")\n",
    "print(df_final_consensus.shape)\n",
    "print(\"Saved to: final_variant_treatment_consensus.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be61e7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of pairs per prediction category\n",
    "prediction_counts = df_final_consensus[\"Resolved_Prediction\"].value_counts()\n",
    "prediction_percent = (prediction_counts / len(df_final_consensus) * 100).round(2)\n",
    "summary = pd.DataFrame({\n",
    "    \"Count\": prediction_counts,\n",
    "    \"Percentage\": prediction_percent\n",
    "})\n",
    "\n",
    "print(\"Number and percentage of Variant + Treatment pairs per prediction category:\")\n",
    "print(summary)\n",
    "df_no_consensus_final = df_final_consensus[\n",
    "    df_final_consensus[\"Resolved_Prediction\"] == \"No consensus\"\n",
    "]\n",
    "\n",
    "print(\"\\nSample of 'No consensus' entries:\")\n",
    "print(df_no_consensus_final.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
