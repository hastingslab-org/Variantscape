{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fbfa999",
   "metadata": {},
   "source": [
    "# Run general classifier for 'study type' classification of articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c75236",
   "metadata": {},
   "source": [
    "- For further information, please refer to https://pypi.org/project/general-classifier/#4-evaluate-prompt-performance\n",
    "- Load dataset and install necessary libraries\n",
    "- Prepare dataset, i.e., drop irrelevant columns and merge \"PaperTitle\" and \"Abstract\"\n",
    "- Set up general classifier (gc), load categories and define prompt\n",
    "- Define LLM to use\n",
    "- Run gc on dataset \n",
    "- Evaluate performance\n",
    "- Merge with initial dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5626d4a9",
   "metadata": {},
   "source": [
    "## 1) Set up libraries and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2d33868",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: guidance in /data/JH/miniconda3/envs/llms/lib/python3.11/site-packages (0.2.0)\n",
      "Requirement already satisfied: diskcache in /data/JH/miniconda3/envs/llms/lib/python3.11/site-packages (from guidance) (5.6.3)\n",
      "Requirement already satisfied: numpy in /data/JH/miniconda3/envs/llms/lib/python3.11/site-packages (from guidance) (1.26.4)\n",
      "Requirement already satisfied: ordered_set in /data/JH/miniconda3/envs/llms/lib/python3.11/site-packages (from guidance) (4.1.0)\n",
      "Requirement already satisfied: platformdirs in /data/JH/miniconda3/envs/llms/lib/python3.11/site-packages (from guidance) (2.5.2)\n",
      "Requirement already satisfied: pydantic in /data/JH/miniconda3/envs/llms/lib/python3.11/site-packages (from guidance) (2.10.6)\n",
      "Requirement already satisfied: referencing in /data/JH/miniconda3/envs/llms/lib/python3.11/site-packages (from guidance) (0.35.1)\n",
      "Requirement already satisfied: requests in /data/JH/miniconda3/envs/llms/lib/python3.11/site-packages (from guidance) (2.32.0)\n",
      "Requirement already satisfied: psutil in /data/JH/miniconda3/envs/llms/lib/python3.11/site-packages (from guidance) (5.9.0)\n",
      "Requirement already satisfied: tiktoken>=0.3 in /data/JH/miniconda3/envs/llms/lib/python3.11/site-packages (from guidance) (0.5.2)\n",
      "Requirement already satisfied: guidance-stitch in /data/JH/miniconda3/envs/llms/lib/python3.11/site-packages (from guidance) (0.1.0)\n",
      "Requirement already satisfied: llguidance==0.5.1 in /data/JH/miniconda3/envs/llms/lib/python3.11/site-packages (from guidance) (0.5.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/janna/.local/lib/python3.11/site-packages (from tiktoken>=0.3->guidance) (2023.5.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/janna/.local/lib/python3.11/site-packages (from requests->guidance) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/janna/.local/lib/python3.11/site-packages (from requests->guidance) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/janna/.local/lib/python3.11/site-packages (from requests->guidance) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/janna/.local/lib/python3.11/site-packages (from requests->guidance) (2022.12.7)\n",
      "Requirement already satisfied: ipywidgets>=8.0.0 in /data/JH/miniconda3/envs/llms/lib/python3.11/site-packages (from guidance-stitch->guidance) (8.0.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /data/JH/miniconda3/envs/llms/lib/python3.11/site-packages (from pydantic->guidance) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /data/JH/miniconda3/envs/llms/lib/python3.11/site-packages (from pydantic->guidance) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /data/JH/miniconda3/envs/llms/lib/python3.11/site-packages (from pydantic->guidance) (4.12.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /data/JH/miniconda3/envs/llms/lib/python3.11/site-packages (from referencing->guidance) (24.3.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.0 in /data/JH/miniconda3/envs/llms/lib/python3.11/site-packages (from referencing->guidance) (0.22.3)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /data/JH/miniconda3/envs/llms/lib/python3.11/site-packages (from ipywidgets>=8.0.0->guidance-stitch->guidance) (6.19.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /data/JH/miniconda3/envs/llms/lib/python3.11/site-packages (from ipywidgets>=8.0.0->guidance-stitch->guidance) (8.18.1)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /data/JH/miniconda3/envs/llms/lib/python3.11/site-packages (from ipywidgets>=8.0.0->guidance-stitch->guidance) (5.7.1)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.7 in /data/JH/miniconda3/envs/llms/lib/python3.11/site-packages (from ipywidgets>=8.0.0->guidance-stitch->guidance) (4.0.7)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.7 in /data/JH/miniconda3/envs/llms/lib/python3.11/site-packages (from ipywidgets>=8.0.0->guidance-stitch->guidance) (3.0.7)\n",
      "Requirement already satisfied: comm>=0.1.1 in /data/JH/miniconda3/envs/llms/lib/python3.11/site-packages (from ipykernel>=4.5.1->ipywidgets>=8.0.0->guidance-stitch->guidance) (0.1.2)\n",
      "Requirement already satisfied: debugpy>=1.0 in /data/JH/miniconda3/envs/llms/lib/python3.11/site-packages (from ipykernel>=4.5.1->ipywidgets>=8.0.0->guidance-stitch->guidance) (1.5.1)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /data/JH/miniconda3/envs/llms/lib/python3.11/site-packages (from ipykernel>=4.5.1->ipywidgets>=8.0.0->guidance-stitch->guidance) (7.4.9)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /data/JH/miniconda3/envs/llms/lib/python3.11/site-packages (from ipykernel>=4.5.1->ipywidgets>=8.0.0->guidance-stitch->guidance) (0.1.6)\n",
      "Requirement already satisfied: nest-asyncio in /data/JH/miniconda3/envs/llms/lib/python3.11/site-packages (from ipykernel>=4.5.1->ipywidgets>=8.0.0->guidance-stitch->guidance) (1.5.6)\n",
      "Requirement already satisfied: packaging in /home/janna/.local/lib/python3.11/site-packages (from ipykernel>=4.5.1->ipywidgets>=8.0.0->guidance-stitch->guidance) (23.1)\n",
      "Requirement already satisfied: pyzmq>=17 in /data/JH/miniconda3/envs/llms/lib/python3.11/site-packages (from ipykernel>=4.5.1->ipywidgets>=8.0.0->guidance-stitch->guidance) (24.0.1)\n",
      "Requirement already satisfied: tornado>=6.1 in /data/JH/miniconda3/envs/llms/lib/python3.11/site-packages (from ipykernel>=4.5.1->ipywidgets>=8.0.0->guidance-stitch->guidance) (6.2)\n",
      "Requirement already satisfied: decorator in /data/JH/miniconda3/envs/llms/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets>=8.0.0->guidance-stitch->guidance) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /data/JH/miniconda3/envs/llms/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets>=8.0.0->guidance-stitch->guidance) (0.18.1)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /data/JH/miniconda3/envs/llms/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets>=8.0.0->guidance-stitch->guidance) (3.0.41)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /data/JH/miniconda3/envs/llms/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets>=8.0.0->guidance-stitch->guidance) (2.15.1)\n",
      "Requirement already satisfied: stack-data in /data/JH/miniconda3/envs/llms/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets>=8.0.0->guidance-stitch->guidance) (0.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /data/JH/miniconda3/envs/llms/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets>=8.0.0->guidance-stitch->guidance) (4.8.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /data/JH/miniconda3/envs/llms/lib/python3.11/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets>=8.0.0->guidance-stitch->guidance) (0.8.3)\n",
      "Requirement already satisfied: entrypoints in /data/JH/miniconda3/envs/llms/lib/python3.11/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets>=8.0.0->guidance-stitch->guidance) (0.4)\n",
      "Requirement already satisfied: jupyter-core>=4.9.2 in /data/JH/miniconda3/envs/llms/lib/python3.11/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets>=8.0.0->guidance-stitch->guidance) (5.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /data/JH/miniconda3/envs/llms/lib/python3.11/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets>=8.0.0->guidance-stitch->guidance) (2.8.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /data/JH/miniconda3/envs/llms/lib/python3.11/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets>=8.0.0->guidance-stitch->guidance) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /data/JH/miniconda3/envs/llms/lib/python3.11/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets>=8.0.0->guidance-stitch->guidance) (0.2.5)\n",
      "Requirement already satisfied: executing in /data/JH/miniconda3/envs/llms/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets>=8.0.0->guidance-stitch->guidance) (0.8.3)\n",
      "Requirement already satisfied: asttokens in /data/JH/miniconda3/envs/llms/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets>=8.0.0->guidance-stitch->guidance) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in /data/JH/miniconda3/envs/llms/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets>=8.0.0->guidance-stitch->guidance) (0.2.2)\n",
      "Requirement already satisfied: six>=1.5 in /data/JH/miniconda3/envs/llms/lib/python3.11/site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets>=8.0.0->guidance-stitch->guidance) (1.16.0)\n",
      "Requirement already satisfied: pydantic in /data/JH/miniconda3/envs/llms/lib/python3.11/site-packages (2.10.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /data/JH/miniconda3/envs/llms/lib/python3.11/site-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /data/JH/miniconda3/envs/llms/lib/python3.11/site-packages (from pydantic) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /data/JH/miniconda3/envs/llms/lib/python3.11/site-packages (from pydantic) (4.12.2)\n",
      "Requirement already satisfied: openai in /data/JH/miniconda3/envs/llms/lib/python3.11/site-packages (1.57.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /data/JH/miniconda3/envs/llms/lib/python3.11/site-packages (from openai) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /data/JH/miniconda3/envs/llms/lib/python3.11/site-packages (from openai) (1.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /data/JH/miniconda3/envs/llms/lib/python3.11/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /data/JH/miniconda3/envs/llms/lib/python3.11/site-packages (from openai) (0.8.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /data/JH/miniconda3/envs/llms/lib/python3.11/site-packages (from openai) (2.10.6)\n",
      "Requirement already satisfied: sniffio in /data/JH/miniconda3/envs/llms/lib/python3.11/site-packages (from openai) (1.2.0)\n",
      "Requirement already satisfied: tqdm>4 in /data/JH/miniconda3/envs/llms/lib/python3.11/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /data/JH/miniconda3/envs/llms/lib/python3.11/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /home/janna/.local/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: certifi in /home/janna/.local/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2022.12.7)\n",
      "Requirement already satisfied: httpcore==1.* in /data/JH/miniconda3/envs/llms/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /data/JH/miniconda3/envs/llms/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /data/JH/miniconda3/envs/llms/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /data/JH/miniconda3/envs/llms/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
      "Requirement already satisfied: general-classifier in /data/JH/miniconda3/envs/llms/lib/python3.11/site-packages (0.1.1)\n",
      "Success!\n"
     ]
    }
   ],
   "source": [
    "# Install libraries\n",
    "!pip install guidance\n",
    "!pip install pydantic --upgrade\n",
    "!pip install openai\n",
    "!pip install general-classifier\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e70bdf5",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-20 21:12:09.335889: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-20 21:12:09.350675: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742501529.367407   31422 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742501529.372421   31422 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-20 21:12:09.389455: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import general_classifier\n",
    "from general_classifier import gc\n",
    "import csv\n",
    "import transformers\n",
    "import re\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSeq2SeqLM, TextStreamer\n",
    "import torch\n",
    "import requests\n",
    "import sentencepiece as spm\n",
    "import time\n",
    "import json\n",
    "import torch\n",
    "import ast\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ef517d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the working directory and file paths\n",
    "input_directory = \"INPUT_DIRECTORY\"\n",
    "output_directory = \"OUTPUT_DIRECTORY\"\n",
    "classifier_directory = \"CLASSIFIER_DIRECTORY\"\n",
    "\n",
    "# Change the working directory\n",
    "os.chdir(classifier_directory)\n",
    "print(\"Current Working Directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2282779f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: /data/JH/marie/TrendyVariants/Input\n",
      "--> Topics and categories loaded!\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "\n",
    "# Load topics and categories\n",
    "os.chdir(input_directory)\n",
    "print(\"Current Working Directory:\", os.getcwd())\n",
    "topics_and_categories = pd.read_csv(\"studydesign.csv\")\n",
    "categories = topics_and_categories.iloc[:, 0].dropna().tolist()\n",
    "print(\"--> Topics and categories loaded!\")\n",
    "\n",
    "# Load dataset to run classifier\n",
    "os.chdir(output_directory)\n",
    "print(\"\\nCurrent Working Directory:\", os.getcwd())\n",
    "\n",
    "cancer_df = pd.read_csv(\"binary_cancer_matrix_filtered.csv\")\n",
    "len_cancer_df=len(cancer_df)\n",
    "print(f\" --> Total rows in cancer dataset: {len_cancer_df:,}\")\n",
    "\n",
    "cancer_df = cancer_df[['PaperId', 'PaperTitle', 'Abstract']].copy()\n",
    "#print(cancer_df)\n",
    "\n",
    "print(\"\\n\\nSuccess!\")\n",
    "os.chdir(classifier_directory)\n",
    "print(\"Current Working Directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0848dfc8",
   "metadata": {},
   "source": [
    "## 2) Set up general calssifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ab0930d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "# Setup gc\n",
    "gc.setModel(newModel=\"meta-llama/Llama-3.3-70B-Instruct\", newModelType=\"DeepInfra\", newInferenceType=\"cloud\", api_key=\"API_KEY\")\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1c5772f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All topics have been removed, counters reset, and related data cleared.\n",
      "Topic 1 (ID=A): Study Type\n",
      "  Prompt: INSTRUCTION: You are a helpful classifier. You select the correct of the possible categories for classifying a piece of text. The topic of the classification is '[TOPIC]'. The allowed categories are '[CATEGORIES]'. QUESTION: The text to be classified is '[TEXT]'. ANSWER: The correct category for this text is '\n",
      "    1. Clinical study (ID=a)\n",
      "    2. Case report study (ID=b)\n",
      "    3. In vitro study (ID=c)\n",
      "    4. In silico study (ID=d)\n",
      "    5. In vivo/Animal study (ID=e)\n",
      "    6. Behavioral study (ID=f)\n",
      "    7. Observational/RWE study (ID=g)\n",
      "    8. Systematic review study (ID=h)\n"
     ]
    }
   ],
   "source": [
    "gc.removeAllTopics()\n",
    "gc.add_topic(\n",
    "    topic_name=\"Study Type\",\n",
    "    categories=categories  # Use extracted categories\n",
    ")\n",
    "# Display all defined topics and their categories\n",
    "gc.show_topics_and_categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4d39f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt for topic ID A updated.\n"
     ]
    }
   ],
   "source": [
    "gc.setPrompt(\n",
    "    topicId=\"A\", \n",
    "    newPrompt=(\n",
    "        \"Prompt: INSTRUCTION: You are a helpful classifier. You are given the abstract of a \"\n",
    "        \"scientific, biomedical publication and you have to select the correct of the possible categories. \"\n",
    "        \"The topic of the classification is '[TOPIC]'. The allowed categories are '[CATEGORIES]'. \"\n",
    "        \"QUESTION: The abstract to be classified is '[TEXT]'. \"\n",
    "        'ANSWER: The correct category for this abstract is \"\".'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a0b0e9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1 (ID=A): Study Type\n",
      "  Prompt: Prompt: INSTRUCTION: You are a helpful classifier. You are given the abstract of a scientific, biomedical publication and you have to select the correct of the possible categories. The topic of the classification is '[TOPIC]'. The allowed categories are '[CATEGORIES]'. QUESTION: The abstract to be classified is '[TEXT]'. ANSWER: The correct category for this abstract is \"\".\n",
      "    1. Clinical study (ID=a)\n",
      "    2. Case report study (ID=b)\n",
      "    3. In vitro study (ID=c)\n",
      "    4. In silico study (ID=d)\n",
      "    5. In vivo/Animal study (ID=e)\n",
      "    6. Behavioral study (ID=f)\n",
      "    7. Observational/RWE study (ID=g)\n",
      "    8. Systematic review study (ID=h)\n"
     ]
    }
   ],
   "source": [
    "# Show prompt and topic with associated categories\n",
    "gc.show_topics_and_categories()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f79058e",
   "metadata": {},
   "source": [
    "## 3) Run general classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc35690",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Create a article subset, remove everything except the column to classify and the PaperID\n",
    "\n",
    "# Create the new column by combining \"PaperTitle\" and \"Abstract\"\n",
    "subset_df[\"PaperTitle_and_Abstracts\"] = subset_df[\"PaperTitle\"].astype(str) + \" \" + subset_df[\"Abstract\"].astype(str)\n",
    "\n",
    "# Keep only the required columns\n",
    "subset_df_gccolumn = subset_df[[\"PaperId\", \"PaperTitle_and_Abstracts\"]].copy()\n",
    "\n",
    "# Save the new dataset as CSV\n",
    "csv_path = \"subset_df_gccolumn.csv\"\n",
    "subset_df_gccolumn.to_csv(csv_path, index=False)\n",
    "\n",
    "# Print confirmation and show first rows\n",
    "print(f\"File saved: {csv_path}\")\n",
    "print(subset_df_gccolumn.head(20))\n",
    "print(\"Length of dataset:\",len(subset_df_gccolumn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df70ea79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run classification on the subset CSV file\n",
    "os.chdir(output_directory)\n",
    "\n",
    "# Get start timestamp\n",
    "start_time = time.time()\n",
    "\n",
    "gc.classify_table(dataset=\"subset_df_gccolumn\", withEvaluation=False, constrainedOutput=True)\n",
    "# withEvaluation=False if not done manually\n",
    "print(\"Success!\")\n",
    "\n",
    "# Get end timestamp\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate runtime duration\n",
    "runtime_duration = end_time - start_time\n",
    "\n",
    "# Generate output file name\n",
    "output_filename = f\"Study_design_classification_runtime_subset_data.txt\"\n",
    "\n",
    "os.chdir(output_directory)\n",
    "# Save timestamp information to file\n",
    "with open(output_filename, \"w\") as f:\n",
    "    f.write(f\"Start Time: {time.ctime(start_time)}\\n\")\n",
    "    f.write(f\"End Time: {time.ctime(end_time)}\\n\")\n",
    "    f.write(f\"Total Runtime (seconds): {runtime_duration:.4f}\\n\")\n",
    "\n",
    "print(f\"Runtime saved in {output_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c292543",
   "metadata": {},
   "source": [
    "## 4) Batch running of general calssifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5b752a",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df=cancer_df.copy()\n",
    "print(f\"Length of cancer dataset to process in batches: {len(subset_df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02e170c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a new column by combining \"PaperTitle\" and \"Abstract\"\n",
    "\n",
    "subset_df[\"PaperTitle_and_Abstracts\"] = subset_df[\"PaperTitle\"].astype(str) + \" \" + subset_df[\"Abstract\"].astype(str)\n",
    "subset_df_gccolumn = subset_df[[\"PaperId\", \"PaperTitle_and_Abstracts\"]].copy()\n",
    "\n",
    "# Change the working directory\n",
    "classifier_directory = \"/data/JH/marie/TrendyVariants/Output/gc_batch_files\"\n",
    "os.chdir(classifier_directory)\n",
    "print(\"Current Working Directory:\", os.getcwd())\n",
    "\n",
    "# Save the full newly created dataset as CSV\n",
    "csv_path = \"subset_df_gccolumn.csv\"\n",
    "subset_df_gccolumn.to_csv(csv_path, index=False)\n",
    "\n",
    "# Print confirmation and show first rows\n",
    "print(f\"File saved: {classifier_directory}\")\n",
    "print(subset_df_gccolumn.head(20))\n",
    "print(\"\\n\\nLength of dataset:\", len(subset_df_gccolumn))\n",
    "\n",
    "# ---- Dynamically Create Batches ----\n",
    "# Define batch size\n",
    "batch_size = 50000\n",
    "total_batches = (len(subset_df_gccolumn) + batch_size - 1) // batch_size\n",
    "print(f\"\\n\\nTotal number of batches: {total_batches}\")\n",
    "\n",
    "\n",
    "# Split dataset into batches and save them\n",
    "for batch_num in range(total_batches):\n",
    "    start_idx = batch_num * batch_size\n",
    "    end_idx = min(start_idx + batch_size, len(subset_df_gccolumn))\n",
    "\n",
    "    # Extract batch\n",
    "    batch_df = subset_df_gccolumn.iloc[start_idx:end_idx]\n",
    "\n",
    "    # Save batch as a separate CSV\n",
    "    batch_filename = os.path.join(classifier_directory, f\"subset_df_gccolumn_batch_{batch_num+1}.csv\")\n",
    "    batch_df.to_csv(batch_filename, index=False)\n",
    "\n",
    "    # Print confirmation\n",
    "    print(f\"Batch {batch_num+1}/{total_batches} saved: {batch_filename} | Articles in batch: {len(batch_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8e22c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch to be processed next: 5\n",
      "Batches already processed: 4 / 5 batches\n",
      "Batches to process: 0\n",
      "Batch size: 50,000 articles\n",
      "Total batches of dataset: 5\n"
     ]
    }
   ],
   "source": [
    "# Batch processing\n",
    "\n",
    "# Manually define the current batch to process!\n",
    "n = 5  # Define the current batch number manually!!!!\n",
    "# if new running\n",
    "lastprocessedbatch=4\n",
    "total_batches=5\n",
    "batch_size = 50000\n",
    "\n",
    "# Check if n and last processed batch are the same\n",
    "if n == lastprocessedbatch:\n",
    "    print(\"\\n !!! UPDATE BATCH NUMBER WARNING !!!\")\n",
    "    print(f\"The current batch number ({n}) is the same as the last processed batch ({lastprocessedbatch}).\")\n",
    "    print(\"Please ensure you are not reprocessing the same batch unless intended.\\n\")\n",
    "\n",
    "print(f\"Batch to be processed next: {n}\")\n",
    "print(f\"Batches already processed: {lastprocessedbatch:,} / {total_batches:,} batches\")\n",
    "print(f\"Batches to process: {total_batches - n}\")\n",
    "print(f\"Batch size: {batch_size:,} articles\")\n",
    "print(f\"Total batches of dataset: {total_batches:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8492647c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Batch 5/5...\n",
      "Processed 100/39078 rows (0.26%)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Processing Batch {n}/{total_batches}...\")\n",
    "\n",
    "# Run classification on batch data\n",
    "os.chdir(classifier_directory)\n",
    "start_time = time.time()\n",
    "\n",
    "gc.classify_table(dataset=f\"subset_df_gccolumn_batch_{n}\", withEvaluation=False, constrainedOutput=True) #Need to define the CSV!\n",
    "# withEvaluation=False if not done manually\n",
    "print(\"\\n\\nSuccess!\")\n",
    "print(\"File saved as subset_df_gccolumn_batch_n_(restuls)!\")\n",
    "\n",
    "# Get end timestamp\n",
    "end_time = time.time()\n",
    "runtime_duration = end_time - start_time\n",
    "\n",
    "\n",
    "##############\n",
    "\n",
    "# Generate output file name\n",
    "output_filename = f\"Study_design_classification_runtime_subset_data_batch_{n}.txt\"\n",
    "lastprocessedbatch=n\n",
    "\n",
    "# Convert runtime to minutes and hours\n",
    "runtime_minutes = runtime_duration / 60\n",
    "runtime_hours = runtime_duration / 3600\n",
    "\n",
    "\n",
    "# Print runtime information to console\n",
    "print(f\"\\n ######### Batch {n} Runtime Summary: ########\")\n",
    "print(f\"- Start Time: {time.ctime(start_time)}\")\n",
    "print(f\"- End Time: {time.ctime(end_time)}\")\n",
    "print(f\"- Total Runtime: {runtime_duration:.4f} seconds | {runtime_minutes:.2f} minutes | {runtime_hours:.2f} hours\")\n",
    "\n",
    "# Save timestamp information to file\n",
    "with open(output_filename, \"w\") as f:\n",
    "    f.write(f\"Start Time: {time.ctime(start_time)}\\n\")\n",
    "    f.write(f\"End Time: {time.ctime(end_time)}\\n\")\n",
    "    f.write(f\"Total Runtime (seconds): {runtime_duration:.4f}\\n\")\n",
    "    f.write(f\"Total Runtime (minutes): {runtime_minutes:.2f}\\n\")\n",
    "    f.write(f\"Total Runtime (hours): {runtime_hours:.2f}\\n\")\n",
    "\n",
    "print(f\"\\nRuntime saved in {output_filename}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c900cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create total runtime calculation\n",
    "\n",
    "# Initialize total runtime accumulator\n",
    "total_cumulative_seconds = 0\n",
    "\n",
    "# Loop through all batch files up to total_batches\n",
    "for n in range(1, total_batches + 1):\n",
    "    filename = f\"Study_design_classification_runtime_subset_data_batch_{n}.txt\"\n",
    "    file_path = os.path.join(classifier_directory, filename)\n",
    "\n",
    "    # Check if the file exists, otherwise skip\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Skipping missing file: {filename}\")\n",
    "        continue\n",
    "\n",
    "    # Read the file and extract runtime in seconds\n",
    "    with open(file_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            if \"Total Runtime (seconds):\" in line:\n",
    "                runtime_seconds = float(line.strip().split(\":\")[1])\n",
    "                total_cumulative_seconds += runtime_seconds\n",
    "                break  # No need to read further\n",
    "\n",
    "# Convert total runtime to minutes and hours\n",
    "total_cumulative_minutes = total_cumulative_seconds / 60\n",
    "total_cumulative_hours = total_cumulative_seconds / 3600\n",
    "\n",
    "# Print results\n",
    "print(f\"\\nTotal cumulative runtime of general calssifier for study categorization:\")\n",
    "print(f\"Seconds: {total_cumulative_seconds:.4f}\")\n",
    "print(f\"Minutes: {total_cumulative_minutes:.2f}\")\n",
    "print(f\"Hours: {total_cumulative_hours:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3061b1",
   "metadata": {},
   "source": [
    "# =============================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0aa3d7d",
   "metadata": {},
   "source": [
    "## 4) Merge batch dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36360ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of total batches:\", total_batches)\n",
    "os.chdir(classifier_directory)\n",
    "\n",
    "for n in range(1, total_batches + 1):\n",
    "    input_filename = f\"subset_df_gccolumn_batch_{n}_(result).csv\"\n",
    "    output_filename = f\"fixed_output_batch_{n}.csv\"\n",
    "\n",
    "    # Check if the file exists, if not, skip it\n",
    "    if not os.path.exists(input_filename):\n",
    "        print(f\"Batch {n}: File not found. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Read the file while forcing a semicolon separator\n",
    "        df = pd.read_csv(input_filename, sep=\";\", engine=\"python\")\n",
    "\n",
    "        # Save it back properly as a CSV with commas as delimiters\n",
    "        df.to_csv(output_filename, index=False, sep=\",\", quoting=1)\n",
    "\n",
    "        # Read again, fix delimiters, rename columns, and save in one go\n",
    "        df.rename(columns={df.columns[0]: \"PaperId,PaperTitle_and_Abstracts\", \n",
    "                           df.columns[1]: \"Study_design\"}, inplace=True)\n",
    "        df.to_csv(output_filename, index=False, sep=\",\", quoting=1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Batch {n}: Error processing file - {e}\")\n",
    "\n",
    "print(\"\\nProcessing complete for all available batches!\")\n",
    "\n",
    "# Print first 5 rows of Batch 1 for verification\n",
    "file_1 = \"fixed_output_batch_1.csv\"\n",
    "if os.path.exists(file_1):\n",
    "    print(pd.read_csv(file_1).head(5))\n",
    "else:\n",
    "    print(\"\\nBatch 1 is not available yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a20c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the batches\n",
    "\n",
    "# List to store DataFrames\n",
    "merged_df_list = []\n",
    "\n",
    "for n in range(1, total_batches + 1):\n",
    "    file_name = f\"fixed_output_batch_{n}.csv\"\n",
    "    \n",
    "    if os.path.exists(file_name):  # Check if file exists before merging\n",
    "        df = pd.read_csv(file_name)\n",
    "        merged_df_list.append(df)\n",
    "        print(f\"Batch {n} added to merge with {len(df)} rows.\")\n",
    "    else:\n",
    "        print(f\"Batch {n} is missing. Skipping...\")\n",
    "\n",
    "# Merge all available DataFrames\n",
    "final_merged_gc_df = pd.concat(merged_df_list, ignore_index=True)\n",
    "final_merged_gc_df.to_csv(\"final_merged_gc_output.csv\", index=False, sep=\",\", quoting=1)\n",
    "print(f\"\\nFinal merged dataset saved as 'final_merged_gc_output.csv' with {len(final_merged_df)} total rows.\")\n",
    "print(final_merged_gc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed7d927",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Split classifier output columns\n",
    "os.chdir(classifier_directory)\n",
    "merged_df = pd.read_csv(\"final_merged_gc_output.csv\")\n",
    "print(\"**Merged batches of GC dataset:**\")\n",
    "print(merged_df)\n",
    "\n",
    "# Split PaperId from PaperTitle_and_Abstracts\n",
    "merged_df[['PaperId', 'PaperTitle_and_Abstracts']] = merged_df['PaperId,PaperTitle_and_Abstracts'].str.split(',', n=1, expand=True)\n",
    "merged_df['PaperId'] = merged_df['PaperId'].str.strip('\"').astype(int)\n",
    "\n",
    "\n",
    "# Drop unnecessary columns\n",
    "merged_df.drop(columns=['PaperId,PaperTitle_and_Abstracts', 'PaperTitle_and_Abstracts'], inplace=True)\n",
    "print(\"\\n\\n**Merged batches of GC dataset after splitting and dropping PaperTitle and Abstract columns:''\")\n",
    "print(merged_df)\n",
    "# Convert PaperId in article_df to integer (to ensure proper merging)\n",
    "article_df['PaperId'] = article_df['PaperId'].astype(int)\n",
    "\n",
    "print(\"\\n\\n-->Success!\")\n",
    "\n",
    "# Save the final merged gc batch file\n",
    "\n",
    "gclength = len(merged_df)\n",
    "output_filename = f\"corrected_classification_gc_output_{gclength}.csv\"\n",
    "merged_df.to_csv(output_filename, index=False)\n",
    "\n",
    "# Print the dataset and its length\n",
    "print(merged_df)\n",
    "print(f\"\\nLength of merged gc dataset: {gclength}\")\n",
    "print(f\"Final file saved as: {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e18bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load  full dataset for merging\n",
    "os.chdir(output_directory)\n",
    "print(\"\\nCurrent Working Directory:\", os.getcwd())\n",
    "cancer_df = pd.read_csv(\"binary_cancer_matrix_filtered.csv\")\n",
    "len_cancer_df=len(cancer_df)\n",
    "print(f\" --> Total rows in cancer dataset: {len_cancer_df:,}\")\n",
    "cancer_df = cancer_df[['PaperId', 'PaperTitle', 'Abstract']].copy()\n",
    "print(f\" --> Reconfirmed: Total rows in cancer dataset: {len_cancer_df:,}\")\n",
    "print(\"--> Full cancer dataset loaded!\")\n",
    "os.chdir(classifier_directory)\n",
    "print(\"\\nCurrent Working Directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24575228",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Perform the final merge\n",
    "os.chdir(classifier_directory)\n",
    "print(\"\\nCurrent Working Directory:\", os.getcwd())\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize progress bar\n",
    "total_rows = len(merged_df)\n",
    "with tqdm(total=total_rows, desc=\"Merging Data\", unit=\"rows\") as pbar:\n",
    "    full_merged_df = merged_df.merge(cancer_df, on=\"PaperId\", how=\"left\", indicator=True)\n",
    "    pbar.update(total_rows)  # Update progress bar after merging\n",
    "\n",
    "# End timing and calculate estimated duration\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Count matches\n",
    "successful_matches = full_merged_df['_merge'].value_counts().get('both', 0)\n",
    "failed_matches = full_merged_df['_merge'].value_counts().get('left_only', 0)\n",
    "\n",
    "# Drop the merge indicator column\n",
    "full_merged_df.drop(columns=['_merge'], inplace=True)\n",
    "\n",
    "# Save the final merged file\n",
    "full_merged_df.to_csv(\"full_merged_output_after_classification.csv\", index=False)\n",
    "\n",
    "# Save the final merged file\n",
    "full_length = len(full_merged_df)\n",
    "output_filename = f\"final_gc_classificaton_output_{full_length}.csv\"\n",
    "full_merged_df.to_csv(output_filename, index=False)\n",
    "\n",
    "# Print match statistics and estimated execution time\n",
    "print(f\"Total Entries in merged_df: {len(merged_df)}\")\n",
    "print(f\"Successful Matches: {successful_matches}\")\n",
    "print(f\"Unmatched Entries: {failed_matches}\")\n",
    "print(f\"Execution Time: {execution_time:.2f} seconds\")\n",
    "print(f\"Final file saved as: {output_filename}\")\n",
    "print(f\"Total rows in final dataset: {full_length}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
