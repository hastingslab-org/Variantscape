{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fbfa999",
   "metadata": {},
   "source": [
    "# Run general classifier for 'study type' classification of articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c75236",
   "metadata": {},
   "source": [
    "- For further information, please refer to https://pypi.org/project/general-classifier/#4-evaluate-prompt-performance\n",
    "- Load dataset and install necessary libraries\n",
    "- Prepare dataset, i.e., drop irrelevant columns and merge \"PaperTitle\" and \"Abstract\"\n",
    "- Set up general classifier (gc), load categories and define prompt\n",
    "- Define LLM to use\n",
    "- Run gc on dataset \n",
    "- Evaluate performance\n",
    "- Merge with initial dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5626d4a9",
   "metadata": {},
   "source": [
    "## 1) Set up libraries and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d33868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries\n",
    "!pip install guidance\n",
    "!pip install pydantic --upgrade\n",
    "!pip install openai\n",
    "!pip install general-classifier\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e70bdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import general_classifier\n",
    "from general_classifier import gc\n",
    "import csv\n",
    "import transformers\n",
    "import re\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSeq2SeqLM, TextStreamer\n",
    "import torch\n",
    "import requests\n",
    "import sentencepiece as spm\n",
    "import time\n",
    "import json\n",
    "import torch\n",
    "import ast\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cf07e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the working directory and file paths\n",
    "input_directory = \"INPUT_DIRECTORY\"\n",
    "output_directory = \"OUTPUT_DIRECTORY\"\n",
    "classifier_directory = \"CLASSIFIER_DIRECTORY\"\n",
    "\n",
    "# Change the working directory\n",
    "os.chdir(classifier_directory)\n",
    "print(\"Current Working Directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2282779f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load topics and categories\n",
    "os.chdir(input_directory)\n",
    "print(\"Current Working Directory:\", os.getcwd())\n",
    "topics_and_categories = pd.read_csv(\"studydesign.csv\")\n",
    "categories = topics_and_categories.iloc[:, 0].dropna().tolist()\n",
    "print(\"--> Topics and categories loaded!\")\n",
    "\n",
    "# Load dataset to run classifier\n",
    "os.chdir(output_directory)\n",
    "print(\"\\nCurrent Working Directory:\", os.getcwd())\n",
    "\n",
    "cancer_df = pd.read_csv(\"binary_cancer_matrix_filtered.csv\")\n",
    "len_cancer_df=len(cancer_df)\n",
    "print(f\" --> Total rows in cancer dataset: {len_cancer_df:,}\")\n",
    "\n",
    "cancer_df = cancer_df[['PaperId', 'PaperTitle', 'Abstract']].copy()\n",
    "print(\"\\n\\nSuccess!\")\n",
    "\n",
    "os.chdir(classifier_directory)\n",
    "print(\"Current Working Directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0848dfc8",
   "metadata": {},
   "source": [
    "## 2) Set up general calssifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab0930d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup gc\n",
    "gc.setModel(newModel=\"meta-llama/Llama-3.3-70B-Instruct\", newModelType=\"DeepInfra\", newInferenceType=\"cloud\", api_key=\"API_KEY\")\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c5772f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gc.removeAllTopics()\n",
    "gc.add_topic(\n",
    "    topic_name=\"Study Type\",\n",
    "    categories=categories \n",
    ")\n",
    "# Display all defined topics and their categories\n",
    "gc.show_topics_and_categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d39f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.setPrompt(\n",
    "    topicId=\"A\", \n",
    "    newPrompt=(\n",
    "        \"Prompt: INSTRUCTION: You are a helpful classifier. You are given the abstract of a \"\n",
    "        \"scientific, biomedical publication and you have to select the correct of the possible categories. \"\n",
    "        \"The topic of the classification is '[TOPIC]'. The allowed categories are '[CATEGORIES]'. \"\n",
    "        \"QUESTION: The abstract to be classified is '[TEXT]'. \"\n",
    "        'ANSWER: The correct category for this abstract is \"\".'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0b0e9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show prompt and topic with associated categories\n",
    "gc.show_topics_and_categories()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f79058e",
   "metadata": {},
   "source": [
    "## 3) Run general classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc35690",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Create a article subset, remove everything except the column to classify and the PaperID\n",
    "\n",
    "# Create the new column by combining \"PaperTitle\" and \"Abstract\"\n",
    "subset_df[\"PaperTitle_and_Abstracts\"] = subset_df[\"PaperTitle\"].astype(str) + \" \" + subset_df[\"Abstract\"].astype(str)\n",
    "\n",
    "# Keep only the required columns\n",
    "subset_df_gccolumn = subset_df[[\"PaperId\", \"PaperTitle_and_Abstracts\"]].copy()\n",
    "\n",
    "# Generate output\n",
    "csv_path = \"subset_df_gccolumn.csv\"\n",
    "subset_df_gccolumn.to_csv(csv_path, index=False)\n",
    "print(f\"File saved: {csv_path}\")\n",
    "print(subset_df_gccolumn.head(20))\n",
    "print(\"Length of dataset:\",len(subset_df_gccolumn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df70ea79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run classification on the subset CSV file\n",
    "os.chdir(output_directory)\n",
    "start_time = time.time()\n",
    "\n",
    "gc.classify_table(dataset=\"subset_df_gccolumn\", withEvaluation=False, constrainedOutput=True)\n",
    "# withEvaluation=False if not done manually\n",
    "print(\"Success!\")\n",
    "\n",
    "# Get end timestamp\n",
    "end_time = time.time()\n",
    "runtime_duration = end_time - start_time\n",
    "output_filename = f\"Study_design_classification_runtime_subset_data.txt\"\n",
    "\n",
    "os.chdir(output_directory)\n",
    "with open(output_filename, \"w\") as f:\n",
    "    f.write(f\"Start Time: {time.ctime(start_time)}\\n\")\n",
    "    f.write(f\"End Time: {time.ctime(end_time)}\\n\")\n",
    "    f.write(f\"Total Runtime (seconds): {runtime_duration:.4f}\\n\")\n",
    "print(f\"Runtime saved in {output_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c292543",
   "metadata": {},
   "source": [
    "## 4) Batch running of general calssifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5b752a",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df=cancer_df.copy()\n",
    "print(f\"Length of cancer dataset to process in batches: {len(subset_df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02e170c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a new column by combining \"PaperTitle\" and \"Abstract\"\n",
    "\n",
    "subset_df[\"PaperTitle_and_Abstracts\"] = subset_df[\"PaperTitle\"].astype(str) + \" \" + subset_df[\"Abstract\"].astype(str)\n",
    "subset_df_gccolumn = subset_df[[\"PaperId\", \"PaperTitle_and_Abstracts\"]].copy()\n",
    "\n",
    "# Change the working directory\n",
    "classifier_directory = \"/data/JH/marie/TrendyVariants/Output/gc_batch_files\"\n",
    "os.chdir(classifier_directory)\n",
    "print(\"Current Working Directory:\", os.getcwd())\n",
    "\n",
    "# Save the full newly created dataset as CSV\n",
    "csv_path = \"subset_df_gccolumn.csv\"\n",
    "subset_df_gccolumn.to_csv(csv_path, index=False)\n",
    "\n",
    "# Print confirmation and show first rows\n",
    "print(f\"File saved: {classifier_directory}\")\n",
    "print(subset_df_gccolumn.head(20))\n",
    "print(\"\\n\\nLength of dataset:\", len(subset_df_gccolumn))\n",
    "\n",
    "# ---- Dynamically Create Batches ----\n",
    "# Define batch size\n",
    "batch_size = 50000\n",
    "total_batches = (len(subset_df_gccolumn) + batch_size - 1) // batch_size\n",
    "print(f\"\\n\\nTotal number of batches: {total_batches}\")\n",
    "\n",
    "\n",
    "# Split dataset into batches and save them\n",
    "for batch_num in range(total_batches):\n",
    "    start_idx = batch_num * batch_size\n",
    "    end_idx = min(start_idx + batch_size, len(subset_df_gccolumn))\n",
    "    batch_df = subset_df_gccolumn.iloc[start_idx:end_idx]\n",
    "    batch_filename = os.path.join(classifier_directory, f\"subset_df_gccolumn_batch_{batch_num+1}.csv\")\n",
    "    batch_df.to_csv(batch_filename, index=False)\n",
    "    print(f\"Batch {batch_num+1}/{total_batches} saved: {batch_filename} | Articles in batch: {len(batch_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e22c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch processing\n",
    "\n",
    "# Manually define the current batch to process!\n",
    "n = 5  # Define the current batch number manually!!!!\n",
    "# if new running\n",
    "lastprocessedbatch=4\n",
    "total_batches=5\n",
    "batch_size = 50000\n",
    "\n",
    "# Check if n and last processed batch are the same\n",
    "if n == lastprocessedbatch:\n",
    "    print(\"\\n !!! UPDATE BATCH NUMBER WARNING !!!\")\n",
    "    print(f\"The current batch number ({n}) is the same as the last processed batch ({lastprocessedbatch}).\")\n",
    "    print(\"Please ensure you are not reprocessing the same batch unless intended.\\n\")\n",
    "\n",
    "print(f\"Batch to be processed next: {n}\")\n",
    "print(f\"Batches already processed: {lastprocessedbatch:,} / {total_batches:,} batches\")\n",
    "print(f\"Batches to process: {total_batches - n}\")\n",
    "print(f\"Batch size: {batch_size:,} articles\")\n",
    "print(f\"Total batches of dataset: {total_batches:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8492647c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"Processing Batch {n}/{total_batches}...\")\n",
    "\n",
    "# Run classification on batch data\n",
    "os.chdir(classifier_directory)\n",
    "start_time = time.time()\n",
    "\n",
    "gc.classify_table(dataset=f\"subset_df_gccolumn_batch_{n}\", withEvaluation=False, constrainedOutput=True) #Need to define the CSV!\n",
    "# withEvaluation=False if not done manually\n",
    "print(\"\\n\\nSuccess!\")\n",
    "print(\"File saved as subset_df_gccolumn_batch_n_(restuls)!\")\n",
    "\n",
    "# Get end timestamp\n",
    "end_time = time.time()\n",
    "runtime_duration = end_time - start_time\n",
    "\n",
    "##############\n",
    "\n",
    "# Generate output\n",
    "output_filename = f\"Study_design_classification_runtime_subset_data_batch_{n}.txt\"\n",
    "lastprocessedbatch=n\n",
    "\n",
    "# Convert runtime to minutes and hours\n",
    "runtime_minutes = runtime_duration / 60\n",
    "runtime_hours = runtime_duration / 3600\n",
    "\n",
    "print(f\"\\n ######### Batch {n} Runtime Summary: ########\")\n",
    "print(f\"- Start Time: {time.ctime(start_time)}\")\n",
    "print(f\"- End Time: {time.ctime(end_time)}\")\n",
    "print(f\"- Total Runtime: {runtime_duration:.4f} seconds | {runtime_minutes:.2f} minutes | {runtime_hours:.2f} hours\")\n",
    "\n",
    "with open(output_filename, \"w\") as f:\n",
    "    f.write(f\"Start Time: {time.ctime(start_time)}\\n\")\n",
    "    f.write(f\"End Time: {time.ctime(end_time)}\\n\")\n",
    "    f.write(f\"Total Runtime (seconds): {runtime_duration:.4f}\\n\")\n",
    "    f.write(f\"Total Runtime (minutes): {runtime_minutes:.2f}\\n\")\n",
    "    f.write(f\"Total Runtime (hours): {runtime_hours:.2f}\\n\")\n",
    "\n",
    "print(f\"\\nRuntime saved in {output_filename}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c900cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create total runtime calculation\n",
    "\n",
    "# Initialize total runtime accumulator\n",
    "total_cumulative_seconds = 0\n",
    "\n",
    "# Loop through all batch files up to total_batches\n",
    "for n in range(1, total_batches + 1):\n",
    "    filename = f\"Study_design_classification_runtime_subset_data_batch_{n}.txt\"\n",
    "    file_path = os.path.join(classifier_directory, filename)\n",
    "\n",
    "    # Check if the file exists, otherwise skip\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Skipping missing file: {filename}\")\n",
    "        continue\n",
    "\n",
    "    # Read the file and extract runtime in seconds\n",
    "    with open(file_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            if \"Total Runtime (seconds):\" in line:\n",
    "                runtime_seconds = float(line.strip().split(\":\")[1])\n",
    "                total_cumulative_seconds += runtime_seconds\n",
    "                break\n",
    "\n",
    "# Convert total runtime to minutes and hours\n",
    "total_cumulative_minutes = total_cumulative_seconds / 60\n",
    "total_cumulative_hours = total_cumulative_seconds / 3600\n",
    "\n",
    "print(f\"\\nTotal cumulative runtime of general calssifier for study categorization:\")\n",
    "print(f\"Seconds: {total_cumulative_seconds:.4f}\")\n",
    "print(f\"Minutes: {total_cumulative_minutes:.2f}\")\n",
    "print(f\"Hours: {total_cumulative_hours:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3061b1",
   "metadata": {},
   "source": [
    "# =============================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0aa3d7d",
   "metadata": {},
   "source": [
    "## 4) Merge batch dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36360ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of total batches:\", total_batches)\n",
    "os.chdir(classifier_directory)\n",
    "\n",
    "for n in range(1, total_batches + 1):\n",
    "    input_filename = f\"subset_df_gccolumn_batch_{n}_(result).csv\"\n",
    "    output_filename = f\"fixed_output_batch_{n}.csv\"\n",
    "\n",
    "    # Check if the file exists, if not, skip it\n",
    "    if not os.path.exists(input_filename):\n",
    "        print(f\"Batch {n}: File not found. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Read the file while forcing a semicolon separator\n",
    "        df = pd.read_csv(input_filename, sep=\";\", engine=\"python\")\n",
    "\n",
    "        # Save it back properly as a CSV with commas as delimiters\n",
    "        df.to_csv(output_filename, index=False, sep=\",\", quoting=1)\n",
    "\n",
    "        # Read again, fix delimiters, rename columns, and save in one go\n",
    "        df.rename(columns={df.columns[0]: \"PaperId,PaperTitle_and_Abstracts\", \n",
    "                           df.columns[1]: \"Study_design\"}, inplace=True)\n",
    "        df.to_csv(output_filename, index=False, sep=\",\", quoting=1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Batch {n}: Error processing file - {e}\")\n",
    "\n",
    "print(\"\\nProcessing complete for all available batches!\")\n",
    "\n",
    "file_1 = \"fixed_output_batch_1.csv\"\n",
    "if os.path.exists(file_1):\n",
    "    print(pd.read_csv(file_1).head(5))\n",
    "else:\n",
    "    print(\"\\nBatch 1 is not available yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a20c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the batches\n",
    "merged_df_list = []\n",
    "\n",
    "for n in range(1, total_batches + 1):\n",
    "    file_name = f\"fixed_output_batch_{n}.csv\"\n",
    "    \n",
    "    if os.path.exists(file_name):\n",
    "        df = pd.read_csv(file_name)\n",
    "        merged_df_list.append(df)\n",
    "        print(f\"Batch {n} added to merge with {len(df)} rows.\")\n",
    "    else:\n",
    "        print(f\"Batch {n} is missing. Skipping...\")\n",
    "\n",
    "# Merge all available DataFrames\n",
    "final_merged_gc_df = pd.concat(merged_df_list, ignore_index=True)\n",
    "final_merged_gc_df.to_csv(\"final_merged_gc_output.csv\", index=False, sep=\",\", quoting=1)\n",
    "print(f\"\\nFinal merged dataset saved as 'final_merged_gc_output.csv' with {len(final_merged_df)} total rows.\")\n",
    "print(final_merged_gc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed7d927",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split classifier output columns and prepare GC dataset\n",
    "os.chdir(classifier_directory)\n",
    "merged_df = pd.read_csv(\"final_merged_gc_output.csv\")\n",
    "print(\"**Merged batches of GC dataset:**\")\n",
    "print(merged_df)\n",
    "\n",
    "merged_df[['PaperId', 'PaperTitle_and_Abstracts']] = merged_df['PaperId,PaperTitle_and_Abstracts'].str.split(',', n=1, expand=True)\n",
    "merged_df['PaperId'] = merged_df['PaperId'].str.strip('\"').astype(int)\n",
    "merged_df.drop(columns=['PaperId,PaperTitle_and_Abstracts', 'PaperTitle_and_Abstracts'], inplace=True)\n",
    "\n",
    "print(\"\\n\\n**Merged batches of GC dataset after splitting and dropping PaperTitle and Abstract columns:**\")\n",
    "print(merged_df)\n",
    "article_df['PaperId'] = article_df['PaperId'].astype(int)\n",
    "print(\"\\n\\n-->Success!\")\n",
    "gclength = len(merged_df)\n",
    "output_filename = f\"corrected_classification_gc_output_{gclength}.csv\"\n",
    "merged_df.to_csv(output_filename, index=False)\n",
    "\n",
    "print(merged_df)\n",
    "print(f\"\\nLength of merged gc dataset: {gclength}\")\n",
    "print(f\"Final file saved as: {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e18bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load  full dataset for merging\n",
    "os.chdir(output_directory)\n",
    "print(\"\\nCurrent Working Directory:\", os.getcwd())\n",
    "cancer_df = pd.read_csv(\"binary_cancer_matrix_filtered.csv\")\n",
    "len_cancer_df=len(cancer_df)\n",
    "print(f\" --> Total rows in cancer dataset: {len_cancer_df:,}\")\n",
    "cancer_df = cancer_df[['PaperId', 'PaperTitle', 'Abstract']].copy()\n",
    "print(f\" --> Reconfirmed: Total rows in cancer dataset: {len_cancer_df:,}\")\n",
    "print(\"--> Full cancer dataset loaded!\")\n",
    "os.chdir(classifier_directory)\n",
    "print(\"\\nCurrent Working Directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24575228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge classified dataset with cancer terms and save final output\n",
    "os.chdir(classifier_directory)\n",
    "print(\"\\nCurrent Working Directory:\", os.getcwd())\n",
    "start_time = time.time()\n",
    "total_rows = len(merged_df)\n",
    "with tqdm(total=total_rows, desc=\"Merging Data\", unit=\"rows\") as pbar:\n",
    "    full_merged_df = merged_df.merge(cancer_df, on=\"PaperId\", how=\"left\", indicator=True)\n",
    "    pbar.update(total_rows)\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "successful_matches = full_merged_df['_merge'].value_counts().get('both', 0)\n",
    "failed_matches = full_merged_df['_merge'].value_counts().get('left_only', 0)\n",
    "full_merged_df.drop(columns=['_merge'], inplace=True)\n",
    "full_merged_df.to_csv(\"full_merged_output_after_classification.csv\", index=False)\n",
    "full_length = len(full_merged_df)\n",
    "output_filename = f\"final_gc_classificaton_output_{full_length}.csv\"\n",
    "full_merged_df.to_csv(output_filename, index=False)\n",
    "\n",
    "print(f\"Total Entries in merged_df: {len(merged_df)}\")\n",
    "print(f\"Successful Matches: {successful_matches}\")\n",
    "print(f\"Unmatched Entries: {failed_matches}\")\n",
    "print(f\"Execution Time: {execution_time:.2f} seconds\")\n",
    "print(f\"Final file saved as: {output_filename}\")\n",
    "print(f\"Total rows in final dataset: {full_length}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
