{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbdd887b",
   "metadata": {},
   "source": [
    "# LLM-based variant extration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83670bcd",
   "metadata": {},
   "source": [
    "# 1) Set up libraries and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e0639c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from functools import reduce\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d977552",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set the working directory and file paths\n",
    "working_directory = \"WORKING_DIRECTORY\"\n",
    "NLP_directory = \"NLP_DIRECTORY\"\n",
    "output_directory = \"OUTPUT_DIRECTORY\"\n",
    "articles_file = \"BioBERT_file.csv\"\n",
    "\n",
    "# Load the articles file\n",
    "os.chdir(output_directory)\n",
    "if \"full_articles\" not in globals():\n",
    "    full_articles = pd.read_csv(articles_file)\n",
    "    print(f\"Loaded {len(full_articles)} articles from CSV.\")\n",
    "else:\n",
    "    print(\"Using preloaded full_articles from memory.\")\n",
    "articles = full_articles\n",
    "print(\"Article import successful!\")\n",
    "print(f\"\\nImported {len(articles):,} articles with {len(articles.columns):,} selected columns.\")\n",
    "\n",
    "# Get the number of rows and columns\n",
    "num_rows = articles.shape[0]\n",
    "num_columns = articles.shape[1]\n",
    "os.chdir(working_directory)\n",
    "print(\"\\nCurrent Working Directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97a1e27",
   "metadata": {},
   "source": [
    "# 2) Select and set up LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75357b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a language model to answer the questions\n",
    "!pip install OpenAI\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from openai import OpenAI\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ccd919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models to be tested\n",
    "models = [\"llama31-70b\", \"llama33-70b\", \"deepseek_v3\", \n",
    "          \"deepseek_r1\", \"deepseek_r1_distill_llama_70b\",\"gpt4o\"]\n",
    "\n",
    "# Mapping model names to their full Hugging Face or DeepInfra identifiers\n",
    "model_fullnames = {\n",
    "    \"llama31-70b\": \"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n",
    "    \"llama33-70b\": \"meta-llama/Llama-3.3-70B-Instruct\",\n",
    "    \"deepseek_v3\": \"deepseek-ai/DeepSeek-V3\",\n",
    "    \"deepseek_r1\": \"deepseek-ai/DeepSeek-R1\",\n",
    "    \"deepseek_r1_distill_llama_70b\": \"deepseek-ai/DeepSeek-R1-Distill-Llama-70B\",\n",
    "    \"gpt4o\": \"gpt-4o\"\n",
    "}\n",
    "\n",
    "SYSTEM_MSG = \"You are a helpful medical question answering assistant. Please carefully follow the exact instructions and do not provide explanations.\"\n",
    "modelname = models[4] #in Python, list indexing starts from 0, not 1.\n",
    "\n",
    "if modelname in [ \"llama2-3b\" ]:  # Local model\n",
    "    model, tokenizer = load(model_fullnames[modelname])\n",
    "    def generateFromPrompt(prompt):\n",
    "        if hasattr(tokenizer, \"apply_chat_template\") and tokenizer.chat_template is not None:\n",
    "            messages = [{\"role\": \"system\", \"content\": SYSTEM_MSG},\n",
    "                {\"role\": \"user\", \"content\": prompt}]\n",
    "            prompt = tokenizer.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "            response = generate(model, tokenizer, prompt=prompt, verbose=False)\n",
    "            return response\n",
    "elif modelname in [ \"gpt35\", \"gpt4o\" ]: # OpenAI models\n",
    "    client = OpenAI(\n",
    "       api_key='API_key1'\n",
    "    )\n",
    "    def generateFromPrompt(promptStr,maxTokens=100):\n",
    "      messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_MSG},\n",
    "        {\"role\": \"user\", \"content\": promptStr}\n",
    "      ]\n",
    "      completion = client.chat.completions.create(\n",
    "        model=model_fullnames[modelname],\n",
    "        messages=messages)\n",
    "      response=completion.choices[0].message.content\n",
    "      return(response)\n",
    "elif modelname in [ \"llama31-70b\" , \"llama33-70b\" , \"deepseek_v3\" , \"deepseek_r1\" , \"deepseek_r1_distill_llama_70b\"]:  # DeepInfra models\n",
    "    client = OpenAI(\n",
    "        api_key = \"API_key2\",\n",
    "        base_url=\"https://api.deepinfra.com/v1/openai\",\n",
    "    )\n",
    "    def generateFromPrompt(promptStr,maxTokens=100):\n",
    "      messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_MSG},\n",
    "        {\"role\": \"user\", \"content\": promptStr}\n",
    "      ]\n",
    "      completion = client.chat.completions.create(\n",
    "        model=model_fullnames[modelname],\n",
    "        messages=messages)\n",
    "      response=completion.choices[0].message.content\n",
    "      return(response)\n",
    "generateFromPrompt(\"hello!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8977f275",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"All installed models:\",   models)\n",
    "print(\"Current model in use:\",   modelname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cb678e",
   "metadata": {},
   "source": [
    "# 3) Define prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfe876d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define multiple prompts in a dictionary\n",
    "# Use prompt #3\n",
    "PROMPTS = {\n",
    "    1: lambda title, abstract: (\n",
    "        f\"Analyze the following title and abstract to identify **genetic variants** mentioned,\" \n",
    "        f\"including specific variant names, base changes, and protein alterations.\\n\"\n",
    "        f\"Only report well-defined variants, such as specific reference IDs or exact mutations.\"\n",
    "        f\"Ignore vague terms like 'mutation detected' or generic mentions of genes (e.g., BRCA1, ATM).\"\n",
    "        f\"If no specific variant is mentioned, respond with: 'No variant.'\\n\\n\"\n",
    "        f\"For each identified variant, provide:\\n\"\n",
    "        f\"- **Variant Name**: e.g., c.2138C>G\\n\"\n",
    "        f\"- **Gene Name**: e.g., BRCA1\\n\"\n",
    "        f\"Return results in the following format: 'Variant: Variant Name, Gene Name' or 'No variant.'\\n\\n\"\n",
    "        f\"Title: {title}\\nAbstract: {abstract}\\n\"\n",
    "    ),\n",
    "\n",
    "    2: lambda title, abstract: (\n",
    "        f\"Extract all genetic variants from the following title and abstract. Only return:\\n\"\n",
    "        f\"- **HGVS Notation** (c., p., g.), e.g., c.2138C>G, p.Arg713Trp, g.32389625G>A\\n\"\n",
    "        f\"- **Protein changes** (e.g., V600E, Arg713Trp)\\n\"\n",
    "        f\"- **rsIDs** (e.g., rs121913529)\\n\"\n",
    "        f\"- Ignore vague terms (e.g., 'mutation found').\\n\\n\"\n",
    "        f\"### Format response strictly as:\\n\"\n",
    "        f\"'Variant: <mutation>, Gene: <gene>' per line, or 'No variant' if none found.\\n\\n\"\n",
    "        f\"Title: {title}\\nAbstract: {abstract}\"\n",
    "    ),\n",
    "    \n",
    "    3: lambda title, abstract: (\n",
    "        f\"Extract only specific genetic variants from the text. Return strictly:\\n\"\n",
    "        f\"- **HGVS Notation** (c., p., g.) e.g., c.2138C>G, p.Arg713Trp\\n\"\n",
    "        f\"- **Protein changes** (e.g., V600E, Arg713Trp)\\n\"\n",
    "        f\"- **rsIDs** (e.g., rs121913529)\\n\"\n",
    "        f\"- Ignore vague terms (e.g., 'mutation found').\\n\\n\"\n",
    "        f\"### Format:\\n\"\n",
    "        f\"- Variant: 'Variant: <mutation>, Gene: <gene>' per line\\n\"\n",
    "        f\"- If none, return: 'No variant'\\n\"\n",
    "        f\"- No extra text, no explanations.\\n\\n\"\n",
    "        f\"Title: {title}\\nAbstract: {abstract}\"\n",
    "    )\n",
    "}\n",
    "print(\"Prompts for gene extraction successfully defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449ca54d",
   "metadata": {},
   "source": [
    "# 4) Run genetic variant extraction with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc298f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================== CONFIGURATION ========================== #\n",
    "os.chdir(working_directory)\n",
    "tqdm.pandas()\n",
    "\n",
    "# Define batch size for processing\n",
    "BATCH_SIZE = 1000\n",
    "modelname = modelname\n",
    "selected_prompt_number = 3\n",
    "today_date = datetime.today().strftime('%Y-%m-%d')\n",
    "start_time_str = datetime.now().strftime('%Y-%m-%d %H:%M:%S') \n",
    "\n",
    "# ========================== FILE PATHS USING ============================ #\n",
    "variant_output_file_path = os.path.join(working_directory, f\"LLM_variant_extraction_{modelname}_prompt{selected_prompt_number}.csv\")\n",
    "runtime_file = os.path.join(working_directory, f\"runtime_summary_{modelname}_prompt{selected_prompt_number}.txt\")\n",
    "progress_log_file = os.path.join(working_directory, f\"progress_log_{modelname}_prompt{selected_prompt_number}.txt\")\n",
    "\n",
    "# ========================== ENSURE FILES EXIST ========================== #\n",
    "def ensure_file_exists(file_path, header_text=None):\n",
    "    \"\"\"Creates the file if it does not exist and optionally writes a header.\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        with open(file_path, \"w\") as f:\n",
    "            if header_text:\n",
    "                f.write(f\"{header_text}\\n\")\n",
    "                f.write(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "ensure_file_exists(runtime_file, f\"### Runtime Log - {today_date} ###\\nStart Time: {start_time_str}\")\n",
    "ensure_file_exists(progress_log_file, f\"### Progress Log - {today_date} ###\")\n",
    "if not os.path.exists(variant_output_file_path):\n",
    "    with open(variant_output_file_path, \"w\") as f:\n",
    "        f.write(\"PaperId,PaperTitle,Abstract,LLM_Prompt,LLM_Response\\n\")\n",
    "\n",
    "# ========================== LOGGING SETUP ========================== #\n",
    "logging.basicConfig(\n",
    "    filename=progress_log_file,\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "print(\"Success! All necessary files and directories are set up.\")\n",
    "print(\"Script Start Time:\", start_time_str)\n",
    "print(\"Defined prompt number:\", selected_prompt_number)\n",
    "print(\"Defined batch size:\", BATCH_SIZE)\n",
    "\n",
    "# ========================== FUNCTION DEFINITIONS ========================== #\n",
    "\n",
    "def screen_publication_for_variants(row, prompt_number):\n",
    "    \"\"\"Generates a dynamic prompt based on the selected prompt number.\"\"\"\n",
    "    title = row['PaperTitle']\n",
    "    abstract = row['Abstract']\n",
    "    if prompt_number not in PROMPTS:\n",
    "        raise ValueError(f\"Invalid prompt number: {prompt_number}. Choose between 0-5.\")\n",
    "    return PROMPTS[prompt_number](title, abstract)\n",
    "\n",
    "def process_with_llm(prompt):\n",
    "    \"\"\"Process the given prompt using the LLM model.\"\"\"\n",
    "    try:\n",
    "        response = generateFromPrompt(prompt) \n",
    "        return response\n",
    "    except Exception as e:\n",
    "        logging.error(f\"LLM processing error: {e}\")\n",
    "        return \"ERROR\"\n",
    "print(\"Functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470b4e9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ========================== RESUME FROM LAST CHECKPOINT ========================== #\n",
    "# Load cancer dataset (ensure it's already loaded in memory)\n",
    "if 'articles' not in globals():\n",
    "    raise ValueError(\"Dataset `articles` is not loaded in memory. Make sure it's defined before running the script.\")\n",
    "\n",
    "# Ensure 'PaperId' column exists for tracking progress\n",
    "if 'PaperId' not in articles.columns:\n",
    "    raise KeyError(\"Dataset must contain a 'PaperId' column to track progress.\")\n",
    "\n",
    "# Check if output file exists and count previously processed articles\n",
    "if os.path.exists(variant_output_file_path):\n",
    "    processed_df = pd.read_csv(variant_output_file_path)\n",
    "    processed_articles = set(processed_df['PaperId'])  \n",
    "    total_processed_articles = len(processed_articles) \n",
    "    print(f\"Resuming from last processed row. {total_processed_articles} articles completed so far.\")\n",
    "else:\n",
    "    processed_articles = set()\n",
    "    total_processed_articles = 0\n",
    "    print(\"Starting fresh processing.\")\n",
    "\n",
    "# Calculate total batches\n",
    "total_batches = (len(articles) // BATCH_SIZE) + (1 if len(articles) % BATCH_SIZE != 0 else 0)\n",
    "if total_processed_articles == len(articles):\n",
    "    print(\"\\nAll batches are complete. No more articles to process.\")\n",
    "    print(\"You have successfully processed the entire dataset.\")\n",
    "    \n",
    "    try:\n",
    "        sys.exit(0)  #Exit normally\n",
    "    except SystemExit:\n",
    "        pass  #Suppress the SystemExit message in Jupyter\n",
    "\n",
    "\n",
    "# Filter only unprocessed articles\n",
    "unprocessed_df = articles[~articles['PaperId'].isin(processed_articles)]\n",
    "total_articles = len(unprocessed_df)    \n",
    "    \n",
    "print(\"Success! All necessary files and directories are set up.\")\n",
    "print(\"Defined prompt number:\", selected_prompt_number)\n",
    "print(\"Defined model:\", modelname)\n",
    "print(\"Defined batch size to run in chunks:\", BATCH_SIZE)\n",
    "print(f\"Total unprocessed articles: {total_articles}\")\n",
    "\n",
    "# ========================== TRACK CUMULATIVE RUNTIME ========================== #\n",
    "# Load previous runtime if exists\n",
    "if os.path.exists(runtime_file):\n",
    "    with open(runtime_file, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        total_runtime_previous = sum(float(line.split(\":\")[-1].strip().split()[0])\n",
    "                                     for line in lines if \"Total runtime so far\" in line)\n",
    "else:\n",
    "    total_runtime_previous = 0.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c72a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================== BATCH PROCESSING ========================== #\n",
    "start_time = time.time()\n",
    "os.chdir(working_directory)\n",
    "\n",
    "# Calculate the next batch number\n",
    "batch_number = (total_processed_articles // BATCH_SIZE) + 1\n",
    "for batch_start in range(0, total_articles, BATCH_SIZE):\n",
    "    batch_end = min(batch_start + BATCH_SIZE, total_articles)\n",
    "    batch = unprocessed_df.iloc[batch_start:batch_end].copy()\n",
    "    print(f\"\\nProcessing Batch {batch_number}/{total_batches} ({batch_start + 1} to {batch_end})...\")\n",
    "\n",
    "    batch_start_time = time.time()\n",
    "    batch['LLM_Prompt'] = batch.apply(lambda row: screen_publication_for_variants(row, selected_prompt_number), axis=1)\n",
    "\n",
    "    # Process with LLM\n",
    "    llm_response_column = f'LLM_Response_{modelname}'\n",
    "    batch[llm_response_column] = batch['LLM_Prompt'].progress_apply(process_with_llm)\n",
    "    batch_runtime = time.time() - batch_start_time\n",
    "    batch_to_save = batch[['PaperId', 'PaperTitle', 'Abstract', 'LLM_Prompt', llm_response_column]]\n",
    "\n",
    "    # Calculate total progress\n",
    "    def generate_progress_bar(percentage, bar_length=20):\n",
    "        filled_length = int(bar_length * percentage / 100)\n",
    "        bar = '|' * filled_length + '-' * (bar_length - filled_length)\n",
    "        return f\"[{bar}] {percentage:.2f}%\"\n",
    "    total_articles = batch_end + len(processed_articles)  \n",
    "    total_articles_to_process = len(unprocessed_df) - batch_end \n",
    "    processed_percentage = (total_articles / len(articles)) * 100\n",
    "    to_process_percentage = (total_articles_to_process / len(articles)) * 100\n",
    "\n",
    "    # Save progress to CSV\n",
    "    if os.path.exists(variant_output_file_path):\n",
    "        batch_to_save.to_csv(variant_output_file_path, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        batch_to_save.to_csv(variant_output_file_path, mode='w', index=False)\n",
    "\n",
    "    total_runtime_so_far = total_runtime_previous + (time.time() - start_time)\n",
    "\n",
    "    with open(runtime_file, \"a\") as f:\n",
    "        f.write(f\"\\nBatch {batch_number}/{total_batches} started at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Batch Runtime: {batch_runtime:.2f} sec\\n\")\n",
    "        f.write(f\"Total runtime so far (all runs combined): {total_runtime_so_far:.2f} sec\\n\")\n",
    "        f.write(f\"Total articles processed in this batch: {batch_end}\\n\")\n",
    "        f.write(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "    logging.info(f\"Processed batch {batch_number}/{total_batches} in {batch_runtime:.2f} sec.\")\n",
    "\n",
    "    if processed_percentage >= 100:\n",
    "        print(\"\\nAll articles have been successfully processed.\")\n",
    "        print(\"No more articles remaining.\")\n",
    "        break\n",
    "    else:\n",
    "        print(f\"\\nPaused! {batch_end} articles processed in this batch.\")\n",
    "        print(f\"{total_articles} articles processed in total {generate_progress_bar(processed_percentage)}\")\n",
    "        print(f\"{total_articles_to_process} articles to process in total {generate_progress_bar(to_process_percentage)}\")\n",
    "        print(\"Check the CSV and runtime file. When ready, rerun the script to continue processing.\")\n",
    "        break \n",
    "\n",
    "# ========================== FINAL SUMMARY ========================== #\n",
    "total_runtime = total_runtime_so_far\n",
    "total_hours = total_runtime // 3600\n",
    "total_minutes = (total_runtime % 3600) // 60\n",
    "total_seconds = total_runtime % 60\n",
    "\n",
    "summary_text = f\"\"\"\n",
    "### Genetic Variant Extraction Summary ###\n",
    "\n",
    "- Model used: {modelname}\n",
    "- Prompt number: {selected_prompt_number}\n",
    "- Total batches processed: {batch_number}/{total_batches}\n",
    "- Total rows processed: {total_articles}\n",
    "- Cumulative runtime: {total_runtime:.2f} seconds ({total_hours:.0f} hr {total_minutes:.0f} min {total_seconds:.2f} sec)\n",
    "\"\"\"\n",
    "\n",
    "print(summary_text)\n",
    "with open(runtime_file, \"a\") as f:\n",
    "    f.write(\"\\n### Final runtime summary ###\\n\")\n",
    "    f.write(f\"End Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    f.write(summary_text)\n",
    "    f.write(\"\\n\" + \"=\" * 60 + \"\\n\")\n",
    "print(\"Final results saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
