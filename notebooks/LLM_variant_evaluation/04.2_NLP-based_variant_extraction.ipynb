{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbdd887b",
   "metadata": {},
   "source": [
    "# String matching and NLP for variant extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83670bcd",
   "metadata": {},
   "source": [
    "# 1) Set up libraries and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e0639c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from functools import reduce\n",
    "from collections import Counter\n",
    "from datetime import datetime, timedelta\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d977552",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set the working directory and file paths\n",
    "working_directory = \"WORKING_DIRECTORY\"\n",
    "output_directory = \"OUTPUT_DIRECTORY\"\n",
    "articles_file = \"BioBERT_file.csv\"\n",
    "\n",
    "os.chdir(output_directory)\n",
    "if \"full_articles\" not in globals():\n",
    "    full_articles = pd.read_csv(articles_file)\n",
    "    print(f\"Loaded {len(full_articles)} articles from CSV.\")\n",
    "else:\n",
    "    print(\"Using preloaded full_articles from memory.\")\n",
    "articles = full_articles\n",
    "print(\"Article import successful!\")\n",
    "print(f\"\\nImported {len(articles):,} articles with {len(articles.columns):,} selected columns.\")\n",
    "\n",
    "# Get the number of rows and columns\n",
    "num_rows = articles.shape[0]\n",
    "num_columns = articles.shape[1]\n",
    "os.chdir(working_directory)\n",
    "print(\"\\nCurrent Working Directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0d2b58",
   "metadata": {},
   "source": [
    "# 2) Run NLP approaches for variant extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558c5ab7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_ner_bionlp13cg_md\")\n",
    "text = \"The variant R175H in TP53 and rs5643728 were observed in breast cancer patients.\"\n",
    "doc = nlp(text)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b176ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define relevant labels\n",
    "tqdm.pandas()\n",
    "relevant_labels = {\"AMINO_ACID\", \"GENE_OR_GENE_PRODUCT\"}\n",
    "def extract_relevant_entities(text):\n",
    "    doc = nlp(str(text))\n",
    "    return [ent.text for ent in doc.ents if ent.label_ in relevant_labels]\n",
    "\n",
    "# Apply NER with progress bar\n",
    "articles[\"TitleEntities\"] = articles[\"PaperTitle\"].progress_apply(extract_relevant_entities)\n",
    "articles[\"AbstractEntities\"] = articles[\"Abstract\"].progress_apply(extract_relevant_entities)\n",
    "def has_any_entity(row):\n",
    "    return bool(row[\"TitleEntities\"] or row[\"AbstractEntities\"])\n",
    "articles_with_entities = articles[articles.progress_apply(has_any_entity, axis=1)]\n",
    "print(f\"Number of articles with relevant entities: {len(articles_with_entities)}\")\n",
    "\n",
    "def extract_real_variants(text):\n",
    "    text = str(text)\n",
    "    variants = []\n",
    "    variants += re.findall(r'rs\\d+', text)                     \n",
    "    variants += re.findall(r'\\b[A-Z]\\d+[A-Z]\\b', text)      \n",
    "    return list(set(variants)) \n",
    "articles[\"TitleVariants\"] = articles[\"PaperTitle\"].progress_apply(extract_real_variants)\n",
    "articles[\"AbstractVariants\"] = articles[\"Abstract\"].progress_apply(extract_real_variants)\n",
    "\n",
    "# Clean RealVariants columns to remove empty lists and replace with \"No variant\"\n",
    "def clean_variants(row):\n",
    "    if len(row[\"TitleVariants\"]) == 0 and len(row[\"AbstractVariants\"]) == 0:\n",
    "        return \"No variant\"\n",
    "    else:\n",
    "        return \", \".join(row[\"TitleVariants\"] + row[\"AbstractVariants\"])\n",
    "\n",
    "articles[\"AllVariants\"] = articles.apply(lambda row: clean_variants(row), axis=1)\n",
    "articles.rename(columns={\"AllVariants\": \"NLP\"}, inplace=True)\n",
    "columns_to_save = [\"PaperId\", \"PaperTitle\", \"TitleEntities\", \"Abstract\", \"AbstractEntities\", \"NLP\"]\n",
    "articles_with_entities.to_csv(\"articles_with_extracted_entities_and_variants.csv\", index=False, columns=columns_to_save)\n",
    "print(\"Saved to 'articles_with_extracted_entities_and_variants.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df83918f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(articles_with_entities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
