{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbdd887b",
   "metadata": {},
   "source": [
    "# Evaluation and confusion matrix creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83670bcd",
   "metadata": {},
   "source": [
    "# 1) Set up libraries and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e0639c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from functools import reduce\n",
    "from collections import Counter\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d977552",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set the working directory and file paths\n",
    "working_directory = \"INPUT_DIRECTORY\"\n",
    "NLP_directory = \"NLP_DIRECTORY\"\n",
    "output_directory = \"OUTPUT_DIRECTORY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b9271f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set working directory\n",
    "os.chdir(working_directory)\n",
    "\n",
    "llm_files = [f for f in os.listdir() if f.startswith(\"LLM_variant_extraction_\") and f.endswith(\"_prompt3.csv\")]\n",
    "df_list = []\n",
    "\n",
    "meta_columns = [\"PaperId\", \"PaperTitle\", \"Abstract\"]\n",
    "meta_df = None\n",
    "\n",
    "for file in llm_files:\n",
    "    model_match = re.search(r\"LLM_variant_extraction_(.*?)_prompt3\\.csv\", file)\n",
    "    model_name = model_match.group(1) if model_match else f\"model_{len(df_list)+1}\"\n",
    "\n",
    "    df = pd.read_csv(file)\n",
    "    df[\"PaperId\"] = pd.to_numeric(df[\"PaperId\"], errors=\"coerce\").fillna(0).astype(\"int64\")\n",
    "\n",
    "    if meta_df is None:\n",
    "        meta_df = df[meta_columns]\n",
    "\n",
    "    response_column = df.columns[-1]\n",
    "    df = df[[\"PaperId\", response_column]].rename(columns={response_column: f\"LLM_Response_{model_name}\"})\n",
    "\n",
    "    df_list.append(df)\n",
    "\n",
    "# Load human variant extraction file\n",
    "human_file = \"Human_variant_extraction.csv\"\n",
    "if os.path.exists(human_file):\n",
    "    human_df = pd.read_csv(human_file)\n",
    "    human_df[\"PaperId\"] = pd.to_numeric(human_df[\"PaperId\"], errors=\"coerce\").fillna(0).astype(\"int64\")\n",
    "    human_col = human_df.columns[-1]\n",
    "    human_df = human_df[[\"PaperId\", human_col]].rename(columns={human_col: \"Human_analysis\"})\n",
    "    df_list.append(human_df)\n",
    "\n",
    "merged_df = reduce(lambda left, right: pd.merge(left, right, on=\"PaperId\", how=\"outer\"), df_list)\n",
    "Variant_LLM_dataset_Evaluation = pd.merge(meta_df, merged_df, on=\"PaperId\", how=\"left\")\n",
    "Variant_LLM_dataset_Evaluation = Variant_LLM_dataset_Evaluation.sort_values(by=\"PaperId\").reset_index(drop=True)\n",
    "\n",
    "print(\"Merged dataset shape:\", Variant_LLM_dataset_Evaluation.shape)\n",
    "print(\"Column names:\", Variant_LLM_dataset_Evaluation.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5783ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data manually and merge to common file including NLP!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663a46f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Upload data\n",
    "file_path = os.path.join(working_directory, \"merged_variant_extraction_llm_nlp_human.csv\")\n",
    "confusion_matrix_df = pd.read_csv(file_path)\n",
    "print(confusion_matrix_df.shape)\n",
    "print(confusion_matrix_df.columns.tolist())\n",
    "print(confusion_matrix_df.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6f8217",
   "metadata": {},
   "source": [
    "## RUN EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31532ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model and human columns\n",
    "model_columns = [\n",
    "    'LLM_Response_llama31-70b',\n",
    "    'LLM_Response_gpt4o',\n",
    "    'LLM_Response_llama33-70b',\n",
    "    'LLM_Response_deepseek_v3',\n",
    "    'NLP'\n",
    "]\n",
    "human_col = 'Human_analysis'\n",
    "\n",
    "### --- Part 1: Exact Row Match (Model vs Human) ---\n",
    "exact_match_counts = {}\n",
    "\n",
    "for model in model_columns:\n",
    "    match_series = confusion_matrix_df[model].fillna(\"\").str.strip() == confusion_matrix_df[human_col].fillna(\"\").str.strip()\n",
    "    exact_match_counts[model] = {\n",
    "        \"Total Matches\": match_series.sum(),\n",
    "        \"Total Rows\": len(match_series),\n",
    "        \"Match Percentage\": 100 * match_series.sum() / len(match_series)\n",
    "    }\n",
    "\n",
    "exact_match_df = pd.DataFrame(exact_match_counts).T\n",
    "print(\"Exact string match results:\")\n",
    "print(exact_match_df)\n",
    "\n",
    "\n",
    "### --- Part 2: Count Variant-Gene Pairs in Each Column ---\n",
    "def count_variant_gene_pairs(text):\n",
    "    if pd.isna(text):\n",
    "        return 0\n",
    "    pattern = r'Variant:\\s*.+?,\\s*Gene:\\s*.+?(\\n|$)'\n",
    "    matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "    return len(matches)\n",
    "\n",
    "variant_gene_counts = {}\n",
    "variant_gene_row_counts = {}\n",
    "\n",
    "for col in model_columns + [human_col]:\n",
    "    counts = confusion_matrix_df[col].apply(count_variant_gene_pairs)\n",
    "    \n",
    "    # Part 2: Total count and max per row\n",
    "    variant_gene_counts[col] = {\n",
    "        \"Total Variant-Gene Pairs\": counts.sum(),\n",
    "        \"Max in Single Entry\": counts.max()\n",
    "    }\n",
    "\n",
    "    # Part 3: Number of rows with at least 1 variant-gene pair, and with 0\n",
    "    num_with_variants = (counts > 0).sum()\n",
    "    num_without_variants = (counts == 0).sum()\n",
    "\n",
    "    variant_gene_row_counts[col] = {\n",
    "        \"Rows With ≥1 Variant-Gene Pair\": num_with_variants,\n",
    "        \"Rows With 0 Variant-Gene Pairs\": num_without_variants,\n",
    "        \"Total Rows\": len(counts),\n",
    "    }\n",
    "\n",
    "variant_gene_df = pd.DataFrame(variant_gene_counts).T\n",
    "variant_detection_df = pd.DataFrame(variant_gene_row_counts).T\n",
    "\n",
    "print(\"\\nVariant-Gene pair counts (Total and Max):\")\n",
    "print(variant_gene_df)\n",
    "\n",
    "print(\"\\nRows with at least one Variant-Gene pair detected:\")\n",
    "print(variant_detection_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5accb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "file_path = os.path.join(working_directory, \"merged_variant_extraction_llm_nlp_human.csv\")\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Define columns\n",
    "model_columns = [\n",
    "    'LLM_Response_llama31-70b',\n",
    "    'LLM_Response_gpt4o',\n",
    "    'LLM_Response_llama33-70b',\n",
    "    'LLM_Response_deepseek_v3',\n",
    "    'NLP'\n",
    "]\n",
    "human_col = 'Human_analysis'\n",
    "\n",
    "# Extract variant-gene pairs from text\n",
    "def extract_pairs(text):\n",
    "    if pd.isna(text):\n",
    "        return set()\n",
    "    pattern = r'Variant:\\s*(.+?),\\s*Gene:\\s*(.+?)(?:\\n|$)'\n",
    "    matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "    return set([f\"Variant: {v.strip()}, Gene: {g.strip()}\" for v, g in matches])\n",
    "\n",
    "# Prepare results\n",
    "results = {}\n",
    "examples = {}\n",
    "\n",
    "# Evaluate each model\n",
    "for model in model_columns:\n",
    "    tp = fp = fn = tn = 0\n",
    "    false_positives = []\n",
    "    false_negatives = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        human_pairs = extract_pairs(row[human_col])\n",
    "        model_pairs = extract_pairs(row[model])\n",
    "        tp_pairs = model_pairs & human_pairs\n",
    "        fp_pairs = model_pairs - human_pairs\n",
    "        fn_pairs = human_pairs - model_pairs\n",
    "        tp += len(tp_pairs)\n",
    "        fp += len(fp_pairs)\n",
    "        fn += len(fn_pairs)\n",
    "        if not human_pairs and not model_pairs:\n",
    "            tn += 1\n",
    "\n",
    "        if fp_pairs:\n",
    "            false_positives.append({\"Row\": idx, \"Pairs\": list(fp_pairs)})\n",
    "        if fn_pairs:\n",
    "            false_negatives.append({\"Row\": idx, \"Pairs\": list(fn_pairs)})\n",
    "\n",
    "    # Metrics\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    total_predictions = tp + fp + fn + tn\n",
    "    accuracy = (tp + tn) / total_predictions if total_predictions > 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "\n",
    "    results[model] = {\n",
    "        \"True Positives (TP)\": tp,\n",
    "        \"False Positives (FP)\": fp,\n",
    "        \"False Negatives (FN)\": fn,\n",
    "        \"True Negatives (TN)\": tn,\n",
    "        \"F1 Score\": f1,\n",
    "        \"Sensitivity (Recall)\": recall,\n",
    "        \"Specificity\": specificity,\n",
    "        \"Precision\": precision,\n",
    "        \"Accuracy\": accuracy\n",
    "    }\n",
    "\n",
    "    examples[model] = {\n",
    "        \"False Positives\": false_positives[:3],\n",
    "        \"False Negatives\": false_negatives[:3]\n",
    "    }\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results).T\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=results_df.reset_index(), x='index', y='F1 Score', palette=\"mako\")\n",
    "plt.title(\"F1 Scores by LLM Model\")\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.xlabel(\"LLM Model\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for metric in [\"Precision\", \"Sensitivity (Recall)\", \"Accuracy\"]:\n",
    "    plt.plot(results_df.index, results_df[metric], marker='o', label=metric)\n",
    "\n",
    "plt.title(\"Model Performance Metrics\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xlabel(\"LLM Model\")\n",
    "plt.ylim(0, 1.05)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display False Positives and False Negatives\n",
    "for model in model_columns:\n",
    "    print(f\"\\nModel: {model}\")\n",
    "    print(\"False Positives (examples):\")\n",
    "    for ex in examples[model][\"False Positives\"]:\n",
    "        print(f\"  Row {ex['Row']} → {ex['Pairs']}\")\n",
    "    print(\"False Negatives (examples):\")\n",
    "    for ex in examples[model][\"False Negatives\"]:\n",
    "        print(f\"  Row {ex['Row']} → {ex['Pairs']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23568b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refine model names\n",
    "new_model_names = {\n",
    "    'LLM_Response_deepseek_v3': 'Deepseek_v3',\n",
    "    'LLM_Response_gpt4o': 'GPT-4o',\n",
    "    'LLM_Response_llama31-70b': 'LLaMA3.1-70b',\n",
    "    'LLM_Response_llama33-70b': 'LLaMA3.3-70b',\n",
    "    'NLP':'en_ner_bionlp13cg_md'\n",
    "}\n",
    "\n",
    "# Define model order\n",
    "ordered_models = [\n",
    "    'LLM_Response_deepseek_v3',\n",
    "    'LLM_Response_gpt4o',\n",
    "    'LLM_Response_llama31-70b',\n",
    "    'LLM_Response_llama33-70b',\n",
    "    'NLP'\n",
    "]\n",
    "\n",
    "# Metrics\n",
    "count_metrics = [\"True Positives (TP)\", \"False Positives (FP)\", \"False Negatives (FN)\", \"True Negatives (TN)\"]\n",
    "score_metrics = [\"Precision\", \"Sensitivity (Recall)\", \"Accuracy\",\"F1 Score\"]\n",
    "counts_df = results_df.loc[ordered_models, count_metrics].copy()\n",
    "scores_df = results_df.loc[ordered_models, score_metrics].copy()\n",
    "counts_df.index = [new_model_names[m] for m in counts_df.index]\n",
    "scores_df.index = [new_model_names[m] for m in scores_df.index]\n",
    "counts_col_percent_df = counts_df.copy()\n",
    "\n",
    "for col in counts_col_percent_df.columns:\n",
    "    col_vals = counts_col_percent_df[col]\n",
    "    if col in [\"False Positives (FP)\", \"False Negatives (FN)\"]:\n",
    "        max_val = col_vals.max()\n",
    "        if max_val > 0:\n",
    "            counts_col_percent_df[col] = (1 - col_vals / max_val) * 100\n",
    "        else:\n",
    "            counts_col_percent_df[col] = 100 \n",
    "    else:\n",
    "        max_val = col_vals.max()\n",
    "        if max_val > 0:\n",
    "            counts_col_percent_df[col] = (col_vals / max_val) * 100\n",
    "\n",
    "            \n",
    "            \n",
    "###### Plot side-by-side heatmaps\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Left: Confusion matrix heatmap\n",
    "sns.heatmap(counts_col_percent_df, annot=counts_df, fmt=\".0f\", cmap=\"Greens\", linewidths=0.5,\n",
    "            ax=axes[0], cbar_kws={'label': 'Relative comparison (%)'})\n",
    "axes[0].set_title(\"Confusion matrix\")\n",
    "axes[0].set_xlabel(\"\")\n",
    "axes[0].set_ylabel(\"Model / approach\")\n",
    "axes[0].tick_params(axis='y', rotation=0)\n",
    "\n",
    "# Right: Standard performance metrics\n",
    "sns.heatmap(scores_df, annot=True, fmt=\".2f\", cmap=\"Blues\", linewidths=0.5,\n",
    "            ax=axes[1], cbar_kws={'label': 'Score'})\n",
    "axes[1].set_title(\"Performance metrics\")\n",
    "axes[1].set_xlabel(\"\")\n",
    "axes[1].set_ylabel(\"\")\n",
    "axes[1].tick_params(axis='y', rotation=0)\n",
    "\n",
    "# Rotate x-ticks for readability\n",
    "for ax in axes:\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "output_path = \"final_llm_nlp_human_evaluations.png\"\n",
    "plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"Plot saved successfully as '{output_path}'.\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
