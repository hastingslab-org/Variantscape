{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6e09ddd",
   "metadata": {},
   "source": [
    "# Extract genes with BioBERT model and MyGene.info API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732021a4",
   "metadata": {},
   "source": [
    "# 1) Install libraries and load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852821b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "print(\"Success!\")\n",
    "from transformers import pipeline\n",
    "# Load  BioBERT model\n",
    "biobert_model = pipeline(\"ner\", model=\"dmis-lab/biobert-base-cased-v1.1\", tokenizer=\"dmis-lab/biobert-base-cased-v1.1\")\n",
    "print(\"BioBERT model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2c9f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from fuzzywuzzy import process, fuzz\n",
    "print(\"Import successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761c47ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the working directory and file paths\n",
    "output_directory = \"OUTPUT_DIRECTORY\"\n",
    "genes_file = \"civic_genes.csv\"\n",
    "articles_file = \"clean_df_mCRPC_step4.csv\"\n",
    "\n",
    "# Change the working directory\n",
    "os.chdir(output_directory)\n",
    "print(\"Current Working Directory:\", os.getcwd())\n",
    "\n",
    "# Load  genes file\n",
    "genes = pd.read_csv(genes_file, header=None)\n",
    "gene_list = genes[0].tolist()\n",
    "print(\"Genes import successful!\")\n",
    "\n",
    "# Load the articles file\n",
    "articles = pd.read_csv(articles_file)\n",
    "print(\"Article import successful!\")\n",
    "\n",
    "# Display dataset info\n",
    "print(f\"Loaded full dataset of {len(articles):,} articles\")\n",
    "print(f\"Loaded {len(gene_list):,} CIViC genes\")\n",
    "\n",
    "# Current extraction\n",
    "article_df=articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d5ab2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of rows and columns\n",
    "num_rows = articles.shape[0]\n",
    "num_columns = articles.shape[1]\n",
    "print(f\"The articles df contains {num_rows:,} rows and {num_columns:,} columns.\")\n",
    "\n",
    "column_names = articles.columns.tolist()\n",
    "print(\"The column names in the articles df are:\")\n",
    "for col in column_names:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa12a937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BioBERT model\n",
    "biobert_model = pipeline(\n",
    "    \"ner\",\n",
    "    model=\"alvaroalon2/biobert_genetic_ner\",\n",
    "    tokenizer=\"alvaroalon2/biobert_genetic_ner\",\n",
    "    device=0 if torch.cuda.is_available() else -1 \n",
    ")\n",
    "print(\"BioBERT model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f775fe3b",
   "metadata": {},
   "source": [
    "# 2) Run BioBERT model with sliding window approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803db384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch gene synonyms from MyGene.info API\n",
    "def get_gene_synonyms(gene_symbol):\n",
    "    \"\"\"Fetches known synonyms, including protein products, for a given gene from MyGene.info.\"\"\"\n",
    "    url = f\"https://mygene.info/v3/query?q={gene_symbol}&fields=symbol,alias,other_names\"\n",
    "    try:\n",
    "        response = requests.get(url).json()\n",
    "        synonyms = set()\n",
    "        for hit in response.get(\"hits\", []):\n",
    "            if \"symbol\" in hit:\n",
    "                synonyms.add(hit[\"symbol\"].upper())\n",
    "            if \"alias\" in hit:\n",
    "                synonyms.update([alias.upper() for alias in hit[\"alias\"]])\n",
    "            if \"other_names\" in hit:\n",
    "                synonyms.update([name.upper() for name in hit[\"other_names\"]])\n",
    "        return synonyms\n",
    "    except:\n",
    "        return {gene_symbol.upper()} \n",
    "\n",
    "# Expand the gene list dynamically with synonyms\n",
    "expanded_gene_list = {gene.upper(): get_gene_synonyms(gene) for gene in gene_list}\n",
    "print(f\"Expanded gene list contains {len(expanded_gene_list)} genes with synonyms.\")\n",
    "\n",
    "# Function to normalize extracted genes\n",
    "def normalize_extracted_genes(found_terms):\n",
    "    \"\"\"Normalize and map extracted entities to closest known gene or protein names.\"\"\"\n",
    "    normalized_genes = set()\n",
    "    for term in found_terms:\n",
    "        term_upper = term.upper()\n",
    "        if term_upper in expanded_gene_list:\n",
    "            normalized_genes.add(term_upper)\n",
    "            continue\n",
    "        cleaned_term = re.sub(r\"[\\[\\]\\(\\),-]\", \" \", term_upper)\n",
    "        cleaned_words = cleaned_term.split()\n",
    "        for word in cleaned_words:\n",
    "            if word in expanded_gene_list:\n",
    "                normalized_genes.add(word)\n",
    "        if not any(gene in normalized_genes for gene in cleaned_words):\n",
    "            match = process.extractOne(term_upper, expanded_gene_list.keys(), scorer=fuzz.ratio)\n",
    "            if match:\n",
    "                best_match, score = match[:2]\n",
    "                if score > 85:\n",
    "                    normalized_genes.add(best_match)\n",
    "    return normalized_genes\n",
    "\n",
    "# Function to split text into overlapping chunks for NER\n",
    "def sliding_window_chunking(text, tokenizer, max_tokens=512, stride=256):\n",
    "    \"\"\"Splits text into overlapping chunks to avoid losing context.\"\"\"\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    if len(tokens) <= max_tokens:\n",
    "        return [tokenizer.decode(tokens, skip_special_tokens=True)]\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), stride):\n",
    "        chunk = tokens[i : i + max_tokens]\n",
    "        if len(chunk) < max_tokens:\n",
    "            break\n",
    "        chunks.append(tokenizer.decode(chunk, skip_special_tokens=True))\n",
    "    return chunks\n",
    "\n",
    "# Function to process text with BioBERT using sliding window\n",
    "def process_biobert(text, model):\n",
    "    \"\"\"Runs BioBERT NER with sliding window chunking.\"\"\"\n",
    "    if pd.isna(text) or len(text.strip()) == 0:\n",
    "        return set()\n",
    "    tokenizer = model.tokenizer  # Get tokenizer\n",
    "    text_chunks = sliding_window_chunking(text, tokenizer)\n",
    "    found_terms = set()\n",
    "    for chunk in text_chunks:\n",
    "        results = model(chunk)\n",
    "        current_term = []\n",
    "        for res in results:\n",
    "            word = res[\"word\"].replace(\"##\", \"\")\n",
    "            if res[\"entity\"].startswith(\"B-\"):\n",
    "                if current_term:\n",
    "                    full_term = \"\".join(current_term)\n",
    "                    found_terms.add(full_term)\n",
    "                current_term = [word]\n",
    "            elif res[\"entity\"].startswith(\"I-\"):\n",
    "                current_term.append(word)\n",
    "        if current_term:\n",
    "            full_term = \"\".join(current_term)\n",
    "            found_terms.add(full_term)\n",
    "    return normalize_extracted_genes(found_terms)\n",
    "\n",
    "print(\"Success!\")\n",
    "\n",
    "\n",
    "##### Gene extraction #####\n",
    "start_time = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "start_timestamp = time.time()\n",
    "print(f\"Processing {len(article_df)} articles with BioBERT. Started at {start_time}\")\n",
    "\n",
    "biobert_results = []\n",
    "for index, row in tqdm(article_df.iterrows(), total=len(article_df), desc=\"Processing Articles\"):\n",
    "    title = row.get(\"PaperTitle\", \"\")\n",
    "    abstract = row.get(\"Abstract\", \"\")\n",
    "    genes_biobert = process_biobert(title, biobert_model) | process_biobert(abstract, biobert_model)\n",
    "    biobert_results.append(\", \".join(genes_biobert))\n",
    "    print(f\"Article {index+1}: {genes_biobert}\")\n",
    "df_results = article_df.copy()\n",
    "df_results[\"BioBERT\"] = biobert_results\n",
    "num_articles = len(df_results)\n",
    "\n",
    "# Generate filenames\n",
    "output_file = f\"sliding_window_filtered_articles_biobert_expanded_{num_articles}_mCRPC.csv\"\n",
    "runtime_file = f\"sliding_window_filtered_articles_biobert_expanded_{num_articles}_runtime_mCRPC.txt\"\n",
    "end_time = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "end_timestamp = time.time()\n",
    "total_runtime = end_timestamp - start_timestamp\n",
    "print(f\"Processing completed at {end_time}. Total runtime: {total_runtime:.2f} seconds.\")\n",
    "with open(runtime_file, \"w\") as f:\n",
    "    f.write(f\"Processing of articles: {num_articles}\\n\")\n",
    "    f.write(f\"Processing started at: {start_time}\\n\")\n",
    "    f.write(f\"Processing completed at: {end_time}\\n\")\n",
    "    f.write(f\"Total runtime: {total_runtime:.2f} seconds\\n\")\n",
    "print(f\"Runtime details saved in: {runtime_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add77f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the etxraction as csv\n",
    "os.chdir(output_directory)\n",
    "output_file = f\"sliding_window_filtered_articles_biobert_expanded_{num_articles}_mCRPC.csv\"\n",
    "df_results.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39495e85",
   "metadata": {},
   "source": [
    "# 3) Create binary matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d2e8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary matrix creation\n",
    "os.chdir(output_directory)\n",
    "num_articles=12532\n",
    "input_file = f\"sliding_window_gene_binary_matrix_BioBERT_MyGeneinfo_hits_only{num_articles}_mCRPC.csv\"\n",
    "\n",
    "BioBERT_dfslw = pd.read_csv(input_file)\n",
    "print(BioBERT_dfslw.head(20))\n",
    "\n",
    "BioBERT_originalslw = BioBERT_dfslw[\"BioBERT\"].copy()\n",
    "BioBERT_dfslw[\"BioBERT\"] = BioBERT_dfslw[\"BioBERT\"].fillna(\"\").astype(str)\n",
    "print(BioBERT_dfslw.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580aded6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract ALL genes from BioBERT column correctly\n",
    "BioBERT_dfslw[\"Extracted_Genesslw\"] = BioBERT_dfslw[\"BioBERT\"].apply(lambda x: [gene.strip() for gene in x.split(',') if gene.strip()])\n",
    "binary_gene_dataslw = {gene: BioBERT_dfslw[\"Extracted_Genesslw\"].apply(lambda genes: 1 if gene in genes else 0) for gene in gene_list}\n",
    "binary_gene_dfslw = pd.DataFrame(binary_gene_dataslw)\n",
    "BioBERT_dfslw = pd.concat([BioBERT_dfslw, binary_gene_dfslw], axis=1)\n",
    "BioBERT_dfslw[\"Sum_Gene_Mentions\"] = binary_gene_dfslw.sum(axis=1)\n",
    "BioBERT_dfslw[\"BioBERT\"] = BioBERT_originalslw\n",
    "BioBERT_dfslw.drop(columns=[\"Extracted_Genesslw\"], inplace=True)\n",
    "print(BioBERT_dfslw.head(20))\n",
    "\n",
    "os.chdir(output_directory)\n",
    "output_filename = f\"sliding_window_gene_binary_matrix_BioBERT_MyGeneinfo_{num_articles}_mCRPC.csv\"\n",
    "BioBERT_dfslw.to_csv(output_filename, index=False)\n",
    "print(f\"File saved as: {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900e193b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total sum of the \"Sum_Gene_Mentions\" column\n",
    "binary_gene_columnsslw = [col for col in BioBERT_dfslw.columns if col in gene_list]\n",
    "BioBERT_dfslw[binary_gene_columnsslw] = BioBERT_dfslw[binary_gene_columnsslw].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "total_gene_mentionsslw = BioBERT_dfslw[\"Sum_Gene_Mentions\"].sum()\n",
    "total_binary_sumslw = BioBERT_dfslw[binary_gene_columnsslw].sum().sum()\n",
    "results_dict_slw = {\n",
    "    \"Metric\": [\"Total_Sum_Gene_Mentions\", \"Total_Binary_Matrix_Sum\"],\n",
    "    \"Value\": [total_gene_mentionsslw, total_binary_sumslw]\n",
    "}\n",
    "\n",
    "results_df_slw = pd.DataFrame(results_dict_slw)\n",
    "results_df_slw.to_csv(\"sliding_window_Sum_Gene_Mentions_mCRPC.txt\", sep=\"\\t\", index=False)\n",
    "print(f\"Results saved to 'sliding_window_Sum_Gene_Mentions_mCRPC.txt'\")\n",
    "print(f\"Total sum of 'Sum_Gene_Mentions' column: {total_gene_mentionsslw:,}\")\n",
    "print(f\"Cross-check: Total sum of all binary matrix values (1s in the matrix): {total_binary_sumslw:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608b5811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subset of hits only\n",
    "input_filename = f\"sliding_window_gene_binary_matrix_BioBERT_MyGeneinfo_{num_articles}_mCRPC.csv\"\n",
    "BioBERT_df = pd.read_csv(input_filename)\n",
    "BioBERT_df_filteredslw = BioBERT_dfslw[BioBERT_dfslw[\"Sum_Gene_Mentions\"] > 0]\n",
    "num_filtered_rowsslw = len(BioBERT_df_filteredslw)\n",
    "total_gene_mentionsslw = BioBERT_df_filteredslw[\"Sum_Gene_Mentions\"].sum()\n",
    "output_filename = f\"sliding_window_gene_binary_matrix_BioBERT_MyGeneinfo_hits_only{num_articles}_mCRPC.csv\"\n",
    "BioBERT_df_filteredslw.to_csv(output_filename, index=False)\n",
    "\n",
    "print(f\"Number of rows after filtering: {num_filtered_rowsslw:,}\")\n",
    "print(f\"Total sum of 'Sum_Gene_Mentions' after filtering: {total_gene_mentionsslw:,}\")\n",
    "print(f\"Filtered file saved as: {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bef4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_wogenementionsslw = len(BioBERT_dfslw) - len(BioBERT_df_filteredslw)\n",
    "print(f\"Articles before gene filtering: {len(BioBERT_dfslw):,}\")\n",
    "print(f\"Articles without gene mentions: {articles_wogenementionsslw:,}\")\n",
    "print(f\"Articles with gene mentions:      {len(BioBERT_df_filteredslw):,}\")\n",
    "print(\"Percentage of articles with gene mentions\",len(BioBERT_df_filteredslw)/len(BioBERT_dfslw)*100)\n",
    "\n",
    "# Transfer dataset\n",
    "BioBERT_df_filtered=BioBERT_df_filteredslw.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858364b5",
   "metadata": {},
   "source": [
    "# 4) Cleaning and normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf90bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "if \"BioBERT_df_filtered\" not in globals():\n",
    "    print(\"Loading dataset from file...\")\n",
    "    BioBERT_df_filtered = pd.read_csv(\"sliding_window_gene_binary_matrix_BioBERT_MyGeneinfo_hits_only12532_mCRPC.csv\")\n",
    "BioBERT_df_filtered_cleanedv1 = BioBERT_df_filtered.copy()\n",
    "print(f\"Length of copy:      {len(BioBERT_df_filtered_cleanedv1):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7900d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "tqdm.pandas()\n",
    "\n",
    "### Clean PaperTitle ###\n",
    "title_patterns = [\n",
    "    r'^\\d{2,3}:\\s*', \n",
    "    r'^\\d{4}:\\s*',\n",
    "    r'^\\d{5}:\\s*', \n",
    "    r'^#\\d{3,4}\\s*', \n",
    "    r'^<PHONE>:\\s*',  \n",
    "]\n",
    "title_pattern = re.compile(\"|\".join(title_patterns))\n",
    "\n",
    "print(\" Cleaning PaperTitle column...\")\n",
    "BioBERT_df_filtered_cleanedv1[\"PaperTitle\"] = BioBERT_df_filtered_cleanedv1[\"PaperTitle\"].astype(str).progress_apply(\n",
    "    lambda x: re.sub(title_pattern, '', x)\n",
    ")\n",
    "\n",
    "### Clean Abstract ###\n",
    "abstract_patterns = [\n",
    "    r'^\\d{1,5}\\^?\\s+(?=Background)', \n",
    "    r'^\\d{1,5}\\s+(?=Objectives[:]?|Abstract[:]?)'\n",
    "]\n",
    "abstract_pattern = re.compile(\"|\".join(abstract_patterns), re.IGNORECASE)\n",
    "\n",
    "print(\"Cleaning Abstract column...\")\n",
    "BioBERT_df_filtered_cleanedv1[\"Abstract\"] = BioBERT_df_filtered_cleanedv1[\"Abstract\"].astype(str).progress_apply(\n",
    "    lambda x: re.sub(abstract_pattern, '', x)\n",
    ")\n",
    "\n",
    "\n",
    "cleaned_file_path = \"cleaned_BioBERT_data_mCRPC.csv\"\n",
    "BioBERT_df_filtered_cleanedv1.to_csv(cleaned_file_path, index=False)\n",
    "end_time = time.time()\n",
    "execution_time = round(end_time - start_time, 2)\n",
    "print(f\"\\n Cleaning complete! Dataset saved as '{cleaned_file_path}'.\")\n",
    "print(f\" Total execution time: {execution_time} seconds.\")\n",
    "print(BioBERT_df_filtered_cleanedv1.head())\n",
    "print(f\"Length of full test dataset: {len(BioBERT_df_filtered_cleanedv1):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadba643",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d0f658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset length\n",
    "len_gene_df = len(BioBERT_df_filtered_cleanedv1)\n",
    "print(f\"Length of full test dataset: {len_gene_df:,}\")\n",
    "with open(\"len_gene_df_mCRPC.txt\", \"w\") as file:\n",
    "    file.write(str(len_gene_df))\n",
    "\n",
    "# Calculate values\n",
    "articles_before = len(BioBERT_df)\n",
    "articles_without_genes = articles_wogenementionsslw\n",
    "articles_with_genes = len(BioBERT_df_filtered)\n",
    "percentage_with_genes = (articles_with_genes / articles_before) * 100\n",
    "\n",
    "# Save output to a text file in the current directory\n",
    "with open(\"gene_NER_article_statistics_mCRPC.txt\", \"w\") as file:\n",
    "    file.write(f\"Articles before gene filtering: {articles_before:,}\\n\")\n",
    "    file.write(f\"Articles without gene mentions: {articles_without_genes:,}\\n\")\n",
    "    file.write(f\"Articles with gene mentions:      {articles_with_genes:,}\\n\")\n",
    "    file.write(f\"Percentage of relevant articles with gene mentions: {percentage_with_genes:.2f}%\\n\")\n",
    "\n",
    "print(f\"\\nArticles before gene filtering: {articles_before:,}\")\n",
    "print(f\"Articles without gene mentions: {articles_without_genes:,}\")\n",
    "print(f\"Articles with gene mentions:      {articles_with_genes:,}\")\n",
    "print(f\"Percentage of relevant articles with gene mentions: {percentage_with_genes:.2f}%\")\n",
    "print(\"File saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
