{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bbb9596",
   "metadata": {},
   "source": [
    "# **Cleaning of dataset from OpenAlex**\n",
    "- Remove duplicates\n",
    "- Remove articles with missing metadata, such as 'PaperTitle', 'PubDate' or 'Abstract'\n",
    "- Remove non english articles, even though marked as 'en' in the metadata\n",
    "- Remove artifacts, i.e., all supplementary tables, figures and data\n",
    "- Remove invalid titles, placeholders, length-based issues\n",
    "- Remove withdrawn papers\n",
    "- Remove'non-research' articles\n",
    "- Clean, harmonize and normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64beb0ca",
   "metadata": {},
   "source": [
    "## 1) Import libraries and load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a82ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from langdetect import detect, DetectorFactory\n",
    "from tqdm import tqdm\n",
    "from rapidfuzz import fuzz\n",
    "from cleantext import clean\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddea7fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the working directories\n",
    "input_directory = \"INPUT_DIRECTORY\"\n",
    "output_directory = \"OUTPUT_DIRECTORY\"\n",
    "print(\"Current Working Directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5152f377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataframe\n",
    "file_path = \"OPEN_ALEX_DATAFILE.csv\"\n",
    "\n",
    "# Check if OpenAlexSearch exists in the global scope\n",
    "if 'OpenAlexSearch' in globals():\n",
    "    OpenAlexSearch = OpenAlexSearch.copy()\n",
    "    print(\"OpenAlexSearch copied from existing DataFrame!\")\n",
    "else:\n",
    "    os.chdir(input_directory)\n",
    "    OpenAlexSearch = pd.read_csv(file_path)\n",
    "\n",
    "# Get the number of articles\n",
    "OpenAlex_article_count = len(OpenAlexSearch)\n",
    "print(\"Number of articles derived from OpenAlex:\", f\"{OpenAlex_article_count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48083ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check column names\n",
    "print(\"Column names of the dataset:\", OpenAlexSearch.columns)\n",
    "\n",
    "# Rename column 'c' to 'CoFoS'\n",
    "OpenAlexSearch = OpenAlexSearch.rename(columns={'c': 'CoFoS'})\n",
    "print(\"Column names of the dataset:\",OpenAlexSearch.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a4ee44",
   "metadata": {},
   "source": [
    "## 2) Removal of duplicates and empty PaperTitle/Abstract/PubDate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af51da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the original dataset before cleaning\n",
    "OpenAlex_rem_dup_metad = OpenAlexSearch.copy()\n",
    "\n",
    "# Define cleaning steps and initialize tqdm progress bar\n",
    "cleaning_steps = [\n",
    "    \"Removing missing/empty PaperId\",\n",
    "    \"Removing duplicate PaperId entries\",\n",
    "    \"Removing duplicate entries with identical PaperTitle & Authors\",\n",
    "    \"Removing missing/empty PaperTitle\",\n",
    "    \"Removing missing/empty Abstract\",\n",
    "    \"Removing missing/empty PubDate\",\n",
    "    \"Validating PubDate format (YYYY-MM-DD)\",\n",
    "    \"Removing missing/empty Authors\"\n",
    "]\n",
    "progress_bar = tqdm(total=len(cleaning_steps), desc=\"Cleaning Data\", unit=\"step\")\n",
    "\n",
    "# Count initial number of rows\n",
    "initial_article_count = len(OpenAlexSearch)\n",
    "\n",
    "# Removing missing/empty PaperId\n",
    "missing_paperid_count = OpenAlex_rem_dup_metad['PaperId'].isna().sum()\n",
    "empty_paperid_count = (OpenAlex_rem_dup_metad['PaperId'].astype(str).str.strip() == \"\").sum()\n",
    "OpenAlex_rem_dup_metad = OpenAlex_rem_dup_metad.dropna(subset=['PaperId'])\n",
    "OpenAlex_rem_dup_metad = OpenAlex_rem_dup_metad[OpenAlex_rem_dup_metad['PaperId'].astype(str).str.strip() != \"\"]\n",
    "progress_bar.update(1)\n",
    "\n",
    "# Removing missing/empty PaperTitle\n",
    "missing_title_count = OpenAlex_rem_dup_metad['PaperTitle'].isna().sum()\n",
    "empty_title_count = (OpenAlex_rem_dup_metad['PaperTitle'].str.strip() == \"\").sum()\n",
    "OpenAlex_rem_dup_metad = OpenAlex_rem_dup_metad.dropna(subset=['PaperTitle'])\n",
    "OpenAlex_rem_dup_metad = OpenAlex_rem_dup_metad[OpenAlex_rem_dup_metad['PaperTitle'].str.strip() != \"\"]\n",
    "progress_bar.update(1)\n",
    "\n",
    "# Removing missing/empty Abstract\n",
    "missing_abstract_count = OpenAlex_rem_dup_metad['Abstract'].isna().sum()\n",
    "empty_abstract_count = (OpenAlex_rem_dup_metad['Abstract'].str.strip() == \"\").sum()\n",
    "OpenAlex_rem_dup_metad = OpenAlex_rem_dup_metad.dropna(subset=['Abstract'])\n",
    "OpenAlex_rem_dup_metad = OpenAlex_rem_dup_metad[OpenAlex_rem_dup_metad['Abstract'].str.strip() != \"\"]\n",
    "progress_bar.update(1)\n",
    "\n",
    "# Removing missing/empty Authors\n",
    "missing_authors_count = OpenAlex_rem_dup_metad['Authors'].isna().sum()\n",
    "empty_authors_count = (OpenAlex_rem_dup_metad['Authors'].astype(str).str.strip() == \"\").sum()\n",
    "OpenAlex_rem_dup_metad = OpenAlex_rem_dup_metad.dropna(subset=['Authors'])\n",
    "OpenAlex_rem_dup_metad = OpenAlex_rem_dup_metad[OpenAlex_rem_dup_metad['Authors'].astype(str).str.strip() != \"\"]\n",
    "progress_bar.update(1)\n",
    "\n",
    "# Removing missing/empty PubDate\n",
    "missing_pubdate_count = OpenAlex_rem_dup_metad['PubDate'].isna().sum()\n",
    "empty_pubdate_count = (OpenAlex_rem_dup_metad['PubDate'].astype(str).str.strip() == \"\").sum()\n",
    "OpenAlex_rem_dup_metad = OpenAlex_rem_dup_metad.dropna(subset=['PubDate'])\n",
    "OpenAlex_rem_dup_metad = OpenAlex_rem_dup_metad[OpenAlex_rem_dup_metad['PubDate'].astype(str).str.strip() != \"\"]\n",
    "progress_bar.update(1)\n",
    "\n",
    "# Validate PubDate format (YYYY-MM-DD)\n",
    "def is_valid_date(date_str):\n",
    "    return bool(re.match(r'^\\d{4}-\\d{2}-\\d{2}$', str(date_str)))\n",
    "invalid_pubdate_count = OpenAlex_rem_dup_metad[~OpenAlex_rem_dup_metad['PubDate'].astype(str).apply(is_valid_date)].shape[0]\n",
    "OpenAlex_rem_dup_metad = OpenAlex_rem_dup_metad[OpenAlex_rem_dup_metad['PubDate'].astype(str).apply(is_valid_date)]\n",
    "progress_bar.update(1)\n",
    "\n",
    "# Removing duplicate PaperId entries\n",
    "duplicate_paperid_count = OpenAlex_rem_dup_metad.duplicated(subset=['PaperId']).sum()\n",
    "OpenAlex_rem_dup_metad = OpenAlex_rem_dup_metad.drop_duplicates(subset=['PaperId'], keep='first')\n",
    "progress_bar.update(1)\n",
    "\n",
    "# Removing duplicate entries with identical PaperTitle & Authors\n",
    "OpenAlex_rem_dup_metad[\"_temp_PaperTitle\"] = OpenAlex_rem_dup_metad[\"PaperTitle\"].str.lower()\n",
    "OpenAlex_rem_dup_metad[\"_temp_Authors\"] = OpenAlex_rem_dup_metad[\"Authors\"].str.lower()\n",
    "duplicate_entries_count = OpenAlex_rem_dup_metad.duplicated(subset=[\"_temp_PaperTitle\", \"_temp_Authors\"]).sum()\n",
    "OpenAlex_rem_dup_metad = OpenAlex_rem_dup_metad.drop_duplicates(subset=[\"_temp_PaperTitle\", \"_temp_Authors\"], keep=\"first\")\n",
    "OpenAlex_rem_dup_metad.drop(columns=[\"_temp_PaperTitle\", \"_temp_Authors\"], inplace=True)\n",
    "progress_bar.update(1)           \n",
    "\n",
    "# Close the progress bar\n",
    "progress_bar.close()\n",
    "\n",
    "# Count final number of rows after all removals\n",
    "article_count_after_duplicates_and_missings = len(OpenAlex_rem_dup_metad)\n",
    "total_removed = initial_article_count - article_count_after_duplicates_and_missings\n",
    "\n",
    "# Sum the duplicate removals\n",
    "total_duplicates_removed = duplicate_paperid_count + duplicate_entries_count\n",
    "\n",
    "# Sum the missing removals\n",
    "missing_values = (\n",
    "    missing_paperid_count + empty_paperid_count +\n",
    "    missing_title_count + empty_title_count +\n",
    "    missing_abstract_count + empty_abstract_count +\n",
    "    missing_pubdate_count + empty_pubdate_count + invalid_pubdate_count +\n",
    "    missing_authors_count + empty_authors_count\n",
    ")\n",
    "\n",
    "# Collect removal statistics\n",
    "removal_log = [\n",
    "    f\"Initial number of articles: {initial_article_count:,}\",\n",
    "    f\"\\n-Removed due to missing PaperId (NaN): {missing_paperid_count:,}\",\n",
    "    f\"-Removed due to empty PaperId (''): {empty_paperid_count:,}\", \n",
    "    f\"-Removed due to missing PaperTitle (NaN): {missing_title_count:,}\",\n",
    "    f\"-Removed due to empty PaperTitle (''): {empty_title_count:,}\",\n",
    "    f\"-Removed due to missing Abstract (NaN): {missing_abstract_count:,}\",\n",
    "    f\"-Removed due to empty Abstract (''): {empty_abstract_count:,}\",\n",
    "    f\"-Removed due to missing PubDate (NaN): {missing_pubdate_count:,}\",\n",
    "    f\"-Removed due to empty PubDate (''): {empty_pubdate_count:,}\",\n",
    "    f\"-Removed due to invalid PubDate format (not YYYY-MM-DD): {invalid_pubdate_count:,}\",\n",
    "    f\"-Removed due to missing Authors (NaN): {missing_authors_count:,}\",\n",
    "    f\"-Removed due to empty Authors (''): {empty_authors_count:,}\",\n",
    "    f\"-Removed due to duplicate PaperId entries: {duplicate_paperid_count:,}\",\n",
    "    f\"-Removed due to duplicate entries with identical PaperTitle & Authors: {duplicate_entries_count:,}\", \n",
    "    \n",
    "    f\"\\nTotal articles with missing data removed: {missing_values:,}\", \n",
    "    f\"Total duplicate entries removed: {total_duplicates_removed:,}\", \n",
    "    f\"Total articles removed: {total_removed:,}\",\n",
    "    \n",
    "    f\"\\nFinal number of articles after cleaning steps: {article_count_after_duplicates_and_missings:,}\"\n",
    "]\n",
    "\n",
    "# Print removal details\n",
    "print(\"\\n\".join(removal_log))\n",
    "\n",
    "# Save the DataFrame as a CSV file\n",
    "os.chdir(output_directory)\n",
    "output_file_path = os.path.join(output_directory, \"OpenAlex_rem_dup_metad.csv\")\n",
    "OpenAlex_rem_dup_metad.to_csv(output_file_path, index=False)\n",
    "print(f\"Saved successfully as: {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0d523c",
   "metadata": {},
   "source": [
    "## 3) Removal of articles in languages other than \"en\", even though set to \"en\" in metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32df521e",
   "metadata": {},
   "source": [
    "- This step is using an NER-based approach, so in this case (2.7 M articles) it is takinng about 2h."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f128cd66",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load dataframe\n",
    "file_path = os.path.join(output_directory, \"OpenAlex_rem_dup_metad.csv\")\n",
    "\n",
    "# Check if OpenAlex_rem_dup_metad exists\n",
    "if 'OpenAlex_rem_dup_metad' in globals():\n",
    "    df_filtered = OpenAlex_rem_dup_metad.copy()\n",
    "    print(\"df_filtered copied from existing OpenAlex_rem_dup_metad DataFrame!\")\n",
    "else:\n",
    "    os.chdir(output_directory)\n",
    "    df_filtered = pd.read_csv(file_path)\n",
    "    print(\"df_filtered loaded from OpenAlex_rem_dup_metad.csv in output directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e393bb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Progress bar\n",
    "tqdm.pandas()\n",
    "\n",
    "# Ensure consistent language detection (fixes randomness)\n",
    "DetectorFactory.seed = 0  \n",
    "\n",
    "# Start tracking runtime\n",
    "start_time = time.time()\n",
    "\n",
    "# Function to detect language separately for PaperTitle and Abstract with retry logic\n",
    "def detect_language(row):\n",
    "    try:\n",
    "        title = str(row['PaperTitle']).strip().lower() if pd.notna(row['PaperTitle']) else \"\"\n",
    "        abstract = str(row['Abstract']).strip() if pd.notna(row['Abstract']) else \"\"\n",
    "\n",
    "        # Apply language detection with retry mechanism\n",
    "        def safe_detect(text, max_retries=3):\n",
    "            for _ in range(max_retries):\n",
    "                try:\n",
    "                    return detect(text) if len(text) > 10 else \"unknown\" \n",
    "                except:\n",
    "                    pass\n",
    "            return \"error\"\n",
    "\n",
    "        title_lang = safe_detect(title)\n",
    "        abstract_lang = safe_detect(abstract)\n",
    "        return \"en\" if title_lang == \"en\" or abstract_lang == \"en\" else \"non-en\"\n",
    "\n",
    "    except Exception:\n",
    "        return \"error\"\n",
    "\n",
    "# Apply language detection with progress bar on the full dataset\n",
    "df_filtered[\"detected_language\"] = df_filtered.progress_apply(detect_language, axis=1)\n",
    "\n",
    "# Separate English and non-English articles (Keep if AT LEAST ONE column is English)\n",
    "df_filtered_cleaned = df_filtered[df_filtered[\"detected_language\"] == \"en\"].drop(columns=[\"detected_language\"])\n",
    "df_dropped = df_filtered[df_filtered[\"detected_language\"] != \"en\"]\n",
    "\n",
    "# Store lengths in variables\n",
    "total_rows_before = len(df_filtered)\n",
    "dropped_rows = len(df_dropped)\n",
    "total_rows_after = len(df_filtered_cleaned)\n",
    "\n",
    "# Stop tracking runtime\n",
    "end_time = time.time()\n",
    "runtime_seconds = end_time - start_time\n",
    "runtime_minutes = runtime_seconds / 60\n",
    "\n",
    "# Save results\n",
    "os.chdir(output_directory)\n",
    "df_dropped.to_csv(os.path.join(output_directory, \"OpenAlexSearch_non_english.csv\"), index=False)\n",
    "df_filtered_cleaned.to_csv(os.path.join(output_directory, \"OpenAlexSearch_language_filtered.csv\"), index=False)\n",
    "\n",
    "# Save runtime and stats to a text file\n",
    "summary_log_path = os.path.join(output_directory, \"language_filter_summary.txt\")\n",
    "with open(summary_log_path, \"w\") as f:\n",
    "    f.write(f\"Language filtering runtime: {runtime_seconds:.2f} seconds ({runtime_minutes:.2f} minutes)\\n\")\n",
    "    f.write(f\"Total rows before filtering: {total_rows_before:,}\\n\")\n",
    "    f.write(f\"Non-English articles removed (dropped_rows): {dropped_rows:,}\\n\")\n",
    "    f.write(f\"Total rows after filtering: {total_rows_after:,}\\n\")\n",
    "print(f\"Language filtering complete. Non-English articles removed: {dropped_rows:,}.\")\n",
    "print(f\"Total time: {runtime_seconds:.2f} seconds ({runtime_minutes:.2f} minutes).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd93363b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print summary\n",
    "print(f\"Language filtering completed in {runtime_seconds:,.2f} seconds\")\n",
    "print(f\"Total rows before language filtering: {total_rows_before:,}\")\n",
    "print(f\"Non-English articles removed: {dropped_rows:,}\")\n",
    "\n",
    "print(f\"Total rows after language filtering: {total_rows_after:,}\")\n",
    "print(f\"Total rows after language filtering: {len(df_filtered_cleaned):,}\")\n",
    "\n",
    "# Investigate dropped rows\n",
    "print(\"\\nDropped rows:\")\n",
    "print(df_dropped[['PaperTitle']].head(20))\n",
    "print(df_dropped[['Abstract']].head(20))\n",
    "print(f\"\\nTotal non-English articles removed: {len(df_dropped):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9cb959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count non-english output\n",
    "file_path = \"OpenAlexSearch_language_filtered.csv\"\n",
    "# Check if df_filtered exists in the global scope\n",
    "if 'df_filtered_cleaned' in globals():\n",
    "    clean_df_step1 = df_filtered_cleaned.copy()\n",
    "    print(\"df_filtered copied from existing DataFrame!\")\n",
    "else:\n",
    "    os.chdir(output_directory)\n",
    "    clean_df_step1 = pd.read_csv(file_path)\n",
    "    print(\"File loaded successfully!\")\n",
    "    \n",
    "\n",
    "# Count non-English articles\n",
    "nonenglish_articles = len(df_dropped)\n",
    "print(f\"Number of non-English articles: {nonenglish_articles:,}\")\n",
    "print(f\"Number of English articles from old dataframe: {len(df_filtered_cleaned):,}\")\n",
    "print(f\"Number of English articles from new dataframe: {len(clean_df_step1):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36a20ca",
   "metadata": {},
   "source": [
    "## 4) Remove artifacts, i.e., all supplementary tables, figures and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41beaebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataframe\n",
    "file_path = \"OpenAlexSearch_language_filtered.csv\"\n",
    "# Check if df_filtered exists in the global scope\n",
    "if 'df_filtered_cleaned' in globals():\n",
    "    clean_df_step1 = df_filtered_cleaned.copy()\n",
    "    print(\"df_filtered copied from existing DataFrame!\")\n",
    "else:\n",
    "    os.chdir(output_directory)\n",
    "    clean_df_step1 = pd.read_csv(file_path)\n",
    "    print(f\"File loaded successfully! Rows: {len(clean_df_step1)}\")\n",
    "print(f\"Len of dataset: {len(clean_df_step1):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867c4fd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Start tracking runtime\n",
    "tqdm.pandas()\n",
    "start_time = time.time()\n",
    "\n",
    "# Keywords to match at the start of the PaperTitle column (sorted, formatted)\n",
    "all_keywords = sorted([\n",
    "    \"Acknowledgments\", \"ADOBE PDF\", \"Addendum\", \"Advertisement\", \"Additional figure\", \"Additional Material\", \n",
    "    \"Additional Table\", \"All Supplemental Data\", \"All Supplemental Figures\", \n",
    "    \"All Supplemental Figures, Tables and Legends\", \"All Supplementary Data\", \n",
    "    \"All Supplementary Figures\", \"All Supplementary Figures and Tables\", \n",
    "    \"All Supplementary Tables, Figures and Methods\", \"Analysis Methods Supplement\", \n",
    "    \"Appendix\", \"Article figures\", \"Author Correction\", \"Auxiliary Supplementary File\", \n",
    "    \"Caption for Suppl Fig.\", \"Captions of supplementary\", \"Combined supplemental figures\", \n",
    "    \"Contents Vol\", \"Contents:\",\n",
    "    \"Corrigendum\", \"Correction\", \"Correction to\", \"Correction:\", \"Corrections to\", \n",
    "    \"Data Augmentation\", \"Data associated with\", \"Data and code\", \"Data and analysis scripts\", \"Data File\", \n",
    "    \"Data for\", \"Data from\", \"Data not shown\", \"Data and metadata\",\n",
    "    \"Data on\", \"Data S\", \"Data Spreadsheet\", \"Data Supplement\", \"Data.zip\", \n",
    "    \"Dataset\", \"Dataset and metadata\", \"Dataset for\", \"Dataset related to\", \n",
    "    \"Description of Supplemental Figures\", \"Description of supplementary\", \n",
    "    \"Download Supplementary\", \"eFigure\", \"Erratum\", \"Extended Data\", \"Expression of Concern\",\n",
    "    \"Fig\", \"Fig.\", \"Fig Supp\", \"FigS\", \"Figure\", \"Figure S\", \"Figures\", \"Instructions for Authors\",\n",
    "    \"File\", \"Funding statement\", \"http://\", \"https://\", \n",
    "    \"Legend for\", \"Legend from\", \"Legend of\", \"Legend to\", \n",
    "    \"Legends for\", \"Legends from\", \"Legends of\", \"Legends to\", \n",
    "    \"Legends Supplemental Figures\", \"Link to Supplementary\", \n",
    "    \"List of figures\", \"List of tables\", \"List of plates\", \"Manuscript Figures\", \n",
    "    \"Mat & Met\", \"Material and Methods\", \"Materials and Methods\", \n",
    "    \"Merged Supplementary\", \"Metadata and data\", \"Metadata record for\", \n",
    "    \"Metadata supporting\", \"Methods References\", \"Methods and Materials from\", \n",
    "    \"Methods and Supplementary\", \"Methods and Tables\", \"Methods file\", \n",
    "    \"Methods from\", \"Methods, Figures\", \"Methods, Table\", \"MET supplementary methods\", \n",
    "    \"methods_\", \"Movie\", \"Multimedia\", \"Methodology and Method\", \"Online supplement\", \n",
    "    \"Online Supplementary Materials\", \"Online Supplementary Tables\", \n",
    "    \"Online-only supplementary\", \"Online-Only Tables\", \n",
    "    \"Original Western Blot Data\", \"Revised Supplementary\", \"S Figure\", \"S Table\", \n",
    "    \"S-Figure\", \"S1 Legend\", \"S1 from\", \"S1.\", \"S1:\", \"S2 from\", \"S2.\", \"S2:\", \n",
    "    \"S3 from\", \"S3.\", \"S3:\", \"S4 from\", \"S4.\", \"S4:\", \"S5 from\", \"S5.\", \"S5:\", \n",
    "    \"S6 from\", \"S6.\", \"S6:\", \"S7 from\", \"S7.\", \"S7:\", \"S8 from\", \"S8.\", \"S8:\", \n",
    "    \"S9 from\", \"S9.\", \"S9:\", \"SI Figure\", \"SI Methods\", \"SI Table\", \n",
    "    \"SI materials\", \"SI tables and figures\", \"SFigure\", \"SF-\", \"SF1\", \n",
    "    \"Sl Figure\", \"Supplement\", \"Supplement Material\", \"Supplement Table\", \n",
    "    \"Supplemental\", \"Supplemental Data\", \"Supplemental Figure\", \n",
    "    \"Supplemental Figures\", \"Supplemental Table\", \"Supplementary\", \n",
    "    \"Supplementary Data\", \"Supplementary Figure\", \"Supplementary Figures\", \n",
    "    \"Supplementary Legends\", \"Supplementary Methods\", \"Supplementary Movie\", \n",
    "    \"Supplementary Table\", \"Supplementary Tables\", \"Supplementary Text\", \n",
    "    \"Supplementary Video\", \"SupplementaryFigures\", \"Supplementary_Figure\", \n",
    "    \"Supplementray Figures\", \"Supplementray Tables\", \"Supplymentary Figure\", \n",
    "    \"Supplymentary Figures\", \"Supplymentary Table\", \"Supporting Data\", \n",
    "    \"Supporting Document\", \"Supporting Figure\", \"Supporting Figures\", \n",
    "    \"Supporting File\", \"Supporting Information\", \"Supporting Legend\", \n",
    "    \"Supporting Legends\", \"Supporting Materials\", \"Supporting Methods\", \n",
    "    \"Supporting Table\", \"Supporting Tables\", \"Supporting Text\", \n",
    "    \"Supporting Video\", \"Suppl\", \"Suppl Figure\", \"Suppl Figures\", \"Suppl File\", \n",
    "    \"Suppl Info\", \"Suppl Information\", \"Suppl Legends\", \"Suppl List of Videos, Tables and Methods\", \n",
    "    \"Suppl Materials and Methods\", \"Suppl Methods\", \"Suppl Movie\", \"Suppl Table\", \n",
    "    \"Suppl Video\", \"Supplimentary Figure\", \"Supplimentary Methods\", \n",
    "    \"Supplimentary Tables\", \"Supplimentary_Figure\", \"Suplementary\", \"Supllementary Figure\", \n",
    "    \"Suplplementary Figure Legend\", \"Suplpementary Table\", \"Supplrmentary Figures\", \n",
    "    \"Supplrmentary Tables\", \"Suppltable\", \"Suppltext\", \"Supplvideo\", \n",
    "    \"Suppl_Figure\", \"Suppl-Figure\", \"Suupplementary Tables\", \"Table\", \"Table of Content\",\n",
    "    \"Validation Figures\", \"Video\", \"Western Blots from\", \"Whole Slide Image\"\n",
    "])\n",
    "\n",
    "# Keywords to match anywhere in the PaperTitle column (sorted, formatted)\n",
    "middle_keywords = sorted([\n",
    "    \"Combined Supplementary Materials\", \"Corrigendum\", \"The Case Files\", \"Additional file\",\n",
    "    \"Figure S\", \"Figures S\", \"Manuscript Figures\", \"SI tables and figures\", \n",
    "    \"Supplemental Data\", \"Supplemental figures\", \"Supplemental from\", \n",
    "    \"Supplemental table\", \"Supplemental figure\", \"Supplementary Figure\", \n",
    "    \"Supplementary Figures\", \"Supplementary Material\", \"Supplementary Methods\", \n",
    "    \"Supplementary Table\", \"Supplementary Tables\", \"Supplementary data\", \n",
    "    \"Supplementary document\", \"Supplementary information\", \"SupplementaryFigures\", \n",
    "    \"Supplementary materials and methods\", \n",
    "    \"Translation for the Article\", \"Video Dataset\", \"Operative Video\"\n",
    "])\n",
    "\n",
    "\n",
    "# Compile regex patterns for faster execution\n",
    "start_pattern = re.compile(r\"^(?:\" + \"|\".join(map(re.escape, all_keywords)) + r\")\", re.IGNORECASE)\n",
    "middle_pattern = re.compile(r\"\\b(?:{})\\b\".format(\"|\".join(map(re.escape, middle_keywords))), re.IGNORECASE)\n",
    "supplementary_pattern = re.compile(r\"[\\w\\-_]*supple\\w*\", re.IGNORECASE)\n",
    "\n",
    "# Count initial rows\n",
    "rows_before_artifacts = len(clean_df_step1)\n",
    "\n",
    "# Apply filtering with a progress bar\n",
    "tqdm.pandas(desc=\"Filtering Data\")\n",
    "\n",
    "clean_df_step1 = clean_df_step1[\n",
    "    ~clean_df_step1[\"PaperTitle\"].progress_apply(lambda title: bool(start_pattern.match(str(title)))) &\n",
    "    ~clean_df_step1[\"PaperTitle\"].progress_apply(lambda title: bool(middle_pattern.search(str(title)))) &\n",
    "    ~clean_df_step1[\"PaperTitle\"].progress_apply(lambda title: \"_\" in str(title)) &\n",
    "    ~clean_df_step1[\"PaperTitle\"].progress_apply(lambda title: bool(supplementary_pattern.search(str(title))))\n",
    "]\n",
    "\n",
    "# Count remaining and removed rows\n",
    "final_row_count = len(clean_df_step1)\n",
    "artifacts_removed = rows_before_artifacts - final_row_count\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nInitial number of rows before artifact removal: {rows_before_artifacts:,}\")\n",
    "print(f\"Number of rows identified as artifacts and removed: {artifacts_removed:,}\")\n",
    "print(f\"Final number of rows in the cleaned dataset: {final_row_count:,}\")\n",
    "clean_df_step1.to_csv(\"filtered_clean_after_artifactremoval.csv\", index=False)\n",
    "\n",
    "# End tracking runtime\n",
    "end_time = time.time()\n",
    "print(f\"Processing completed in {end_time - start_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a446bea",
   "metadata": {},
   "source": [
    "## 5) Removing invalid titles, placeholders, length-based issues,  withdrawn papers, and 'non-research' articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e07bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataframe\n",
    "file_path = \"filtered_clean_after_artifactremoval.csv\"\n",
    "\n",
    "# Check if cleaned_df exists in the global scope\n",
    "if 'clean_df_step1' in globals():\n",
    "    clean_df_step2 = clean_df_step1.copy()\n",
    "    print(\"cleaned_df copied from existing DataFrame!\")\n",
    "else:\n",
    "    os.chdir(output_directory)\n",
    "    clean_df_step1 = pd.read_csv(file_path)\n",
    "    clean_df_step2 = clean_df_step1.copy()\n",
    "\n",
    "    # Store initial row count before cleaning\n",
    "    article_count_before_invalid = len(cleaned_df)\n",
    "    print(f\"File loaded successfully! Row count: {article_count_before_invalid}\")\n",
    "    \n",
    "print(f\"Len clean_df_step1: {len(clean_df_step1):,}\")\n",
    "print(f\"Check len clean_df_step2: {len(clean_df_step2):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f90753d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store initial article count\n",
    "article_count_before_invalid = len(clean_df_step1)\n",
    "\n",
    "# ================================================\n",
    "\n",
    "# Initialize removal counts\n",
    "nonsense_title_counts = 0\n",
    "title_length_counts = 0\n",
    "abstract_length_counts = 0\n",
    "withdraw_counts = 0\n",
    "non_research_counts = 0\n",
    "\n",
    "# Define cleaning steps for progress bar\n",
    "new_cleaning_steps = [\n",
    "    \"Removing numeric/nonsense titles\",\n",
    "    \"Removing placeholder titles\",\n",
    "    \"Filtering by title length\",\n",
    "    \"Filtering by abstract length\",\n",
    "    \"Removing withdrawn/withdrawal articles\",\n",
    "    \"Removing non-research articles (Editorials, News, etc.)\"\n",
    "]\n",
    "progress_bar = tqdm(total=len(new_cleaning_steps), desc=\"Additional Cleaning\", unit=\"step\")\n",
    "\n",
    "# ========== Removing numeric/nonsense titles ==========\n",
    "pattern_numeric = re.compile(r'^[^A-Za-z]+$')  # Matches titles with no letters (only numbers/symbols)\n",
    "mask_numeric = clean_df_step2[\"PaperTitle\"].apply(lambda title: bool(pattern_numeric.match(str(title))))\n",
    "nonsense_title_counts += mask_numeric.sum()\n",
    "clean_df_step2 = clean_df_step2[~mask_numeric]\n",
    "progress_bar.update(1)\n",
    "print(f\"Identified numeric/nonsense titles: {mask_numeric.sum():,}\")\n",
    "\n",
    "# ========== Removing placeholder titles ==========\n",
    "placeholders = {\n",
    "    \"Title Not Available\", \"No Title\", \"Untitled\", \"N/A\",\n",
    "    \"[No title available]\", \"No title available\", \"No title available - PubMed\",\n",
    "    \"Error in Text\", \"Errors in Box\", \"Error in Author Name\", \"Error in Author Names\",\n",
    "    \"Error in Author Surnames\", \"Errors in article text\", \"Error in Figure Label and Caption\",\n",
    "    \"Error in Presentation of Author\", \"Errors in Author Name\", \"Error in Author Name\",\n",
    "    \"Error in Table Text\",\"Error in Corresponding Authorship\",\n",
    "}\n",
    "\n",
    "placeholder_pattern = re.compile(\n",
    "    r\"\\b(?:{})\\b\".format(\"|\".join(map(re.escape, placeholders))), re.IGNORECASE\n",
    ")\n",
    "mask_placeholders = clean_df_step2[\"PaperTitle\"].apply(lambda title: bool(placeholder_pattern.search(str(title))))\n",
    "print(f\"Identified placeholders: {mask_placeholders.sum():,}\")\n",
    "nonsense_title_counts += mask_placeholders.sum()\n",
    "clean_df_step2 = clean_df_step2[~mask_placeholders]\n",
    "progress_bar.update(1)\n",
    "\n",
    "# ========== Filtering by title length ==========\n",
    "def count_real_words(text):\n",
    "    words = re.findall(r'\\b[a-zA-Z]{2,}\\b', str(text))\n",
    "    return len(words)\n",
    "\n",
    "def valid_title_length(title, min_words=3, max_words=60):\n",
    "    return min_words <= count_real_words(title) <= max_words\n",
    "\n",
    "clean_df_step2 = clean_df_step2.reset_index(drop=True)\n",
    "mask_valid_title = clean_df_step2[\"PaperTitle\"].apply(valid_title_length)\n",
    "\n",
    "title_length_counts = (~mask_valid_title).sum()\n",
    "print(f\"Invalid PaperTitle count (too short/long): {title_length_counts:,}\")\n",
    "\n",
    "# Remove invalid titles first\n",
    "clean_df_step2 = clean_df_step2[mask_valid_title]\n",
    "progress_bar.update(1)\n",
    "\n",
    "# ========== Filtering by abstract length ==========\n",
    "def valid_abstract_length(abstract, min_words=80, max_words=1000):\n",
    "    return min_words <= count_real_words(abstract) <= max_words\n",
    "\n",
    "mask_valid_abstract = clean_df_step2[\"Abstract\"].apply(valid_abstract_length)\n",
    "abstract_length_counts = (~mask_valid_abstract).sum()\n",
    "\n",
    "print(f\"Invalid Abstract count (too short/long): {abstract_length_counts:,}\")\n",
    "\n",
    "# Remove invalid abstracts (after title filtering)\n",
    "clean_df_step2 = clean_df_step2[mask_valid_abstract]\n",
    "progress_bar.update(1)\n",
    "\n",
    "# ========== Removing withdrawn/retracted papers ==========\n",
    "withdrawn_retraction_pattern = re.compile(\n",
    "    r\"(?i)^(?:\\[?)?(Withdrawn|Withdrawal|Retracted|Retraction|Errata|Erratum|Revoked|Removed|Correction Notice|Corrigendum)\\b\"\n",
    ")\n",
    "mask_withdrawn = clean_df_step2[\"PaperTitle\"].apply(lambda title: bool(withdrawn_retraction_pattern.search(str(title))))\n",
    "withdraw_counts = mask_withdrawn.sum()\n",
    "clean_df_step2 = clean_df_step2[~mask_withdrawn]\n",
    "progress_bar.update(1)\n",
    "print(f\"Identified withdrawn: {withdraw_counts:,}\")\n",
    "\n",
    "\n",
    "# ========== Removing non-research articles ==========\n",
    "non_research_keywords = [\n",
    "    \"A reply to Letter to the editor\", \"About the Author\", \"An Update from the Editor-in-Chief\",\n",
    "    \"Announcement:\", \"Announcements\", \"Appeal\", \"ASO Author Reflection\", \"AUTHOR COPY ONLY\", \n",
    "    \"Associate Editor\", \"Author comment:\", \"Author profile\", \"Author Reflections:\", \"Author reply\", \n",
    "    \"Author response\", \"Author response to\", \"Author view\", \"Author's reply\", \"Author's respond\", \n",
    "    \"Editor's Reflection\", \"Editors Reflection\", \"Editors' Reflection\", \"Editor' Reflection\",\n",
    "    \"Author's response\", \"Author's view\", \"Authors reply\", \"Authors respond\", \"Authors response\", \n",
    "    \"Authors view\", \"Authors' Reply\", \"Authors' response\", \"Authors's response\", \"Book Review\", \"EditorinChief\",\n",
    "    \"Comment\", \"Comment on\", \"Commentary\", \"Conference Summary\", \"Conversation with the Editor\", \n",
    "    \"Correspondence\", \"Editorial\", \"Editor Profile\", \"Editor response for\", \"Editor change\", \"New Editor\",\n",
    "    \"Editor-in-Chief\", \"Editorinchief's introduction\", \"Editor's evaluation\", \"Editor's Introduction\", \n",
    "    \"Editor's Message\", \"Editor's note\", \"Editors Introduction\", \"Editors Message\", \"Editors Note on\", \n",
    "    \"Editors Spotlight\", \"Editors change\", \"Editors note\", \"Editors' Introduction\", \"Editors' Message\", \n",
    "    \"Editors' note\", \"Foreword\", \"From the Editor\", \"From the Editor-in-Chief\", \"From the Editors...\", \n",
    "    \"From the Editors Desk...\", \"From the Editor's desk\", \"From the editors desk\", \"From the Guest Editor\", \"Guest editor\",\n",
    "    \"Guest editorial\", \"Introducing our new Editors\", \"Issue Editor Foreword\", \"Issue Information Editorial Board\", \n",
    "    \"Letter to editor\", \"Letter to the Editor\", \"Letter:\", \"Letter Of The Editor\", \"Letters of the editor\", \"Letter-to-the-editor\", \n",
    "    \"Letters to editor\", \"Letters to the editor\", \"Meet the First Author\", \"Meet the author\", \"Meet the editor\", \n",
    "    \"Message from Editor\", \"Message from the Editor\", \"News\", \"Opinion\", \"our author\", \"our editor\", \n",
    "    \"Reply:\", \"In reply:\", \"Reply to:\", \"In reply to:\",\n",
    "    \"Perspective\", \"Preface\", \"Proceedings\", \"Publisher'?s? Note\", \"Reply by Author\", \"Reply by the author\", \n",
    "    \"Reply to Editorial Comment\", \"Reply to Letter to\", \"Reply to correspondence\", \"Reply to the Letter to\", \n",
    "    \"Response to Editorial\", \"Response to a letter\", \"Response to letter\", \"senior editor\", \"Special Section\", \n",
    "    \"The Author Reply\", \"The Authors Reply\", \"To the Editor\", \"Transitioning Between Editor\", \"Viewpoint\", \n",
    "    \"Welcome to our\", \"Executive Editor\",\"Assistant Editor\",\n",
    "]\n",
    "\n",
    "\n",
    "# Convert keywords into a single regex pattern (match anywhere in the title)\n",
    "escaped_keywords = [re.escape(keyword) for keyword in non_research_keywords]\n",
    "non_research_pattern = \"|\".join(escaped_keywords)  # No caret (^) to match anywhere\n",
    "\n",
    "clean_df_step2[\"PaperTitle\"] = clean_df_step2[\"PaperTitle\"].fillna(\"\").str.strip().str.replace(r\"\\s+\", \" \", regex=True)\n",
    "mask_non_research = clean_df_step2[\"PaperTitle\"].str.contains(non_research_pattern, case=False, na=False)\n",
    "non_research_counts = mask_non_research.sum()\n",
    "clean_df_step2 = clean_df_step2[~mask_non_research]\n",
    "progress_bar.update(1)\n",
    "print(f\"Identified non-research articles: {non_research_counts:,}\")\n",
    "\n",
    "progress_bar.close()\n",
    "\n",
    "# ========== Compute final counts ==========\n",
    "# Calculate total removed\n",
    "total_removed = (\n",
    "    nonsense_title_counts\n",
    "    + title_length_counts\n",
    "    + abstract_length_counts\n",
    "    + withdraw_counts\n",
    "    + non_research_counts\n",
    ")\n",
    "final_article_count = len(clean_df_step2)\n",
    "\n",
    "# ========== Final summary & consistency check ==========\n",
    "removal_log = [\n",
    "    f\"Initial count: {article_count_before_invalid:,}\",\n",
    "    f\"Removed numeric/nonsense titles & placeholders: {nonsense_title_counts:,}\",\n",
    "    f\"Removed due to title length issues: {title_length_counts:,}\",\n",
    "    f\"Removed due to abstract length issues: {abstract_length_counts:,}\",\n",
    "    f\"Removed withdrawn articles: {withdraw_counts:,}\",\n",
    "    f\"Removed non-research articles: {non_research_counts:,}\",\n",
    "    f\"Final number of articles: {final_article_count:,}\",\n",
    "    f\"\\nTotal removed (should match sum above): {total_removed:,}\"\n",
    "]\n",
    "\n",
    "print(\"\\n\".join(removal_log))\n",
    "\n",
    "# Sanity check\n",
    "expected_total_removed = (\n",
    "    nonsense_title_counts\n",
    "    + title_length_counts\n",
    "    + abstract_length_counts\n",
    "    + withdraw_counts\n",
    "    + non_research_counts\n",
    ")\n",
    "\n",
    "if total_removed == expected_total_removed:\n",
    "    print(f\"Total removed articles match: {total_removed:,}\")\n",
    "else:\n",
    "    print(f\"Mismatch detected!\")\n",
    "    print(f\"Expected total removed: {expected_total_removed:,}\")\n",
    "    print(f\"Actual total removed: {total_removed:,}\")\n",
    "    print(f\"Difference: {abs(expected_total_removed - total_removed):,}\")\n",
    "\n",
    "# Save final cleaned dataset\n",
    "clean_df_step2.to_csv(\"clean_df_step2.csv\", index=False)\n",
    "print(\"\\nDataset saved as 'clean_df_step2.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d73bc3a",
   "metadata": {},
   "source": [
    "## 6) Cleaning, harmonizing and normalization of PaperTitles and Abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16eef87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataframe\n",
    "file_path = \"clean_df_step2.csv\"\n",
    "if 'clean_df_step2' in globals():\n",
    "    clean_df_step3 = clean_df_step2.copy()\n",
    "    print(\"clean_df_step3 copied from clean_df_step2!\")\n",
    "else:\n",
    "    os.chdir(output_directory)\n",
    "    clean_df_step2 = pd.read_csv(\"clean_df_step2.csv\")\n",
    "    clean_df_step3 = clean_df_step2.copy()\n",
    "    print(\"File loaded successfully!\")\n",
    "    \n",
    "print(f\"Len df2: {len(clean_df_step2):,}\")\n",
    "print(f\"Len df3: {len(clean_df_step3):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7315e7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Store initial row count before cleaning\n",
    "cleaning_step2_count = len(clean_df_step2)\n",
    "\n",
    "# Define cleaning function for \"PaperTitle\"\n",
    "def clean_paper_title(title):\n",
    "    if not isinstance(title, str) or title.strip() == \"\":\n",
    "        return None  # Return None for empty or non-string values.\n",
    "\n",
    "    # Use clean-text to clean the title\n",
    "    title = clean(\n",
    "        title,\n",
    "        fix_unicode=True,  # Fix broken unicode characters\n",
    "        to_ascii=True,     # Transliterate to closest ASCII characters\n",
    "        lower=False,       # Preserve original casing\n",
    "        no_line_breaks=True,  # Remove line breaks\n",
    "        no_urls=True,      # Remove URLs\n",
    "        no_emails=True,    # Remove email addresses\n",
    "        no_phone_numbers=True,  # Remove phone numbers\n",
    "        no_numbers=False,  # Retain numbers\n",
    "        no_digits=False,   # Retain digits\n",
    "        no_currency_symbols=True,  # Remove currency symbols\n",
    "        no_punct=False\n",
    "    )\n",
    "\n",
    "    # Additional custom cleaning specific to titles\n",
    "    title = title.strip()\n",
    "\n",
    "    # Remove titles that are just numbers or codes\n",
    "    if re.fullmatch(r\"^\\d+$\", title):  # Only numbers â†’ Remove\n",
    "        return None\n",
    "\n",
    "    # Convert ALL CAPS to Title Case\n",
    "    if title.isupper():\n",
    "        title = title.title()\n",
    "\n",
    "    # Remove non-informative titles\n",
    "    if len(title) <= 3 or re.fullmatch(r\"[^A-Za-z0-9]+\", title):\n",
    "        return None\n",
    "\n",
    "    return title\n",
    "\n",
    "# Define cleaning function for \"Abstract\"\n",
    "def clean_abstract(abstract):\n",
    "    if not isinstance(abstract, str) or abstract.strip() == \"\":\n",
    "        return None\n",
    "\n",
    "    abstract = clean(\n",
    "        abstract,\n",
    "        fix_unicode=True,\n",
    "        to_ascii=True,\n",
    "        lower=False,\n",
    "        no_line_breaks=False,\n",
    "        no_urls=True,\n",
    "        no_emails=True,\n",
    "        no_phone_numbers=True,\n",
    "        no_numbers=False,\n",
    "        no_digits=False,\n",
    "        no_currency_symbols=True,\n",
    "        no_punct=False,\n",
    "        replace_with_url=\"<URL>\",\n",
    "    )\n",
    "\n",
    "    # Trim whitespace\n",
    "    abstract = abstract.strip()\n",
    "\n",
    "    # Remove abstracts that start with specific unwanted text\n",
    "    blocked_phrases = [\n",
    "        \"Our website uses cookies to enhance your experience.\",\n",
    "        \"(Cell Reports\"\n",
    "    ]\n",
    "    \n",
    "    # Check if abstract starts with any blocked phrase\n",
    "    if any(abstract.startswith(phrase) for phrase in blocked_phrases):\n",
    "        return None  # Mark it as None (to be dropped later)\n",
    "    return abstract\n",
    "\n",
    "# Apply cleaning functions\n",
    "clean_df_step3 = clean_df_step2.copy()  # Ensure clean_df_step3 is defined\n",
    "\n",
    "clean_df_step3[\"PaperTitle\"] = clean_df_step3[\"PaperTitle\"].progress_apply(clean_paper_title)\n",
    "clean_df_step3[\"Abstract\"] = clean_df_step3[\"Abstract\"].progress_apply(clean_abstract)\n",
    "\n",
    "# Remove rows where \"PaperTitle\" has 2 or fewer words\n",
    "def has_min_words(title, min_words=3):\n",
    "    \"\"\"Check if title has at least the required number of words.\"\"\"\n",
    "    if not isinstance(title, str):\n",
    "        return False\n",
    "    return len(title.split()) >= min_words\n",
    "\n",
    "mask_valid_title = clean_df_step3[\"PaperTitle\"].progress_apply(lambda title: has_min_words(title, 3))\n",
    "clean_df_step3 = clean_df_step3[mask_valid_title]\n",
    "\n",
    "# Drop any rows where \"PaperTitle\" or \"Abstract\" is now empty\n",
    "clean_df_step3 = clean_df_step3.dropna(subset=[\"PaperTitle\", \"Abstract\"]).reset_index(drop=True)\n",
    "\n",
    "# Store final row count after cleaning\n",
    "final_cleaning_step3_count = len(clean_df_step3)\n",
    "cleaning_step3_rows_removed = cleaning_step2_count - final_cleaning_step3_count\n",
    "\n",
    "# Save the cleaned dataset\n",
    "clean_df_step3.to_csv(\"clean_df_step3.csv\", index=False)\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nCleaning complete!\")\n",
    "print(f\"Initial number of articles: {cleaning_step2_count}\")\n",
    "print(f\"Final number of articles after cleaning: {final_cleaning_step3_count}\")\n",
    "print(f\"Total number of removed articles: {cleaning_step3_rows_removed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2739305c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More specific normalization of PaperTitle and Abstracts, without removing any furhter columns\n",
    "\n",
    "file_path = \"clean_df_step3.csv\"\n",
    "\n",
    "if 'clean_df_step3' in globals():\n",
    "    clean_df_step4 = clean_df_step3.copy()\n",
    "    print(\"clean_df_step4 copied from clean_df_step3!\")\n",
    "else:\n",
    "    os.chdir(output_directory)\n",
    "    clean_df_step3 = pd.read_csv(file_path)\n",
    "    clean_df_step4 = clean_df_step3.copy()\n",
    "    print(\"File loaded successfully!\")\n",
    "\n",
    "    \n",
    "# Store initial row count before cleaning\n",
    "cleaning_step3_count = len(clean_df_step3)\n",
    "\n",
    "# Enable tqdm progress bars for pandas operations\n",
    "tqdm.pandas()\n",
    "\n",
    "# Helper function to convert ALL CAPS text to sentence case\n",
    "def to_sentence_case(text):\n",
    "    if text.isupper():  # Check if it's all caps\n",
    "        return text.capitalize()  # Convert to sentence case\n",
    "    return text  # Return unchanged if not all caps\n",
    "\n",
    "# Cleaning function for PaperTitle\n",
    "def clean_paper_title(title):\n",
    "    if not isinstance(title, str) or title.strip() == \"\":\n",
    "        return title \n",
    "    \n",
    "    # Remove unwanted keywords at the beginning of the title\n",
    "    title = re.sub(\n",
    "        r\"^\\s*(\\*?\\[?(invited|keynote|winner|regular paper|blog)\\]?\\*?)\\s*[:\\-]*\",\n",
    "        \"\",\n",
    "        title,\n",
    "        flags=re.IGNORECASE\n",
    "    )\n",
    "    title = re.sub(r\"[_-]{5,}\", \"\", title)  # Remove long underscores or dashes\n",
    "    title = re.sub(r\"^\\s*[_]*Abstract[_]*\\s*[:\\-]*\", \"\", title, flags=re.IGNORECASE)  # Remove \"Abstract\" heading\n",
    "    title = re.sub(r\"^\\s*[\\.\\,\\-\\:\\;\\//\\=\\s]+\", \"\", title)  # Remove leading . , - : ; // = and spaces\n",
    "    title = re.sub(r\"^\\[([^\\]]+)\\]\\.?\\s*$\", r\"\\1\", title)  # Remove surrounding square brackets + trailing dot\n",
    "    title = re.sub(r\"\\s+\", \" \", title).strip()  # Normalize spaces\n",
    "    title = to_sentence_case(title)  # Convert to sentence case if all caps\n",
    "    return title\n",
    "\n",
    "# Cleaning function for Abstract\n",
    "def clean_abstract(abstract):\n",
    "    if not isinstance(abstract, str) or abstract.strip() == \"\":\n",
    "        return abstract\n",
    "\n",
    "    # Remove everything before \"introduction\" if abstract starts with \"You have access\"\n",
    "    if abstract.lower().startswith(\"you have access\"):\n",
    "        match = re.search(r\"\\bintroduction\\b\", abstract, re.IGNORECASE)\n",
    "        if match:\n",
    "            abstract = abstract[match.start():]  # Keep everything from \"introduction\" onwards\n",
    "\n",
    "    # If it starts with \"//\", remove everything until \"abstract\" appears\n",
    "    if abstract.startswith(\"//\"):\n",
    "        match = re.search(r\"\\babstracts?\\b\", abstract, re.IGNORECASE)\n",
    "        if match:\n",
    "            abstract = abstract[match.start():]  # Keep only the part after \"abstract\"\n",
    "\n",
    "    # Remove leading colons and spaces\n",
    "    abstract = re.sub(r\"^\\s*[:]+\\s*\", \"\", abstract)  # Remove any leading : and spaces\n",
    "\n",
    "    # Remove long underscores/dashes at the start\n",
    "    abstract = re.sub(r\"[_-]{5,}\", \"\", abstract)  # Remove long underscores or dashes\n",
    "    abstract = re.sub(r\"^\\s*[_]*Abstract[_]*\\s*[:\\-]*\", \"\", abstract, flags=re.IGNORECASE)  # Remove \"Abstract\" heading\n",
    "    abstract = re.sub(r\"^\\s*[\\.\\,\\-\\:\\;\\=\\s]+\", \"\", abstract)  # Remove leading . , - : ; = and spaces\n",
    "    abstract = re.sub(r\"\\s*<[^>]+>\\s*\", \" \", abstract)  # Remove XML-like tags like <CUR>\n",
    "    abstract = re.sub(r\"[,\\.]{2,}\", \".\", abstract)  # Normalize excessive dots and commas\n",
    "    abstract = re.sub(r\"\\s+\", \" \", abstract).strip()  # Normalize spaces\n",
    "\n",
    "    abstract = to_sentence_case(abstract)  # Convert to sentence case if all caps\n",
    "\n",
    "    return abstract\n",
    "\n",
    "# Apply cleaning with progress bar\n",
    "print(\"\\nCleaning 'PaperTitle' column...\")\n",
    "clean_df_step4[\"PaperTitle\"] = clean_df_step4[\"PaperTitle\"].progress_apply(clean_paper_title)\n",
    "\n",
    "print(\"\\nCleaning 'Abstract' column...\")\n",
    "clean_df_step4[\"Abstract\"] = clean_df_step4[\"Abstract\"].progress_apply(clean_abstract)\n",
    "\n",
    "# Store initial row count after cleaning\n",
    "cleaning_step4_count = len(clean_df_step4)\n",
    "cleaning_step4_rows_removed = cleaning_step3_count - cleaning_step4_count\n",
    "\n",
    "cleaning_allstep_rows_removed = cleaning_step3_rows_removed+cleaning_step4_rows_removed\n",
    "clean_df_step4_file_path = \"clean_df_step4.csv\"\n",
    "clean_df_step4.to_csv(clean_df_step4_file_path, index=False)\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nNormalization complete!\")\n",
    "print(f\"File saved: {clean_df_step4_file_path}\")\n",
    "print(f\"Count before cleaning step 4: {cleaning_step3_count:,}\")\n",
    "print(f\"Count after cleaning step 4: {cleaning_step4_count:,}\")\n",
    "print(f\"Total number of removed articles in step 4: {cleaning_step4_rows_removed:,}\") #should be 0, we only normalize!\n",
    "print(f\"Total number of removed articles in step 3 & 4: {cleaning_allstep_rows_removed:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5711ef05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for PubDate and investgiate dataset\n",
    "clean_df_step4_2019 = clean_df_step4[\n",
    "    (clean_df_step4['PubDate'] >= '2019-01-01') & \n",
    "    (clean_df_step4['PubDate'] <= '2019-12-31')\n",
    "]\n",
    "\n",
    "clean_df_step4_2019.to_csv(\"clean_df_step4_2019.csv\", index=False)\n",
    "print(\"Filtered dataset saved as 'clean_df_step4_2019.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d413481",
   "metadata": {},
   "source": [
    "## 7) Print summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0755eb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Collect summary statistics\n",
    "counted_total_removed = (total_duplicates_removed +\n",
    "                        missing_values +\n",
    "                        nonenglish_articles + \n",
    "                        artifacts_removed +\n",
    "                        nonsense_title_counts +\n",
    "                        title_length_counts +    \n",
    "                        abstract_length_counts +\n",
    "                        withdraw_counts +\n",
    "                        non_research_counts +\n",
    "                        cleaning_allstep_rows_removed\n",
    ")\n",
    "\n",
    "true_total_removed = OpenAlex_article_count-cleaning_step4_count\n",
    "removal_statistics = {\n",
    "     ## 1) Removal of duplicates and empty PaperTitle/Abstract/PubDate rows\n",
    "    \"\\n\\nArticles from OpenAlex\": OpenAlex_article_count,\n",
    "    \"- Duplicates\": total_duplicates_removed,\n",
    "    \"- Missing title, abstract, or metadata\": missing_values,\n",
    "    ## 2) Removal of articles in languages other than \"en\", even though set to \"en\" in metadata\n",
    "    \"- Non-English articles\": nonenglish_articles,\n",
    "    ## 3) Remove artifacts, i.e., all supplementary tables, figures and data\n",
    "    \"- Artifacts & supplementaries\": artifacts_removed,\n",
    "    ## 4) Removing invalid titles, placeholders, length-based issues,  withdrawn papers, and 'non-research' articles\n",
    "    \"- Nonesense title entries\": nonsense_title_counts, \n",
    "    \"- Length-based issues\": title_length_counts+abstract_length_counts,\n",
    "    \"- Withdrawn articles\": withdraw_counts,\n",
    "    \"- Non-research articles\": non_research_counts,\n",
    "    ## 5) Cleaning, harmonizing and normalization of PaperTitles and AbstractsÂ¶\n",
    "    \"- Cleaning, harmonizing and normalization\": cleaning_allstep_rows_removed,\n",
    "\n",
    "    \"\\nTrue total removed\": true_total_removed,\n",
    "    \"Counted total removed\": counted_total_removed,\n",
    "    \n",
    "    \"\\n\\n\\nFinal articles for NER gene extraction\": cleaning_step4_count\n",
    "}\n",
    "\n",
    "print(\"\\n**Final Cleaning Summary:**\")\n",
    "for key, value in removal_statistics.items():\n",
    "    print(f\"{key}: {format(value, ',')}\")\n",
    "\n",
    "txt_file_path = \"final_cleaning_summary.txt\"\n",
    "with open(txt_file_path, \"w\") as f:\n",
    "    f.write(\"Final Cleaning Summary\\n\")\n",
    "    f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "    for key, value in removal_statistics.items():\n",
    "        f.write(f\"{key}: {format(value, ',')}\\n\")\n",
    "\n",
    "print(f\"\\n Summary saved to: {txt_file_path}\")\n",
    "\n",
    "csv_file_path = \"final_cleaning_summary.csv\"\n",
    "\n",
    "df_summary = pd.DataFrame(\n",
    "    [(key, format(value, ',')) for key, value in removal_statistics.items()],\n",
    "    columns=[\"Metric\", \"Value\"]\n",
    ")\n",
    "df_summary.to_csv(csv_file_path, index=False)\n",
    "print(f\" CSV saved to: {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c08f562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure creation\n",
    "\n",
    "# Import dataframe\n",
    "os.chdir(output_directory)\n",
    "file_path = \"clean_df_step4.csv\"\n",
    "\n",
    "if 'clean_df_step4' in globals():\n",
    "    figure_df = clean_df_step4.copy()\n",
    "    print(\"figure_df copied from existing DataFrame!\")\n",
    "else:\n",
    "    os.chdir(output_directory)\n",
    "    clean_df_step4 = pd.read_csv(file_path)\n",
    "    figure_df = clean_df_step4.copy()\n",
    "    \n",
    "figure_df['PubDate'] = pd.to_datetime(figure_df['PubDate'], errors='coerce')\n",
    "figure_df['Year'] = figure_df['PubDate'].dt.year\n",
    "figure_df['YearMonth'] = figure_df['PubDate'].dt.to_period('M').astype(str)\n",
    "figure_df['YearWeek'] = figure_df['PubDate'].dt.to_period('W').astype(str)\n",
    "figure_df = figure_df[figure_df['Year'].between(2014, 2025)]\n",
    "yearly_counts = figure_df['Year'].value_counts().sort_index()\n",
    "monthly_counts = figure_df['YearMonth'].value_counts().sort_index()\n",
    "weekly_counts = figure_df['YearWeek'].value_counts().sort_index()\n",
    "years = list(range(2015, 2026))\n",
    "shifted_labels = list(range(2014, 2025))\n",
    "colors = plt.cm.tab20.colors\n",
    "\n",
    "print(\"Success!\")\n",
    "\n",
    "# ====== Yearly Publications ======\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(yearly_counts.index + 1, yearly_counts.values, color=colors[0])  # Shift years +1 for alignment\n",
    "\n",
    "# Styling\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of articles')\n",
    "plt.title('Publications per year (2014-2024)')\n",
    "plt.xlim(2014, 2026)\n",
    "plt.ylim(0, yearly_counts.max() * 1.1)\n",
    "plt.xticks(years, shifted_labels) \n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# ====== Monthly Publications ======\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(monthly_counts.index, monthly_counts.values, marker='o', linestyle='-', color=colors[1], linewidth=2)\n",
    "plt.xticks([f\"{year}-01\" for year in range(2014, 2026)], range(2014, 2026))\n",
    "\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of articles')\n",
    "plt.title('Publications per month (2014-2024)')\n",
    "plt.xlim(monthly_counts.index.min(), monthly_counts.index.max())\n",
    "plt.ylim(0, monthly_counts.max() * 1.1)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# ====== Weekly Publications ======\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "weekly_counts_smoothed = weekly_counts.rolling(window=4, center=True).mean()\n",
    "plt.plot(weekly_counts.index, weekly_counts_smoothed, linestyle='-', color=colors[2], linewidth=2)\n",
    "plt.xticks([f\"{year}-W01\" for year in range(2014, 2026)], range(2014, 2026))\n",
    "\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of articles')\n",
    "plt.title('Publications per week (2014-2024)')\n",
    "plt.xlim(weekly_counts.index.min(), weekly_counts.index.max())\n",
    "plt.ylim(0, weekly_counts.max() * 1.1)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
