{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6e09ddd",
   "metadata": {},
   "source": [
    "# BioBERT-based gene extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732021a4",
   "metadata": {},
   "source": [
    "# 1) Install libraries and load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e2c9f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import successful!\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "print(\"Success!\")\n",
    "from transformers import pipeline\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from fuzzywuzzy import process, fuzz\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "print(\"Import successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "fa12a937",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BioBERT model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load BioBERT Genetic NER Model (Using CUDA if available)\n",
    "biobert_model = pipeline(\n",
    "    \"ner\",\n",
    "    model=\"alvaroalon2/biobert_genetic_ner\",\n",
    "    tokenizer=\"alvaroalon2/biobert_genetic_ner\",\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "print(\"BioBERT model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "761c47ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genes import successful!\n",
      "Using preloaded full_articles from memory.\n",
      "Article import successful!\n",
      "\n",
      "Imported 100 articles with 9 selected columns.\n",
      "Imported 161 oncomine genes.\n",
      "\n",
      "Current Working Directory: /data/JH/marie/TrendyVariants/ICIMTH\n"
     ]
    }
   ],
   "source": [
    "# Set the working directory and file paths\n",
    "working_directory = \"WORKING_DIRECTORY\"\n",
    "input_directory = \"INPUT_DIRECTORY\"\n",
    "output_directory = \"OUTPUT_DIRECTORY\"\n",
    "articles_file = \"articles.csv\"\n",
    "genes_file = \"genes.csv\"\n",
    "\n",
    "# Change the working directory\n",
    "os.chdir(output_directory)\n",
    "print(\"Current Working Directory:\", os.getcwd())\n",
    "\n",
    "os.chdir(input_directory)\n",
    "genes = pd.read_csv(genes_file, header=None)\n",
    "gene_list = genes[0].tolist()\n",
    "print(\"Genes import successful!\")\n",
    "\n",
    "os.chdir(output_directory)\n",
    "if \"full_articles\" not in globals():\n",
    "    full_articles = pd.read_csv(articles_file)\n",
    "    print(f\"Loaded {len(full_articles)} articles from CSV.\")\n",
    "else:\n",
    "    print(\"Using preloaded full_articles from memory.\")\n",
    "articles = full_articles.head(100)\n",
    "print(\"Article import successful!\")\n",
    "print(f\"\\nImported {len(articles)} articles with {len(articles.columns)} selected columns.\")\n",
    "print(f\"Imported {len(gene_list):,} oncomine genes.\")\n",
    "\n",
    "num_rows = articles.shape[0]\n",
    "num_columns = articles.shape[1]\n",
    "os.chdir(working_directory)\n",
    "print(\"\\nCurrent Working Directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e88d830",
   "metadata": {},
   "source": [
    "# 2) Run BioBERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e1bd79ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded gene list contains 161 genes with synonyms.\n",
      "Success!\n",
      "Processing 100 articles with BioBERT. Started at 2025-03-06 12:08:39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Articles:   0%|                                                                                  | 0/100 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1355 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 1: set()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Articles:   4%|██▉                                                                       | 4/100 [00:00<00:10,  9.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 2: set()\n",
      "Article 3: set()\n",
      "Article 4: set()\n",
      "Article 5: set()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Articles:   8%|█████▉                                                                    | 8/100 [00:00<00:07, 13.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 6: set()\n",
      "Article 7: set()\n",
      "Article 8: set()\n",
      "Article 9: set()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Articles:  10%|███████▎                                                                 | 10/100 [00:00<00:06, 13.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 10: {'EGFR'}\n",
      "Article 11: {'TP53', 'CTNNB1'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Articles:  14%|██████████▏                                                              | 14/100 [00:01<00:06, 12.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 12: set()\n",
      "Article 13: {'NRG1', 'PTEN'}\n",
      "Article 14: {'TP53', 'AKT1', 'PIK3CA', 'MTOR'}\n",
      "Article 15: set()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Articles:  18%|█████████████▏                                                           | 18/100 [00:01<00:05, 13.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 16: set()\n",
      "Article 17: set()\n",
      "Article 18: set()\n",
      "Article 19: set()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Articles:  22%|████████████████                                                         | 22/100 [00:01<00:05, 14.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 20: set()\n",
      "Article 21: {'PRKACA', 'ESR1'}\n",
      "Article 22: set()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Articles:  24%|█████████████████▌                                                       | 24/100 [00:01<00:05, 14.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 23: set()\n",
      "Article 24: set()\n",
      "Article 25: set()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Articles:  28%|████████████████████▍                                                    | 28/100 [00:02<00:04, 14.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 26: set()\n",
      "Article 27: set()\n",
      "Article 28: set()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Articles:  32%|███████████████████████▎                                                 | 32/100 [00:02<00:04, 15.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 29: set()\n",
      "Article 30: set()\n",
      "Article 31: set()\n",
      "Article 32: {'PMS2', 'MSH2', 'MLH1', 'MSH6'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Articles:  34%|████████████████████████▊                                                | 34/100 [00:02<00:04, 15.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 33: set()\n",
      "Article 34: set()\n",
      "Article 35: set()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Articles:  38%|███████████████████████████▋                                             | 38/100 [00:02<00:04, 14.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 36: {'EGFR'}\n",
      "Article 37: set()\n",
      "Article 38: {'JAK3'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Articles:  42%|██████████████████████████████▋                                          | 42/100 [00:03<00:03, 14.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 39: set()\n",
      "Article 40: set()\n",
      "Article 41: set()\n",
      "Article 42: set()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Articles:  44%|████████████████████████████████                                         | 44/100 [00:03<00:03, 14.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 43: set()\n",
      "Article 44: set()\n",
      "Article 45: set()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Articles:  48%|███████████████████████████████████                                      | 48/100 [00:03<00:03, 14.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 46: set()\n",
      "Article 47: set()\n",
      "Article 48: set()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Articles:  50%|████████████████████████████████████▌                                    | 50/100 [00:03<00:03, 13.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 49: set()\n",
      "Article 50: set()\n",
      "Article 51: set()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Articles:  54%|███████████████████████████████████████▍                                 | 54/100 [00:03<00:03, 15.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 52: set()\n",
      "Article 53: set()\n",
      "Article 54: set()\n",
      "Article 55: set()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Articles:  58%|██████████████████████████████████████████▎                              | 58/100 [00:04<00:02, 14.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 56: set()\n",
      "Article 57: set()\n",
      "Article 58: set()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Articles:  62%|█████████████████████████████████████████████▎                           | 62/100 [00:04<00:02, 15.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 59: set()\n",
      "Article 60: set()\n",
      "Article 61: set()\n",
      "Article 62: set()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Articles:  66%|████████████████████████████████████████████████▏                        | 66/100 [00:04<00:02, 15.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 63: {'TP53'}\n",
      "Article 64: {'MAPK1', 'BRAF'}\n",
      "Article 65: set()\n",
      "Article 66: set()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Articles:  70%|███████████████████████████████████████████████████                      | 70/100 [00:04<00:01, 16.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 67: set()\n",
      "Article 68: set()\n",
      "Article 69: set()\n",
      "Article 70: set()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Articles:  74%|██████████████████████████████████████████████████████                   | 74/100 [00:05<00:01, 15.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 71: set()\n",
      "Article 72: set()\n",
      "Article 73: set()\n",
      "Article 74: {'NOTCH1'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Articles:  76%|███████████████████████████████████████████████████████▍                 | 76/100 [00:05<00:01, 15.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 75: set()\n",
      "Article 76: set()\n",
      "Article 77: set()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Articles:  80%|██████████████████████████████████████████████████████████▍              | 80/100 [00:05<00:01, 15.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 78: set()\n",
      "Article 79: set()\n",
      "Article 80: {'CDKN2A', 'MET', 'ALK', 'EGFR'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Articles:  84%|█████████████████████████████████████████████████████████████▎           | 84/100 [00:05<00:01, 14.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 81: set()\n",
      "Article 82: set()\n",
      "Article 83: set()\n",
      "Article 84: set()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Articles:  86%|██████████████████████████████████████████████████████████████▊          | 86/100 [00:06<00:00, 15.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 85: set()\n",
      "Article 86: set()\n",
      "Article 87: {'POLE'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Articles:  90%|█████████████████████████████████████████████████████████████████▋       | 90/100 [00:06<00:00, 14.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 88: set()\n",
      "Article 89: {'NF2'}\n",
      "Article 90: set()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Articles:  94%|████████████████████████████████████████████████████████████████████▌    | 94/100 [00:06<00:00, 15.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 91: {'EZH2', 'KRAS'}\n",
      "Article 92: {'SRC'}\n",
      "Article 93: set()\n",
      "Article 94: set()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Articles:  98%|███████████████████████████████████████████████████████████████████████▌ | 98/100 [00:06<00:00, 15.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 95: set()\n",
      "Article 96: set()\n",
      "Article 97: set()\n",
      "Article 98: set()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Articles: 100%|████████████████████████████████████████████████████████████████████████| 100/100 [00:06<00:00, 14.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 99: set()\n",
      "Article 100: set()\n",
      "Processing completed at 2025-03-06 12:08:46. Total runtime: 6.92 seconds.\n",
      "Runtime details saved in: filtered_articles_biobert_expanded_100_runtime.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "##### BioBERT with slinding window\n",
    "# Function to fetch gene synonyms from MyGene.info API\n",
    "def get_gene_synonyms(gene_symbol):\n",
    "    \"\"\"Fetches known synonyms, including protein products, for a given gene from MyGene.info.\"\"\"\n",
    "    url = f\"https://mygene.info/v3/query?q={gene_symbol}&fields=symbol,alias,other_names\"\n",
    "    try:\n",
    "        response = requests.get(url).json()\n",
    "        synonyms = set()\n",
    "        for hit in response.get(\"hits\", []):\n",
    "            if \"symbol\" in hit:\n",
    "                synonyms.add(hit[\"symbol\"].upper())\n",
    "            if \"alias\" in hit:\n",
    "                synonyms.update([alias.upper() for alias in hit[\"alias\"]])\n",
    "            if \"other_names\" in hit:\n",
    "                synonyms.update([name.upper() for name in hit[\"other_names\"]])\n",
    "        return synonyms\n",
    "    except:\n",
    "        return {gene_symbol.upper()} \n",
    "\n",
    "# Expand the gene list dynamically with synonyms\n",
    "expanded_gene_list = {gene.upper(): get_gene_synonyms(gene) for gene in gene_list}\n",
    "print(f\"Expanded gene list contains {len(expanded_gene_list)} genes with synonyms.\")\n",
    "\n",
    "# Function to normalize extracted genes\n",
    "def normalize_extracted_genes(found_terms):\n",
    "    \"\"\"Normalize and map extracted entities to closest known gene or protein names.\"\"\"\n",
    "    normalized_genes = set()\n",
    "    for term in found_terms:\n",
    "        term_upper = term.upper()\n",
    "        if term_upper in expanded_gene_list:\n",
    "            normalized_genes.add(term_upper)\n",
    "            continue\n",
    "        cleaned_term = re.sub(r\"[\\[\\]\\(\\),-]\", \" \", term_upper)\n",
    "        cleaned_words = cleaned_term.split()\n",
    "        for word in cleaned_words:\n",
    "            if word in expanded_gene_list:\n",
    "                normalized_genes.add(word)\n",
    "        if not any(gene in normalized_genes for gene in cleaned_words):\n",
    "            match = process.extractOne(term_upper, expanded_gene_list.keys(), scorer=fuzz.ratio)\n",
    "            if match:\n",
    "                best_match, score = match[:2]\n",
    "                if score > 85:\n",
    "                    normalized_genes.add(best_match)\n",
    "    return normalized_genes\n",
    "\n",
    "# Function to split text into overlapping chunks for NER\n",
    "def sliding_window_chunking(text, tokenizer, max_tokens=512, stride=256):\n",
    "    \"\"\"Splits text into overlapping chunks to avoid losing context.\"\"\"\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    if len(tokens) <= max_tokens:\n",
    "        return [tokenizer.decode(tokens, skip_special_tokens=True)]\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), stride):\n",
    "        chunk = tokens[i : i + max_tokens]\n",
    "        if len(chunk) < max_tokens:\n",
    "            break\n",
    "        chunks.append(tokenizer.decode(chunk, skip_special_tokens=True))\n",
    "    return chunks\n",
    "\n",
    "# Function to process text with BioBERT using sliding window\n",
    "def process_biobert(text, model):\n",
    "    \"\"\"Runs BioBERT NER with sliding window chunking.\"\"\"\n",
    "    if pd.isna(text) or len(text.strip()) == 0:\n",
    "        return set()\n",
    "    tokenizer = model.tokenizer\n",
    "    text_chunks = sliding_window_chunking(text, tokenizer)\n",
    "    found_terms = set()\n",
    "    for chunk in text_chunks:\n",
    "        results = model(chunk)\n",
    "        current_term = []\n",
    "        for res in results:\n",
    "            word = res[\"word\"].replace(\"##\", \"\")\n",
    "            if res[\"entity\"].startswith(\"B-\"):\n",
    "                if current_term:\n",
    "                    full_term = \"\".join(current_term)\n",
    "                    found_terms.add(full_term)\n",
    "                current_term = [word]\n",
    "            elif res[\"entity\"].startswith(\"I-\"):\n",
    "                current_term.append(word)\n",
    "        if current_term:\n",
    "            full_term = \"\".join(current_term)\n",
    "            found_terms.add(full_term)\n",
    "    return normalize_extracted_genes(found_terms)\n",
    "\n",
    "print(\"Success!\")\n",
    "\n",
    "\n",
    "##### Gene extraction #####\n",
    "start_time = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "start_timestamp = time.time()\n",
    "print(f\"Processing {len(article_df)} articles with BioBERT. Started at {start_time}\")\n",
    "\n",
    "biobert_results = []\n",
    "for index, row in tqdm(article_df.iterrows(), total=len(article_df), desc=\"Processing Articles\"):\n",
    "    title = row.get(\"PaperTitle\", \"\")\n",
    "    abstract = row.get(\"Abstract\", \"\")\n",
    "    genes_biobert = process_biobert(title, biobert_model) | process_biobert(abstract, biobert_model)\n",
    "    biobert_results.append(\", \".join(genes_biobert))\n",
    "    print(f\"Article {index+1}: {genes_biobert}\")\n",
    "\n",
    "df_results = article_df.copy()\n",
    "df_results[\"BioBERT\"] = biobert_results\n",
    "num_articles = len(df_results)\n",
    "\n",
    "output_file = f\"filtered_articles_biobert_expanded_{num_articles}.csv\"\n",
    "runtime_file = f\"filtered_articles_biobert_expanded_{num_articles}_runtime.txt\"\n",
    "\n",
    "end_time = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "end_timestamp = time.time()\n",
    "total_runtime = end_timestamp - start_timestamp\n",
    "print(f\"Processing completed at {end_time}. Total runtime: {total_runtime:.2f} seconds.\")\n",
    "\n",
    "with open(runtime_file, \"w\") as f:\n",
    "    f.write(f\"Processing of articles: {num_articles}\\n\")\n",
    "    f.write(f\"Processing started at: {start_time}\\n\")\n",
    "    f.write(f\"Processing completed at: {end_time}\\n\")\n",
    "    f.write(f\"Total runtime: {total_runtime:.2f} seconds\\n\")\n",
    "print(f\"Runtime details saved in: {runtime_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d43a025",
   "metadata": {},
   "source": [
    "## 3) Create binary matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "61fe855b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset: 100\n",
      "Column length of dataset: 10\n",
      "Columns of dataset: Index(['PaperId', 'PaperTitle', 'Citations', 'CoFoS', 'Authors', 'Abstract',\n",
      "       'Language', 'PubYear', 'PubDate', 'BioBERT'],\n",
      "      dtype='object')\n",
      "\n",
      "File saved as: BioBERT_evaluation_results.csv\n",
      "Results saved to 'Sum_Entity_Mentions_Evaluations.txt'\n",
      "Total sum of 'Sum_Entity_Mentions' column: 30\n",
      "Cross-check: Total sum of all binary matrix values (1s in the matrix): 30\n"
     ]
    }
   ],
   "source": [
    "# Binary matrix creation\n",
    "BioBERT_df = df_results.copy()\n",
    "print(\"Length of dataset:\", len(BioBERT_df))\n",
    "print(\"Column length of dataset:\", len(BioBERT_df.columns))\n",
    "print(\"Columns of dataset:\", BioBERT_df.columns)\n",
    "BioBERT_original = BioBERT_df[\"BioBERT\"].copy()\n",
    "BioBERT_df[\"BioBERT\"] = BioBERT_df[\"BioBERT\"].fillna(\"\").astype(str)\n",
    "BioBERT_df[\"Extracted_Genes\"] = BioBERT_df[\"BioBERT\"].apply(lambda x: [gene.strip() for gene in x.split(',') if gene.strip()])\n",
    "binary_gene_data = {gene: BioBERT_df[\"Extracted_Genes\"].apply(lambda genes: 1 if gene in genes else 0) for gene in gene_list}\n",
    "binary_gene_df = pd.DataFrame(binary_gene_data)\n",
    "BioBERT_df = pd.concat([BioBERT_df, binary_gene_df], axis=1)\n",
    "\n",
    "BioBERT_df[\"Sum_Entity_Mentions\"] = binary_gene_df.sum(axis=1)\n",
    "\n",
    "BioBERT_df[\"BioBERT\"] = BioBERT_original\n",
    "BioBERT_df.drop(columns=[\"Extracted_Genes\"], inplace=True)\n",
    "\n",
    "# Save as CSV\n",
    "os.chdir(working_directory)\n",
    "output_filename = f\"BioBERT_evaluation_results.csv\"\n",
    "BioBERT_df.to_csv(output_filename, index=False)\n",
    "print(f\"\\nFile saved as: {output_filename}\")\n",
    "\n",
    "os.chdir(working_directory)\n",
    "binary_gene_columns = [col for col in BioBERT_df.columns if col in gene_list]\n",
    "BioBERT_df[binary_gene_columns] = BioBERT_df[binary_gene_columns].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "total_gene_mentions = BioBERT_df[\"Sum_Entity_Mentions\"].sum()\n",
    "total_binary_sum = BioBERT_df[binary_gene_columns].sum().sum()\n",
    "\n",
    "results_dict = {\n",
    "    \"Metric\": [\"Total_Sum_Entity_Mentions\", \"Total_Binary_Matrix_Sum\"],\n",
    "    \"Value\": [total_gene_mentions, total_binary_sum]\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(results_dict)\n",
    "results_df.to_csv(\"BiobERT_evaluation_results.txt\", sep=\"\\t\", index=False)\n",
    "\n",
    "print(f\"Results saved to 'Sum_Entity_Mentions_Evaluations.txt'\")\n",
    "print(f\"Total sum of 'Sum_Entity_Mentions' column: {total_gene_mentions}\")\n",
    "print(f\"Cross-check: Total sum of all binary matrix values (1s in the matrix): {total_binary_sum}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
