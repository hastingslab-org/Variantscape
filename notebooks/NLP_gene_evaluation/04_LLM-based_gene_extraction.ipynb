{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbdd887b",
   "metadata": {},
   "source": [
    "# LLM-based gene extraction (GPT-4o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83670bcd",
   "metadata": {},
   "source": [
    "# 1) Set up libraries and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e0639c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from fuzzywuzzy import fuzz, process\n",
    "from datetime import datetime, timedelta\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d977552",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set the working directory and file paths\n",
    "working_directory = \"WORKING_DIRECTORY\"\n",
    "input_directory = \"INPUT_DIRECTORY\"\n",
    "output_directory = \"OUTPUT_DIRECTORY\"\n",
    "articles_file = \"articles.csv\"\n",
    "genes_file = \"genes.csv\"\n",
    "\n",
    "# Change the working directory\n",
    "os.chdir(output_directory)\n",
    "print(\"Current Working Directory:\", os.getcwd())\n",
    "\n",
    "os.chdir(input_directory)\n",
    "genes = pd.read_csv(genes_file, header=None)\n",
    "gene_list = genes[0].tolist()\n",
    "print(\"Genes import successful!\")\n",
    "\n",
    "os.chdir(output_directory)\n",
    "if \"full_articles\" not in globals():\n",
    "    full_articles = pd.read_csv(articles_file)\n",
    "    print(f\"Loaded {len(full_articles)} articles from CSV.\")\n",
    "else:\n",
    "    print(\"Using preloaded full_articles from memory.\")\n",
    "articles = full_articles.head(100)\n",
    "print(\"Article import successful!\")\n",
    "print(f\"\\nImported {len(articles):,} articles with {len(articles.columns):,} selected columns.\")\n",
    "print(f\"Imported {len(gene_list):,} oncomine genes.\")\n",
    "\n",
    "num_rows = articles.shape[0]\n",
    "num_columns = articles.shape[1]\n",
    "os.chdir(working_directory)\n",
    "print(\"\\nCurrent Working Directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97a1e27",
   "metadata": {},
   "source": [
    "# 2) Select and set up LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75357b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a language model to answer the questions\n",
    "!pip install OpenAI\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from openai import OpenAI\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ccd919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models to be tested\n",
    "models = [\"llama31-70b\", \"llama33-70b\", \"deepseek_v3\", \n",
    "          \"deepseek_r1\", \"deepseek_r1_distill_llama_70b\",\"gpt4o\"]\n",
    "\n",
    "# Mapping model names to their full Hugging Face or DeepInfra identifiers\n",
    "model_fullnames = {\n",
    "    \"llama31-70b\": \"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n",
    "    \"llama33-70b\": \"meta-llama/Llama-3.3-70B-Instruct\",\n",
    "    \"deepseek_v3\": \"deepseek-ai/DeepSeek-V3\",\n",
    "    \"deepseek_r1\": \"deepseek-ai/DeepSeek-R1\",\n",
    "    \"deepseek_r1_distill_llama_70b\": \"deepseek-ai/DeepSeek-R1-Distill-Llama-70B\",\n",
    "    \"gpt4o\": \"gpt-4o\"\n",
    "}\n",
    "\n",
    "SYSTEM_MSG = \"You are a helpful medical question answering assistant. Please carefully follow the exact instructions and do not provide explanations.\"\n",
    "modelname = models[5]\n",
    "\n",
    "if modelname in [ \"llama2-3b\" ]:  # Local model\n",
    "    model, tokenizer = load(model_fullnames[modelname])\n",
    "    def generateFromPrompt(prompt):\n",
    "        if hasattr(tokenizer, \"apply_chat_template\") and tokenizer.chat_template is not None:\n",
    "            messages = [{\"role\": \"system\", \"content\": SYSTEM_MSG},\n",
    "                {\"role\": \"user\", \"content\": prompt}]\n",
    "            prompt = tokenizer.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "            response = generate(model, tokenizer, prompt=prompt, verbose=False)\n",
    "            return response\n",
    "elif modelname in [ \"gpt35\", \"gpt4o\" ]: # OpenAI models\n",
    "    client = OpenAI(\n",
    "       api_key='API_key1'  \n",
    "    )\n",
    "    def generateFromPrompt(promptStr,maxTokens=100):\n",
    "      messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_MSG},\n",
    "        {\"role\": \"user\", \"content\": promptStr}\n",
    "      ]\n",
    "      completion = client.chat.completions.create(\n",
    "        model=model_fullnames[modelname],\n",
    "        messages=messages)\n",
    "      response=completion.choices[0].message.content\n",
    "      return(response)\n",
    "elif modelname in [ \"llama31-70b\" , \"llama33-70b\" , \"deepseek_v3\" , \"deepseek_r1\" , \"deepseek_r1_distill_llama_70b\"]:  # DeepInfra models\n",
    "    client = OpenAI(\n",
    "        api_key = \"API_key2\",\n",
    "        base_url=\"https://api.deepinfra.com/v1/openai\",\n",
    "    )\n",
    "    def generateFromPrompt(promptStr,maxTokens=100):\n",
    "      messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_MSG},\n",
    "        {\"role\": \"user\", \"content\": promptStr}\n",
    "      ]\n",
    "      completion = client.chat.completions.create(\n",
    "        model=model_fullnames[modelname],\n",
    "        messages=messages)\n",
    "      response=completion.choices[0].message.content\n",
    "      return(response)\n",
    "    \n",
    "generateFromPrompt(\"hello!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8977f275",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"All installed models:\",   models)\n",
    "print(\"Current model in use:\",   modelname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cb678e",
   "metadata": {},
   "source": [
    "# 3) Define prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfe876d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define multiple prompts in a dictionary \n",
    "# The model will extract genes and gene products, filtering against `gene_list`\n",
    "# Use prompt #1\n",
    "\n",
    "PROMPTS = {\n",
    "    1: lambda title, abstract: (\n",
    "        f\"Extract all gene names and their gene products (e.g., TP53 and p53) from the given title and abstract.\"\n",
    "        f\"Only return genes that are present in the following predefined list:\\n\"\n",
    "        f\"{', '.join(gene_list)}.\\n\"\n",
    "        f\"If a gene is mentioned multiple times, only list it once.\\n\"\n",
    "        f\"Return the extracted genes as a **comma-separated list** (e.g., 'CTNNB1, RET, BRCA1').\\n\"\n",
    "        f\"If no genes from the list are present, return an **empty response** (do not return 'None' or 'No genes found').\\n\"\n",
    "        f\"Strictly **no additional information, no explanations, no formatting**.\\n\\n\"\n",
    "        f\"Title: {title}\\nAbstract: {abstract}\"\n",
    "    ),\n",
    "\n",
    "    2: lambda title, abstract: (\n",
    "        f\"Identify all gene symbols and their corresponding gene products mentioned in the given title and abstract.\\n\"\n",
    "        f\"Only include genes that exist in the predefined list:\\n\"\n",
    "        f\"{', '.join(gene_list)}.\\n\"\n",
    "        f\"Return the result as a simple **comma-separated list** (e.g., 'BRCA1, TP53, EGFR').\\n\"\n",
    "        f\"If no matching genes are found, return **an empty response** (do not print anything).\\n\"\n",
    "        f\"Ensure strict compliance:\\n\"\n",
    "        f\"- Do not include extra text or explanations.\\n\"\n",
    "        f\"- No formatting, no bullet points, no sentence structure.\\n\\n\"\n",
    "        f\"Title: {title}\\nAbstract: {abstract}\"\n",
    "    )\n",
    "}\n",
    "\n",
    "print(\"Prompts for gene extraction successfully defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449ca54d",
   "metadata": {},
   "source": [
    "# 4) Run genetic variant extraction with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc298f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================== CONFIGURATION ========================== #\n",
    "os.chdir(working_directory)\n",
    "tqdm.pandas()\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# Set model name and prompt selection\n",
    "modelname = modelname\n",
    "selected_prompt_number = 1\n",
    "\n",
    "today_date = datetime.today().strftime('%Y-%m-%d')\n",
    "start_time_str = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# ========================== FILE PATHS USING ============================ #\n",
    "variant_output_file_path = os.path.join(working_directory, f\"ICIMTH_LLM_variant_extraction_{modelname}_prompt{selected_prompt_number}.csv\")\n",
    "runtime_file = os.path.join(working_directory, f\"ICIMTH_runtime_summary_{modelname}_prompt{selected_prompt_number}.txt\")\n",
    "progress_log_file = os.path.join(working_directory, f\"ICIMTH_progress_log_{modelname}_prompt{selected_prompt_number}.txt\")\n",
    "\n",
    "# ========================== ENSURE FILES EXIST ========================== #\n",
    "def ensure_file_exists(file_path, header_text=None):\n",
    "    \"\"\"Creates the file if it does not exist and optionally writes a header.\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        with open(file_path, \"w\") as f:\n",
    "            if header_text:\n",
    "                f.write(f\"{header_text}\\n\")\n",
    "                f.write(\"=\" * 60 + \"\\n\")\n",
    "ensure_file_exists(runtime_file, f\"### Runtime Log - {today_date} ###\\nStart Time: {start_time_str}\")\n",
    "ensure_file_exists(progress_log_file, f\"### Progress Log - {today_date} ###\")\n",
    "if not os.path.exists(variant_output_file_path):\n",
    "    with open(variant_output_file_path, \"w\") as f:\n",
    "        f.write(\"PaperId,PaperTitle,Abstract,LLM_Prompt,LLM_Response\\n\")\n",
    "\n",
    "# ========================== LOGGING SETUP ========================== #\n",
    "logging.basicConfig(\n",
    "    filename=progress_log_file,\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "print(\"Success! All necessary files and directories are set up.\")\n",
    "print(\"Script Start Time:\", start_time_str)\n",
    "print(\"Defined prompt number:\", selected_prompt_number)\n",
    "print(\"Defined batch size:\", BATCH_SIZE)\n",
    "\n",
    "# ========================== FUNCTION DEFINITIONS ========================== #\n",
    "def screen_publication_for_variants(row, prompt_number):\n",
    "    \"\"\"Generates a dynamic prompt based on the selected prompt number.\"\"\"\n",
    "    title = row['PaperTitle']\n",
    "    abstract = row['Abstract']\n",
    "    if prompt_number not in PROMPTS:\n",
    "        raise ValueError(f\"Invalid prompt number: {prompt_number}. Choose between 0-5.\")\n",
    "    return PROMPTS[prompt_number](title, abstract)\n",
    "\n",
    "def process_with_llm(prompt):\n",
    "    \"\"\"Process the given prompt using the LLM model.\"\"\"\n",
    "    try:\n",
    "        response = generateFromPrompt(prompt)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        logging.error(f\"LLM processing error: {e}\")\n",
    "        return \"ERROR\"\n",
    "\n",
    "print(\"Functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470b4e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================== RESUME FROM LAST CHECKPOINT ========================== #\n",
    "if 'articles' not in globals():\n",
    "    raise ValueError(\"Dataset `articles` is not loaded in memory. Make sure it's defined before running the script.\")\n",
    "if 'PaperId' not in articles.columns:\n",
    "    raise KeyError(\"Dataset must contain a 'PaperId' column to track progress.\")\n",
    "if os.path.exists(variant_output_file_path):\n",
    "    processed_df = pd.read_csv(variant_output_file_path)\n",
    "    processed_articles = set(processed_df['PaperId'])  # Track completed articles\n",
    "    total_processed_articles = len(processed_articles)  # Total processed so far\n",
    "    print(f\"Resuming from last processed row. {total_processed_articles} articles completed so far.\")\n",
    "else:\n",
    "    processed_articles = set()\n",
    "    total_processed_articles = 0\n",
    "    print(\"Starting fresh processing.\")\n",
    "total_batches = (len(articles) // BATCH_SIZE) + (1 if len(articles) % BATCH_SIZE != 0 else 0)\n",
    "if total_processed_articles == len(articles):\n",
    "    print(\"\\nAll batches are complete. No more articles to process.\")\n",
    "    print(\"You have successfully processed the entire dataset.\")\n",
    "    try:\n",
    "        sys.exit(0) \n",
    "    except SystemExit:\n",
    "        pass\n",
    "unprocessed_df = articles[~articles['PaperId'].isin(processed_articles)]\n",
    "total_articles = len(unprocessed_df)    \n",
    "    \n",
    "print(\"Success! All necessary files and directories are set up.\")\n",
    "print(\"Defined prompt number:\", selected_prompt_number)\n",
    "print(\"Defined batch size to run in chunks:\", BATCH_SIZE)\n",
    "print(f\"Total unprocessed articles: {total_articles}\")\n",
    "\n",
    "# ========================== TRACK CUMULATIVE RUNTIME ========================== #\n",
    "# Load previous runtime if exists\n",
    "if os.path.exists(runtime_file):\n",
    "    with open(runtime_file, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        total_runtime_previous = sum(float(line.split(\":\")[-1].strip().split()[0])\n",
    "                                     for line in lines if \"Total runtime so far\" in line)\n",
    "else:\n",
    "    total_runtime_previous = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c72a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================== BATCH PROCESSING ========================== #\n",
    "start_time = time.time()\n",
    "os.chdir(working_directory)\n",
    "\n",
    "# Calculate the next batch number\n",
    "batch_number = (total_processed_articles // BATCH_SIZE) + 1\n",
    "for batch_start in range(0, total_articles, BATCH_SIZE):\n",
    "    batch_end = min(batch_start + BATCH_SIZE, total_articles)\n",
    "    batch = unprocessed_df.iloc[batch_start:batch_end].copy()\n",
    "    print(f\"\\nProcessing Batch {batch_number}/{total_batches} ({batch_start + 1} to {batch_end})...\")\n",
    "    batch_start_time = time.time()\n",
    "    batch['LLM_Prompt'] = batch.apply(lambda row: screen_publication_for_variants(row, selected_prompt_number), axis=1)\n",
    "    llm_response_column = f'LLM_Response_{modelname}'\n",
    "    batch[llm_response_column] = batch['LLM_Prompt'].progress_apply(process_with_llm)\n",
    "    batch_runtime = time.time() - batch_start_time\n",
    "    batch_to_save = batch[['PaperId', 'PaperTitle', 'Abstract', 'LLM_Prompt', llm_response_column]]\n",
    "    def generate_progress_bar(percentage, bar_length=20):\n",
    "        filled_length = int(bar_length * percentage / 100)\n",
    "        bar = '|' * filled_length + '-' * (bar_length - filled_length)\n",
    "        return f\"[{bar}] {percentage:.2f}%\"\n",
    "\n",
    "    total_articles = batch_end + len(processed_articles)  # Updated for total count\n",
    "    total_articles_to_process = len(unprocessed_df) - batch_end  # Remaining articles\n",
    "    processed_percentage = (total_articles / len(articles)) * 100\n",
    "    to_process_percentage = (total_articles_to_process / len(articles)) * 100\n",
    "\n",
    "    if os.path.exists(variant_output_file_path):\n",
    "        batch_to_save.to_csv(variant_output_file_path, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        batch_to_save.to_csv(variant_output_file_path, mode='w', index=False)\n",
    "    total_runtime_so_far = total_runtime_previous + (time.time() - start_time)\n",
    "\n",
    "    with open(runtime_file, \"a\") as f:\n",
    "        f.write(f\"\\nBatch {batch_number}/{total_batches} started at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Batch Runtime: {batch_runtime:.2f} sec\\n\")\n",
    "        f.write(f\"Total runtime so far (all runs combined): {total_runtime_so_far:.2f} sec\\n\")\n",
    "        f.write(f\"Total articles processed in this batch: {batch_end}\\n\")\n",
    "        f.write(\"=\" * 60 + \"\\n\")\n",
    "    logging.info(f\"Processed batch {batch_number}/{total_batches} in {batch_runtime:.2f} sec.\")\n",
    "\n",
    "    if processed_percentage >= 100:\n",
    "        print(\"\\nAll articles have been successfully processed.\")\n",
    "        print(\"No more articles remaining.\")\n",
    "        break\n",
    "    else:\n",
    "        print(f\"\\nPaused! {batch_end} articles processed in this batch.\")\n",
    "        print(f\"{total_articles} articles processed in total {generate_progress_bar(processed_percentage)}\")\n",
    "        print(f\"{total_articles_to_process} articles to process in total {generate_progress_bar(to_process_percentage)}\")\n",
    "        print(\"Check the CSV and runtime file. When ready, rerun the script to continue processing.\")\n",
    "        break \n",
    "\n",
    "# ========================== FINAL SUMMARY ========================== #\n",
    "total_runtime = total_runtime_so_far\n",
    "total_hours = total_runtime // 3600\n",
    "total_minutes = (total_runtime % 3600) // 60\n",
    "total_seconds = total_runtime % 60\n",
    "\n",
    "summary_text = f\"\"\"\n",
    "### Genetic Variant Extraction Summary ###\n",
    "\n",
    "- Model used: {modelname}\n",
    "- Prompt number: {selected_prompt_number}\n",
    "- Total batches processed: {batch_number}/{total_batches}\n",
    "- Total rows processed: {total_articles}\n",
    "- Cumulative runtime: {total_runtime:.2f} seconds ({total_hours:.0f} hr {total_minutes:.0f} min {total_seconds:.2f} sec)\n",
    "\"\"\"\n",
    "print(summary_text)\n",
    "\n",
    "with open(runtime_file, \"a\") as f:\n",
    "    f.write(\"\\n### Final runtime summary ###\\n\")\n",
    "    f.write(f\"End Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    f.write(summary_text)\n",
    "    f.write(\"\\n\" + \"=\" * 60 + \"\\n\")\n",
    "print(\"Final results saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd4dd3f",
   "metadata": {},
   "source": [
    "# **--> Rerun \"FROM LAST CHECKPOINT**\" to continue batch processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6050617",
   "metadata": {},
   "source": [
    "# 5) Make binary matrix for gene identification evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf6eff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_file = \"ICIMTH_LLM_variant_extraction_gpt4o_prompt1.csv\"\n",
    "os.chdir(working_directory)\n",
    "LLM_variant_df = pd.read_csv(LLM_file)\n",
    "print(\"\\nFinal DataFrame Preview:\")\n",
    "print(LLM_variant_df.head(50))\n",
    "print(\"\\nLength of dataframe\",len(LLM_variant_df))\n",
    "\n",
    "print(\"\\nColumns in the final DataFrame:\")\n",
    "print(LLM_variant_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38eb274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================== CONFIGURATION ========================== #\n",
    "if 'LLM_variant_df' not in globals():\n",
    "    raise ValueError(\"LLM_variant_df is not defined. Please ensure batch processing is completed.\")\n",
    "llm_response_column = 'LLM_Response'\n",
    "required_columns = [\"PaperId\", \"PaperTitle\", \"Abstract\", llm_response_column]\n",
    "missing_columns = [col for col in required_columns if col not in LLM_variant_df.columns]\n",
    "if missing_columns:\n",
    "    raise KeyError(f\"Missing columns in dataset: {missing_columns}\")\n",
    "\n",
    "# ========================== NORMALIZATION FUNCTION ============================================= #\n",
    "expanded_gene_list = {gene.upper(): {gene.upper()} for gene in gene_list}\n",
    "print(f\"Expanded gene list contains {len(expanded_gene_list)} genes.\")\n",
    "def normalize_extracted_entities(found_terms):\n",
    "    \"\"\"Normalize extracted genes using fuzzy matching against `gene_list`.\"\"\"\n",
    "    normalized_entities = set()\n",
    "    for term in found_terms:\n",
    "        term_upper = term.upper()\n",
    "\n",
    "        if term_upper in expanded_gene_list:\n",
    "            normalized_entities.add(term_upper)\n",
    "        else:\n",
    "            match = process.extractOne(term_upper, expanded_gene_list.keys(), scorer=fuzz.ratio)\n",
    "            if match:\n",
    "                best_match, score = match[:2]\n",
    "                if score > 85:\n",
    "                    normalized_entities.add(best_match)\n",
    "    return normalized_entities\n",
    "\n",
    "# ========================== GENE EXTRACTION FUNCTION ======================================== #\n",
    "def extract_genes_from_text(text):\n",
    "    \"\"\"Extract gene mentions from LLM response text.\"\"\"\n",
    "    if pd.isna(text) or len(text.strip()) == 0:\n",
    "        return set()\n",
    "    words = text.replace(\",\", \"\").split()\n",
    "    extracted_terms = set(word.upper() for word in words if word.upper() in expanded_gene_list)\n",
    "    return normalize_extracted_entities(extracted_terms)\n",
    "\n",
    "# ========================== PROCESSING LLM RESPONSES ======================================== #\n",
    "start_time = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "start_timestamp = time.time()\n",
    "print(f\"Processing {len(LLM_variant_df)} articles for gene extraction. Started at {start_time}\")\n",
    "tqdm.pandas(desc=\"Extracting genes from LLM responses\")\n",
    "LLM_variant_df[\"Extracted_Genes\"] = LLM_variant_df[llm_response_column].progress_apply(extract_genes_from_text)\n",
    "LLM_variant_df[\"Extracted_Genes\"] = LLM_variant_df[\"Extracted_Genes\"].apply(lambda genes: \", \".join(genes) if genes else \"\")\n",
    "\n",
    "# ========================== CREATE BINARY MATRIX ============================================= #\n",
    "print(\"Creating binary gene presence matrix...\")\n",
    "LLM_variant_df[\"Extracted_Gene_List\"] = LLM_variant_df[\"Extracted_Genes\"].apply(lambda x: x.split(\", \") if isinstance(x, str) else [])\n",
    "binary_gene_data = {gene: LLM_variant_df[\"Extracted_Gene_List\"].apply(lambda genes: 1 if gene in genes else 0) for gene in gene_list}\n",
    "binary_gene_df = pd.DataFrame(binary_gene_data)\n",
    "LLM_variant_df = pd.concat([LLM_variant_df, binary_gene_df], axis=1)\n",
    "LLM_variant_df[\"Sum_Entity_Mentions\"] = binary_gene_df.sum(axis=1)\n",
    "\n",
    "# ========================== SAVE RESULTS ==================================================== #\n",
    "output_filename = os.path.join(working_directory, \"LLM_evaluation_gpt4o.csv\")\n",
    "LLM_variant_df.drop(columns=[\"Extracted_Gene_List\"], errors=\"ignore\").to_csv(output_filename, index=False)\n",
    "print(f\"\\nGene extraction complete! Results saved as: {output_filename}\")\n",
    "\n",
    "# ========================== GENERATE SUMMARY =============================================== #\n",
    "summary_results = LLM_variant_df[\"Sum_Entity_Mentions\"].sum()\n",
    "print(\"\\n### Gene Extraction Summary ###\")\n",
    "print(f\"Total gene mentions found: {summary_results}\")\n",
    "summary_file = os.path.join(working_directory, \"LLM_Gene_Extraction_Summary.txt\")\n",
    "with open(summary_file, \"w\") as f:\n",
    "    f.write(\"### Gene Extraction Summary ###\\n\")\n",
    "    f.write(f\"Total gene mentions found: {summary_results}\\n\")\n",
    "print(f\"\\nExtraction summary saved in: {summary_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
