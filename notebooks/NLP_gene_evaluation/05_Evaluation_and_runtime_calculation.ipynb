{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6e09ddd",
   "metadata": {},
   "source": [
    "# Evaluation of gene name extraction and runtime calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732021a4",
   "metadata": {},
   "source": [
    "# 1) Install libraries and load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2c9f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "print(\"Import successful!\")\n",
    "print(\"Current Python version:\", sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761c47ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the working directory and file paths\n",
    "working_directory = \"WORKING_DIRECTORY\"\n",
    "input_directory = \"INPUT_DIRECTORY\"\n",
    "output_directory = \"OUTPUT_DIRECTORY\"\n",
    "articles_file = \"articles.csv\"\n",
    "genes_file = \"genes.csv\"\n",
    "\n",
    "# Change the working directory\n",
    "os.chdir(output_directory)\n",
    "print(\"Current Working Directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa726281",
   "metadata": {},
   "source": [
    "# 2) Merge all datasets and vizualize performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a51ad94",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load and merge datasets\n",
    "os.chdir(working_directory)\n",
    "print(\"Current Working Directory:\", os.getcwd())\n",
    "\n",
    "file_mapping = {\n",
    "    \"human_evaluation_results.csv\": \"Human\",\n",
    "    \"string_matching_evaluation_results.csv\": \"String_Matching\",\n",
    "    \"scispacy_evaluation_bionlp13cg.csv\": \"bionlp13cg\",\n",
    "    \"scispacy_evaluation_jnlpba.csv\": \"jnlpba\",\n",
    "    \"scispacy_evaluation_craft.csv\": \"craft\",\n",
    "    \"BioBERT_evaluation_results.csv\": \"BioBERT\",\n",
    "    \"LLM_evaluation_gpt4o.csv\": \"gpt4o\"\n",
    "}\n",
    "\n",
    "datasets = {}\n",
    "for file, model in file_mapping.items():\n",
    "    file_path = os.path.join(working_directory, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    datasets[model] = df\n",
    "\n",
    "sum_entity_mentions = {\n",
    "    model: df[\"Sum_Entity_Mentions\"].sum()\n",
    "    for model, df in datasets.items()\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(sum_entity_mentions.keys(), sum_entity_mentions.values(), color=\"blue\", alpha=0.7)\n",
    "plt.xlabel(\"Approach\")\n",
    "plt.ylabel(\"Total Extracted Genes\")\n",
    "plt.title(\"Total Number of Extracted Genes Per Approach\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ef8a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create gene count matrix\n",
    "os.chdir(working_directory)\n",
    "print(\"Current Working Directory:\", os.getcwd())\n",
    "\n",
    "file_mapping = {\n",
    "    \"human_evaluation_results.csv\": \"Human\",\n",
    "    \"string_matching_evaluation_results.csv\": \"String_Matching\",\n",
    "    \"scispacy_evaluation_bionlp13cg.csv\": \"bionlp13cg\",\n",
    "    \"scispacy_evaluation_jnlpba.csv\": \"jnlpba\",\n",
    "    \"scispacy_evaluation_craft.csv\": \"craft\",\n",
    "    \"BioBERT_evaluation_results.csv\": \"BioBERT\",\n",
    "    \"LLM_evaluation_gpt4o.csv\": \"gpt4o\"\n",
    "}\n",
    "\n",
    "datasets = {}\n",
    "for file, model in file_mapping.items():\n",
    "    file_path = os.path.join(working_directory, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    df.columns = df.columns.str.strip()\n",
    "    datasets[model] = df\n",
    "\n",
    "print(\"\\n All datasets loaded successfully!\")\n",
    "\n",
    "os.chdir(input_directory)\n",
    "genes = pd.read_csv(genes_file, header=None)\n",
    "gene_list = genes[0].tolist()\n",
    "print(\" Genes import successful!\")\n",
    "os.chdir(working_directory)\n",
    "\n",
    "human_df = datasets[\"Human\"]\n",
    "actual_gene_columns = [gene for gene in gene_list if gene in human_df.columns]\n",
    "gene_count_matrix = pd.DataFrame(index=actual_gene_columns)\n",
    "\n",
    "for model, df in datasets.items():\n",
    "    gene_counts = df[actual_gene_columns].sum()\n",
    "    gene_count_matrix[model] = gene_counts\n",
    "\n",
    "output_file = os.path.join(working_directory, \"final_evaluation_matrix.csv\")\n",
    "gene_count_matrix.to_csv(output_file)\n",
    "print(f\"\\n Gene count matrix saved as '{output_file}'\")\n",
    "\n",
    "print(gene_count_matrix.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359b4fcc",
   "metadata": {},
   "source": [
    "# 3) Create evaluation matrix and figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9fff50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion and evaluation matrix\n",
    "os.chdir(working_directory)\n",
    "print(\"Current Working Directory:\", os.getcwd())\n",
    "file_mapping = {\n",
    "    \"human_evaluation_results.csv\": \"Human\",\n",
    "    \"string_matching_evaluation_results.csv\": \"String_Matching\",\n",
    "    \"scispacy_evaluation_bionlp13cg.csv\": \"bionlp13cg\",\n",
    "    \"scispacy_evaluation_jnlpba.csv\": \"jnlpba\",\n",
    "    \"scispacy_evaluation_craft.csv\": \"craft\",\n",
    "    \"BioBERT_evaluation_results.csv\": \"BioBERT\",\n",
    "    \"LLM_evaluation_gpt4o.csv\": \"gpt4o\"\n",
    "}\n",
    "\n",
    "datasets = {}\n",
    "for file, model in file_mapping.items():\n",
    "    file_path = os.path.join(working_directory, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    df.columns = df.columns.str.strip()\n",
    "    datasets[model] = df\n",
    "\n",
    "print(\"\\n All datasets loaded successfully!\")\n",
    "\n",
    "os.chdir(input_directory)\n",
    "genes = pd.read_csv(genes_file, header=None)\n",
    "gene_list = genes[0].tolist()\n",
    "print(\" Genes import successful!\")\n",
    "os.chdir(working_directory)\n",
    "\n",
    "human_df = datasets[\"Human\"]\n",
    "actual_gene_columns = [gene for gene in gene_list if gene in human_df.columns]\n",
    "\n",
    "gene_count_matrix = pd.DataFrame(index=actual_gene_columns)\n",
    "\n",
    "for model, df in datasets.items():\n",
    "    gene_counts = df[actual_gene_columns].sum()\n",
    "    gene_count_matrix[model] = gene_counts\n",
    "\n",
    "gene_count_matrix = gene_count_matrix.fillna(0).astype(int)\n",
    "human_counts = gene_count_matrix[\"Human\"]\n",
    "\n",
    "evaluation_results = {}\n",
    "\n",
    "for model, df in datasets.items():\n",
    "    if model == \"Human\":\n",
    "        continue\n",
    "    model_counts = gene_count_matrix[model]\n",
    "    TP = (model_counts.where(model_counts <= human_counts, human_counts)).sum()\n",
    "    FP = (model_counts - human_counts).clip(lower=0).sum()\n",
    "    FN = (human_counts - model_counts).clip(lower=0).sum()\n",
    "    TN = ((model_counts == 0) & (human_counts == 0)).sum()\n",
    "\n",
    "    sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN) if (TP + TN + FP + FN) > 0 else 0\n",
    "    f1_score = 2 * (precision * sensitivity) / (precision + sensitivity) if (precision + sensitivity) > 0 else 0\n",
    "    jaccard_index = TP / (TP + FP + FN) if (TP + FP + FN) > 0 else 0\n",
    "\n",
    "    evaluation_results[model] = {\n",
    "        \"True Positives\": TP,\n",
    "        \"False Positives\": FP,\n",
    "        \"False Negatives\": FN,\n",
    "        \"True Negatives\": TN,\n",
    "        \"Sensitivity (recall)\": sensitivity,\n",
    "        \"Specificity\": specificity,\n",
    "        \"Precision\": precision,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"F1-Score\": f1_score,\n",
    "        \"Jaccard index\": jaccard_index,\n",
    "    }\n",
    "\n",
    "evaluation_df = pd.DataFrame.from_dict(evaluation_results, orient=\"index\")\n",
    "output_file = os.path.join(working_directory, \"confusion_matrix_evaluation.csv\")\n",
    "evaluation_df.to_csv(output_file)\n",
    "print(f\"\\n Evaluation results saved as '{output_file}'\")\n",
    "print(evaluation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64025798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create heatmap figure\n",
    "metrics_data = evaluation_df.iloc[:, 4:]\n",
    "vmin = metrics_data.min().min()\n",
    "vmax = metrics_data.max().max()\n",
    "\n",
    "custom_cmap = LinearSegmentedColormap.from_list(\n",
    "    \"orange_to_green\",\n",
    "    [\"#fbe8a6\", \"#a3c586\", \"#004d00\"]\n",
    ")\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.set(style=\"whitegrid\")\n",
    "heatmap = sns.heatmap(\n",
    "    metrics_data,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap=custom_cmap,\n",
    "    linewidths=0.5,\n",
    "    annot_kws={\"size\": 9},\n",
    "    vmin=vmin,\n",
    "    vmax=vmax\n",
    ")\n",
    "\n",
    "cbar = heatmap.collections[0].colorbar\n",
    "cbar.ax.tick_params(labelsize=8)\n",
    "plt.title(\"Model evaluation (compared to ground truth)\", fontsize=10, color='black')\n",
    "plt.ylabel(\"Model\", fontsize=9, color='black', labelpad=25)\n",
    "plt.xlabel(\"Performance metrics\", fontsize=9, color='black')\n",
    "plt.xticks(rotation=45, fontsize=8, color='black')\n",
    "heatmap.tick_params(axis='x', pad=-3)\n",
    "\n",
    "for label in heatmap.get_xticklabels():\n",
    "    label.set_ha('right')\n",
    "\n",
    "heatmap.tick_params(\n",
    "    axis='x',\n",
    "    which='both',\n",
    "    bottom=True,\n",
    "    top=False,\n",
    "    length=5,\n",
    "    width=1,\n",
    "    direction='out',\n",
    "    pad=2\n",
    ")\n",
    "\n",
    "plt.yticks(fontsize=8, color='black')\n",
    "plt.tight_layout()\n",
    "custom_y_labels = ['GPT-4o (LLM)' if label.get_text() == 'gpt4o' else label.get_text() for label in heatmap.get_yticklabels()]\n",
    "custom_y_labels = ['Sting-matching' if label.get_text() == 'String_Matching' else label.get_text() for label in heatmap.get_yticklabels()]\n",
    "heatmap.set_yticklabels(custom_y_labels, rotation=0, fontsize=8, color='black')\n",
    "plt.savefig(\"heatmap_confusion_matrix.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.savefig(\"heatmap_confusion_matrix.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de10e85a",
   "metadata": {},
   "source": [
    "# 4) Calculate runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee826eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory and load the CSV\n",
    "os.chdir(working_directory)\n",
    "runtime = pd.read_csv(\"model_runtimes.csv\")\n",
    "print(runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ab7861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart creation for runtime\n",
    "runtime[\"model\"] = runtime[\"model\"].replace({\n",
    "    \"LLM\": \"LLM\",\n",
    "    \"BioBERT\": \"BioBERT\",\n",
    "    \"scispaCy\": \"SciSpaCy\",\n",
    "    \"String-matching\": \"String-matching\"\n",
    "})\n",
    "\n",
    "model_order = [\"String-matching\", \"SciSpaCy\", \"BioBERT\", \"LLM\"]\n",
    "runtime[\"model\"] = pd.Categorical(runtime[\"model\"], categories=model_order, ordered=True)\n",
    "runtime_sorted = runtime.sort_values(\"model\")\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "ax = sns.barplot(\n",
    "    data=runtime_sorted,\n",
    "    y=\"model\",\n",
    "    x=\"min\",\n",
    "    color=\"darkorange\",\n",
    "    height=0.6\n",
    ")\n",
    "\n",
    "for i, v in enumerate(runtime_sorted[\"min\"]):\n",
    "    ax.text(v + 10, i, f\"{int(v):,}\", color='black', va='center', fontsize=8)\n",
    "ax.xaxis.set_major_formatter(mticker.FuncFormatter(lambda x, _: f'{int(x):,}'))\n",
    "x_max = runtime_sorted[\"min\"].max()\n",
    "plt.xlim(0, x_max * 1.2)\n",
    "\n",
    "plt.title(\"Runtime to process 100,000 articles\", fontsize=10, color='black', loc='left', x=-0.05)\n",
    "plt.xlabel(\"Time (minutes)\", fontsize=9, color='black')\n",
    "plt.ylabel(\"Model\", fontsize=9, color='black')\n",
    "plt.xticks(fontsize=8, color='black')\n",
    "plt.yticks(fontsize=8, color='black')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"barplot_model_runtimes_narrow.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.savefig(\"barplot_model_runtimes_narrow.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
