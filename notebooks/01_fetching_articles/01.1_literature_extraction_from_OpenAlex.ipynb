{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f069300",
   "metadata": {},
   "source": [
    "# Extract literature from OpenAlex\n",
    "- Fetch literature (Paper title, abstract and metadata) from OpenAlex API\n",
    "- Define search string: search_term = '(\"keywordA\" OR \"keywordb\" OR \"keywordc\")'\n",
    "- Define time range of relevant publications: publication_year_range = '2014-2024'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01104317",
   "metadata": {},
   "source": [
    "## Set up of environment\n",
    "- Import libraries\n",
    "- Define working directory\n",
    "- Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227883ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import nltk\n",
    "from collections import namedtuple\n",
    "from nltk.corpus import brown\n",
    "import time\n",
    "import re\n",
    "import urllib.parse\n",
    "import html\n",
    "import logging\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "nltk.download('brown')\n",
    "\n",
    "print(\"All imports successful.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a644797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print current working directory\n",
    "print(\"Current Working Directory:\", os.getcwd())\n",
    "\n",
    "# Global Flags\n",
    "FETCH_PAPERS = True  # Toggle for paper fetching\n",
    "ALLOW_DOWNLOAD = True  # Toggle for downloading functionality\n",
    "LLM_EXECUTION = True  # Toggle for LLM-based tasks\n",
    "\n",
    "# Define the base directory for file handling\n",
    "base_folder = os.path.join(os.getcwd(), 'FOLDER NAME')\n",
    "\n",
    "# Check if FOLDER NAME folder exists, create if not\n",
    "if not os.path.exists(base_folder):\n",
    "    os.makedirs(base_folder, exist_ok=True)\n",
    "    print(f\"Folder 'FOLDER NAME' created at: {base_folder}\")\n",
    "else:\n",
    "    print(f\"Folder 'FOLDER NAME' already exists at: {base_folder}\")\n",
    "\n",
    "# Define path for Analysis subfolder\n",
    "analysis_folder = os.path.join(base_folder, 'Analysis')\n",
    "\n",
    "# Check if Analysis folder exists, create if not\n",
    "if not os.path.exists(analysis_folder):\n",
    "    os.makedirs(analysis_folder, exist_ok=True)\n",
    "    print(f\"Folder 'Analysis' created at: {analysis_folder}\")\n",
    "else:\n",
    "    print(f\"Folder 'Analysis' already exists at: {analysis_folder}\")\n",
    "\n",
    "# Logging setup\n",
    "error_log_file = os.path.join(base_folder, 'fetch_errors.log')\n",
    "logging.basicConfig(filename=error_log_file, level=logging.ERROR)\n",
    "\n",
    "# Confirm save location\n",
    "print(f\"Logging and outputs will be saved in: {base_folder}\")\n",
    "\n",
    "# Get today's date in YYYY-MM-DD format\n",
    "today_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "print(f\"Today's date: {today_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fba646",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define helper functions\n",
    "def to_abstract(paper):\n",
    "    \"\"\"Reconstruct abstract from inverted index in OpenAlex data.\"\"\"\n",
    "    if 'abstract_inverted_index' in paper and paper['abstract_inverted_index']:\n",
    "        words = sorted(\n",
    "            [(k, index) for k, v in paper['abstract_inverted_index'].items() for index in v],\n",
    "            key=lambda x: x[1]\n",
    "        )\n",
    "        return ' '.join([word[0] for word in words])\n",
    "    return ''\n",
    "\n",
    "def is_english(text, brown_corpus, threshold=0.15):\n",
    "    \"\"\"Check if text is in English using word overlap with Brown corpus.\"\"\"\n",
    "    if not text:\n",
    "        return 'EMPTY', 'NA'\n",
    "    tokens = set(re.findall(r\"\\w+\", text.lower()))\n",
    "    lang_ratio = len(tokens & brown_corpus.value) / len(tokens) if tokens else 0\n",
    "    return 'PASS', 'en' if lang_ratio > threshold else 'non-en'\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean text by decoding HTML entities, removing HTML tags, and handling encoding artifacts.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        try:\n",
    "            # Decode HTML entities\n",
    "            text = html.unescape(text)\n",
    "            # Remove HTML tags\n",
    "            text = re.sub(r'<[^>]+>', '', text)\n",
    "            # Replace problematic characters\n",
    "            text = (text.replace('‚Äê', '—')\n",
    "                        .replace('‚Äì', '-')\n",
    "                        .replace('‚Äî', '–')\n",
    "                        .replace('‚Äò', \"'\")\n",
    "                        .replace('‚Äô', \"'\")\n",
    "                        .replace('‚Ä¢', '•')\n",
    "                        .replace('‚Äû', '\"')\n",
    "                        .replace('‚Äú', '\"')\n",
    "                        .replace('‚Ä¶', '…')\n",
    "                        .replace('¬†', ' ')\n",
    "                        .replace('√∫', 'ú')\n",
    "                        .replace('√©', 'é')\n",
    "                        .replace('√±', 'ñ')\n",
    "                        .replace('√≥', 'ó')\n",
    "                        .replace('√∂', 'ö')\n",
    "                        .replace('√', '')\n",
    "                        .replace('‚Ä', '')\n",
    "                        .replace('‚', ''))\n",
    "            # Normalize whitespace\n",
    "            text = re.sub(r'\\s+', ' ', text).strip()\n",
    "            # Remove non-ASCII characters\n",
    "            text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Text cleaning failed: {e}\")\n",
    "            return \"\"\n",
    "    return text\n",
    "\n",
    "def format_ids(items, base_url):\n",
    "    \"\"\"Format IDs from items.\"\"\"\n",
    "    return '|'.join([clean_text(item['id'].replace(base_url, '')) for item in items if 'id' in item])\n",
    "\n",
    "def format_authors(authorships):\n",
    "    \"\"\"Extract and clean author names.\"\"\"\n",
    "    return ', '.join([clean_text(auth['author']['display_name']) for auth in authorships if 'author' in auth])\n",
    "\n",
    "def format_citations(referenced_works):\n",
    "    \"\"\"Format citations for the paper.\"\"\"\n",
    "    return '|'.join([clean_text(ref.replace('https://openalex.org/W', '')) for ref in referenced_works])\n",
    "\n",
    "def fetch_with_retries(url, headers, retries=3, delay=2):\n",
    "    \"\"\"Fetch data with retry logic.\"\"\"\n",
    "    for attempt in range(retries):\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            return response\n",
    "        print(f\"Attempt {attempt + 1} failed with status code {response.status_code}. Retrying in {delay} seconds...\")\n",
    "        time.sleep(delay)\n",
    "    return None\n",
    "\n",
    "def fetch_articles(base_url, headers):\n",
    "    \"\"\"Fetch articles using the OpenAlex API.\"\"\"\n",
    "    cursor = '*'\n",
    "    papers_list = []\n",
    "    total_pages = None\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    with tqdm(desc=\"Fetching data\", unit=\"page\") as pbar:\n",
    "        while cursor:\n",
    "            response = fetch_with_retries(f\"{base_url}&per_page=100&cursor={cursor}\", headers)\n",
    "            if response and response.status_code == 200:\n",
    "                data = response.json()\n",
    "\n",
    "                # Initialize total_pages on the first successful response\n",
    "                if total_pages is None and 'meta' in data and 'count' in data['meta']:\n",
    "                    total_results = data['meta']['count']\n",
    "                    total_pages = (total_results // 100) + (1 if total_results % 100 > 0 else 0)\n",
    "                    print(f\"Total number of pages to process: {total_pages}\")\n",
    "                    pbar.total = total_pages\n",
    "\n",
    "                for paper in data.get('results', []):\n",
    "                    try:\n",
    "                        abstract = clean_text(to_abstract(paper))\n",
    "                        title = clean_text(paper.get('title', ''))\n",
    "                        pub_date = clean_text(paper.get('publication_date', ''))\n",
    "                        pub_year = str(paper.get('publication_year', ''))\n",
    "\n",
    "                        status, lang = is_english(f\"{title} {abstract}\", bc_brown)\n",
    "                        if lang == 'en':\n",
    "                            authors = format_authors(paper.get('authorships', []))\n",
    "                            citations = format_citations(paper.get('referenced_works', []))\n",
    "                            concepts = format_ids(paper.get('concepts', []), 'https://openalex.org/C')\n",
    "\n",
    "                            papers_list.append({\n",
    "                                'PaperId': clean_text(paper['id'].replace('https://openalex.org/W', '')),\n",
    "                                'PaperTitle': title,\n",
    "                                'Citations': citations,\n",
    "                                'c': concepts,\n",
    "                                'Authors': authors,\n",
    "                                'Abstract': abstract,\n",
    "                                'Language': 'en',\n",
    "                                'PubYear': pub_year,\n",
    "                                'PubDate': pub_date\n",
    "                            })\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"Error processing paper: {e}\")\n",
    "\n",
    "                cursor = data['meta'].get('next_cursor')\n",
    "                pbar.update(1) \n",
    "                time.sleep(1.5)  # Delay to avoid hitting rate limits!\n",
    "            else:\n",
    "                print(\"Failed to fetch data or rate limit exceeded.\")\n",
    "                break\n",
    "\n",
    "    total_time_minutes = round((time.time() - start_time) / 60, 2)\n",
    "    print(f\"Total fetch time: {total_time_minutes} minutes.\")\n",
    "    return papers_list\n",
    "\n",
    "print(\"Helper functions successfully defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7877f53",
   "metadata": {},
   "source": [
    "## Run fetching of publications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0baaae9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize Brown corpus for language detection\n",
    "Brown = namedtuple(\"Brown\", field_names=['value'])\n",
    "bc_brown = Brown(value=set(word.lower() for word in brown.words()))\n",
    "\n",
    "# Set up email for OpenAlex API requests\n",
    "headers = {'email': 'EMAIL ADDRESS'}\n",
    "\n",
    "# Define search parameters for all kind of cancers\n",
    "search_term = '(\"cancer\" OR \"carcinoma\" OR \"tumor\")'\n",
    "publication_year_range = '2014-2024'\n",
    "encoded_search_term = urllib.parse.quote(search_term)\n",
    "base_url = (\n",
    "    f'https://api.openalex.org/works?filter=has_abstract:true,title_and_abstract.search:'\n",
    "    f'{encoded_search_term},publication_year:{publication_year_range}&sort=publication_year:desc'\n",
    ")\n",
    "\n",
    "# Set global flags for fetching papers\n",
    "FETCH_PAPERS = True\n",
    "\n",
    "# Create a folder for saving results\n",
    "drive_folder = os.path.join(os.getcwd(), 'FOLDER NAME')\n",
    "if not os.path.exists(drive_folder):\n",
    "    os.makedirs(drive_folder)\n",
    "    print(f\"Created folder: {drive_folder}\")\n",
    "else:\n",
    "    print(f\"Using existing folder: {drive_folder}\")\n",
    "\n",
    "\n",
    "# Fetch and save articles\n",
    "if FETCH_PAPERS:\n",
    "    print(\"Fetching articles...\")\n",
    "    papers = fetch_articles(base_url, headers)\n",
    "    if papers:\n",
    "        print(f\"Fetched {len(papers)} articles.\")\n",
    "        \n",
    "        # Convert articles to a DataFrame\n",
    "        df = pd.DataFrame(papers)\n",
    "\n",
    "        # Create the output filename with the current date\n",
    "        file_name = f\"search_{today_date}_articles.csv\"\n",
    "        output_file = os.path.join(drive_folder, file_name)\n",
    "\n",
    "        # Save the DataFrame to a CSV file\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"Saved articles to CSV in FOLDER NAME folder: {output_file}\")\n",
    "    else:\n",
    "        print(\"No articles fetched.\")\n",
    "else:\n",
    "    print(\"Fetching is disabled.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:llms] *",
   "language": "python",
   "name": "conda-env-llms-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
