{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6e09ddd",
   "metadata": {},
   "source": [
    "# Extract 161 Oncomine genes with BioBERT and MyGene.info API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a2c317",
   "metadata": {},
   "source": [
    "- Define genes of interest, i.e., 161 Oncomine NGS panel\n",
    "- Load OpenAlex dataset after full cleaning\n",
    "- Load BioBERT model to detect genes\n",
    "- Connect to MyGene.info API to receive gene-associated products (i.e., proteins) and synonyms\n",
    "- Extract all publications with gene mentions, and create binary matrix\n",
    "- Drop all publications without gene mentions, and create new dataset of relevant articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732021a4",
   "metadata": {},
   "source": [
    "## 1) Install libraries and load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dbef50",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "from transformers import pipeline\n",
    "# Load  BioBERT model\n",
    "biobert_model = pipeline(\"ner\", model=\"dmis-lab/biobert-base-cased-v1.1\", tokenizer=\"dmis-lab/biobert-base-cased-v1.1\")\n",
    "print(\"BioBERT model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2c9f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from fuzzywuzzy import process, fuzz\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "print(\"Import successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761c47ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the working directory and file paths\n",
    "working_directory = \"WORKING _DIRECTORY\"\n",
    "input_directory = \"INPUT_DIRECTORY\"\n",
    "output_directory = \"OUTPUT_DIRECTORY\"\n",
    "articles_file = \"CLEANED_ARTICLE_FILE.csv\"\n",
    "genes_file = \"GENS_OF_INTEREST.csv\"\n",
    "\n",
    "# Change the working directory\n",
    "os.chdir(output_directory)\n",
    "print(\"Current Working Directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d96bb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load OncoMine genes file\n",
    "output_directory \n",
    "os.chdir(input_directory)\n",
    "genes = pd.read_csv(genes_file, header=None)\n",
    "gene_list = genes[0].tolist()\n",
    "print(\"Genes import successful!\")\n",
    "\n",
    "# Load the articles file\n",
    "os.chdir(output_directory)\n",
    "articles = pd.read_csv(articles_file)\n",
    "print(\"Article import successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9362ed0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display dataset info\n",
    "print(f\"Loaded full dataset of {len(articles):,} articles\")\n",
    "print(f\"Loaded {len(gene_list):,} oncomine genes\")\n",
    "\n",
    "# Get the number of rows and columns\n",
    "num_rows = articles.shape[0]\n",
    "num_columns = articles.shape[1]\n",
    "print(f\"\\nThe articles df contains {num_rows:,} rows and {num_columns:,} columns.\")\n",
    "\n",
    "# Display column names in the df\n",
    "column_names = articles.columns.tolist()\n",
    "\n",
    "print(\"The column names in the articles df are:\")\n",
    "for col in column_names:\n",
    "    print(col)\n",
    "    \n",
    "# Current extraction\n",
    "article_df=articles\n",
    "print(f\"\\nThis execution will be processing: {len(article_df):,} articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa12a937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BioBERT Genetic NER Model\n",
    "biobert_model = pipeline(\n",
    "    \"ner\",\n",
    "    model=\"alvaroalon2/biobert_genetic_ner\",\n",
    "    tokenizer=\"alvaroalon2/biobert_genetic_ner\",\n",
    "    device=0 if torch.cuda.is_available() else -1  # Use GPU if available\n",
    ")\n",
    "print(\"BioBERT model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e88d830",
   "metadata": {},
   "source": [
    "## 2) Extract gene mentions in articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760d7def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch gene synonyms and associated proteins (from MyGene.info API)\n",
    "def get_gene_synonyms(gene_symbol):\n",
    "    \"\"\"Fetches known synonyms, including protein products, for a given gene from MyGene.info.\"\"\"\n",
    "    url = f\"https://mygene.info/v3/query?q={gene_symbol}&fields=symbol,alias,other_names\"\n",
    "    try:\n",
    "        response = requests.get(url).json()\n",
    "        synonyms = set()\n",
    "        for hit in response.get(\"hits\", []):\n",
    "            if \"symbol\" in hit:\n",
    "                synonyms.add(hit[\"symbol\"].upper())\n",
    "            if \"alias\" in hit:\n",
    "                synonyms.update([alias.upper() for alias in hit[\"alias\"]]) \n",
    "            if \"other_names\" in hit:\n",
    "                synonyms.update([name.upper() for name in hit[\"other_names\"]])\n",
    "        return synonyms\n",
    "    except:\n",
    "        return {gene_symbol.upper()}\n",
    "\n",
    "\n",
    "# Step 1: Expand the gene list dynamically with synonyms and protein names\n",
    "expanded_gene_list = {}\n",
    "for gene in gene_list:\n",
    "    expanded_gene_list[gene.upper()] = get_gene_synonyms(gene)\n",
    "print(f\"Expanded gene list contains {len(expanded_gene_list)} genes with synonyms.\")\n",
    "\n",
    "\n",
    "# Function to normalize and reconstruct gene names from BioBERT\n",
    "def normalize_extracted_genes(found_terms):\n",
    "    \"\"\"Normalize and map extracted entities to closest known gene or protein names.\"\"\"\n",
    "    normalized_genes = set()\n",
    "\n",
    "    for term in found_terms:\n",
    "        term_upper = term.upper()  # Convert to uppercase for case-insensitive matching\n",
    "\n",
    "        # Step 1: Direct match with expanded gene list\n",
    "        if term_upper in expanded_gene_list:\n",
    "            normalized_genes.add(term_upper)\n",
    "            continue\n",
    "\n",
    "        #Step 2: Handle cases with hyphens or brackets\n",
    "        cleaned_term = re.sub(r\"[\\[\\]\\(\\),-]\", \" \", term_upper)  # Remove special chars\n",
    "        cleaned_words = cleaned_term.split()  # Split into individual words\n",
    "\n",
    "        # If one of the words is a known gene, add it\n",
    "        for word in cleaned_words:\n",
    "            if word in expanded_gene_list:\n",
    "                normalized_genes.add(word)\n",
    "\n",
    "        # Step 3: Apply fuzzy matching only if no direct match\n",
    "        if not any(gene in normalized_genes for gene in cleaned_words):\n",
    "            match = process.extractOne(term_upper, expanded_gene_list.keys(), scorer=fuzz.ratio)\n",
    "            if match:\n",
    "                best_match, score = match[:2]  # Extract first two elements only\n",
    "                if score > 85:  # Set threshold for fuzzy match\n",
    "                    normalized_genes.add(best_match)\n",
    "\n",
    "    return normalized_genes\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa07a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process text with BioBERT\n",
    "def process_biobert(text, model):\n",
    "    \"\"\"Runs BioBERT NER, extracts genes, normalizes, and maps them using MyGene.info.\"\"\"\n",
    "    if pd.isna(text) or len(text.strip()) == 0:\n",
    "        return set()\n",
    "\n",
    "    results = model(text)\n",
    "    found_terms = set()\n",
    "    current_term = []\n",
    "\n",
    "    for res in results:\n",
    "        word = res[\"word\"].replace(\"##\", \"\") \n",
    "        if res[\"entity\"].startswith(\"B-\"):  \n",
    "            if current_term:\n",
    "                full_term = \"\".join(current_term)\n",
    "                found_terms.add(full_term)\n",
    "            current_term = [word]\n",
    "        elif res[\"entity\"].startswith(\"I-\"): \n",
    "            current_term.append(word)\n",
    "\n",
    "    # Add last detected term\n",
    "    if current_term:\n",
    "        full_term = \"\".join(current_term)\n",
    "        found_terms.add(full_term)\n",
    "\n",
    "    # Normalize extracted gene names\n",
    "    return normalize_extracted_genes(found_terms)\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf281f2",
   "metadata": {},
   "source": [
    "# ================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f775fe3b",
   "metadata": {},
   "source": [
    "## 3) Run gene extraction with BioBERT and sliding window approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803db384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gene extration with BioBERT and slinding window approach\n",
    "\n",
    "# Function to fetch gene synonyms from MyGene.info API\n",
    "def get_gene_synonyms(gene_symbol):\n",
    "    \"\"\"Fetches known synonyms, including protein products, for a given gene from MyGene.info.\"\"\"\n",
    "    url = f\"https://mygene.info/v3/query?q={gene_symbol}&fields=symbol,alias,other_names\"\n",
    "    try:\n",
    "        response = requests.get(url).json()\n",
    "        synonyms = set()\n",
    "        for hit in response.get(\"hits\", []):\n",
    "            if \"symbol\" in hit:\n",
    "                synonyms.add(hit[\"symbol\"].upper())\n",
    "            if \"alias\" in hit:\n",
    "                synonyms.update([alias.upper() for alias in hit[\"alias\"]])\n",
    "            if \"other_names\" in hit:\n",
    "                synonyms.update([name.upper() for name in hit[\"other_names\"]])\n",
    "        return synonyms\n",
    "    except:\n",
    "        return {gene_symbol.upper()} \n",
    "\n",
    "# Expand the gene list dynamically with synonyms\n",
    "expanded_gene_list = {gene.upper(): get_gene_synonyms(gene) for gene in gene_list}\n",
    "print(f\"Expanded gene list contains {len(expanded_gene_list)} genes with synonyms.\")\n",
    "\n",
    "# Function to normalize extracted genes\n",
    "def normalize_extracted_genes(found_terms):\n",
    "    \"\"\"Normalize and map extracted entities to closest known gene or protein names.\"\"\"\n",
    "    normalized_genes = set()\n",
    "    for term in found_terms:\n",
    "        term_upper = term.upper()\n",
    "        if term_upper in expanded_gene_list:\n",
    "            normalized_genes.add(term_upper)\n",
    "            continue\n",
    "        cleaned_term = re.sub(r\"[\\[\\]\\(\\),-]\", \" \", term_upper)\n",
    "        cleaned_words = cleaned_term.split()\n",
    "        for word in cleaned_words:\n",
    "            if word in expanded_gene_list:\n",
    "                normalized_genes.add(word)\n",
    "        if not any(gene in normalized_genes for gene in cleaned_words):\n",
    "            match = process.extractOne(term_upper, expanded_gene_list.keys(), scorer=fuzz.ratio)\n",
    "            if match:\n",
    "                best_match, score = match[:2]\n",
    "                if score > 85:\n",
    "                    normalized_genes.add(best_match)\n",
    "    return normalized_genes\n",
    "\n",
    "# Function to split text into overlapping chunks for NER\n",
    "def sliding_window_chunking(text, tokenizer, max_tokens=512, stride=256):\n",
    "    \"\"\"Splits text into overlapping chunks to avoid losing context.\"\"\"\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    if len(tokens) <= max_tokens:\n",
    "        return [tokenizer.decode(tokens, skip_special_tokens=True)]\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), stride):\n",
    "        chunk = tokens[i : i + max_tokens]\n",
    "        if len(chunk) < max_tokens:\n",
    "            break\n",
    "        chunks.append(tokenizer.decode(chunk, skip_special_tokens=True))\n",
    "    return chunks\n",
    "\n",
    "# Function to process text with BioBERT using sliding window\n",
    "def process_biobert(text, model):\n",
    "    \"\"\"Runs BioBERT NER with sliding window chunking.\"\"\"\n",
    "    if pd.isna(text) or len(text.strip()) == 0:\n",
    "        return set()\n",
    "    tokenizer = model.tokenizer  # Get tokenizer\n",
    "    text_chunks = sliding_window_chunking(text, tokenizer)\n",
    "    found_terms = set()\n",
    "    for chunk in text_chunks:\n",
    "        results = model(chunk)\n",
    "        current_term = []\n",
    "        for res in results:\n",
    "            word = res[\"word\"].replace(\"##\", \"\")\n",
    "            if res[\"entity\"].startswith(\"B-\"):\n",
    "                if current_term:\n",
    "                    full_term = \"\".join(current_term)\n",
    "                    found_terms.add(full_term)\n",
    "                current_term = [word]\n",
    "            elif res[\"entity\"].startswith(\"I-\"):\n",
    "                current_term.append(word)\n",
    "        if current_term:\n",
    "            full_term = \"\".join(current_term)\n",
    "            found_terms.add(full_term)\n",
    "    return normalize_extracted_genes(found_terms)\n",
    "\n",
    "print(\"Success!\")\n",
    "\n",
    "##### Gene extraction #####\n",
    "start_time = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "start_timestamp = time.time()\n",
    "print(f\"Processing {len(article_df)} articles with BioBERT. Started at {start_time}\")\n",
    "\n",
    "biobert_results = []\n",
    "for index, row in tqdm(article_df.iterrows(), total=len(article_df), desc=\"Processing Articles\"):\n",
    "    title = row.get(\"PaperTitle\", \"\")\n",
    "    abstract = row.get(\"Abstract\", \"\")\n",
    "    genes_biobert = process_biobert(title, biobert_model) | process_biobert(abstract, biobert_model)\n",
    "    biobert_results.append(\", \".join(genes_biobert))\n",
    "    print(f\"Article {index+1}: {genes_biobert}\")\n",
    "\n",
    "# Create df_results while keeping all columns\n",
    "df_results = article_df.copy()\n",
    "df_results[\"BioBERT\"] = biobert_results\n",
    "num_articles = len(df_results)\n",
    "\n",
    "# Generate filenames\n",
    "output_file = f\"sliding_window_filtered_articles_biobert_expanded_{num_articles}.csv\"\n",
    "runtime_file = f\"sliding_window_filtered_articles_biobert_expanded_{num_articles}_runtime.txt\"\n",
    "\n",
    "end_time = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "end_timestamp = time.time()\n",
    "total_runtime = end_timestamp - start_timestamp\n",
    "print(f\"Processing completed at {end_time}. Total runtime: {total_runtime:.2f} seconds.\")\n",
    "\n",
    "# Save runtime details in a text file\n",
    "with open(runtime_file, \"w\") as f:\n",
    "    f.write(f\"Processing of articles: {num_articles}\\n\")\n",
    "    f.write(f\"Processing started at: {start_time}\\n\")\n",
    "    f.write(f\"Processing completed at: {end_time}\\n\")\n",
    "    f.write(f\"Total runtime: {total_runtime:.2f} seconds\\n\")\n",
    "print(f\"Runtime details saved in: {runtime_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add77f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the etxraction as csv\n",
    "os.chdir(output_directory)\n",
    "output_file = f\"sliding_window_filtered_articles_biobert_expanded_{num_articles}.csv\"\n",
    "\n",
    "# Save full DataFrame to CSV, keeping all columns\n",
    "df_results.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d2e8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary matrix creation\n",
    "os.chdir(output_directory)\n",
    "\n",
    "input_file = f\"sliding_window_filtered_articles_biobert_expanded_2128318.csv\"\n",
    "BioBERT_dfslw = pd.read_csv(input_file)\n",
    "print(BioBERT_dfslw.head(20))\n",
    "\n",
    "# KEEP THE ORIGINAL BioBERT COLUMN UNCHANGED \n",
    "BioBERT_originalslw = BioBERT_dfslw[\"BioBERT\"].copy()\n",
    "BioBERT_dfslw[\"BioBERT\"] = BioBERT_dfslw[\"BioBERT\"].fillna(\"\").astype(str)\n",
    "print(BioBERT_dfslw.head(20))\n",
    "\n",
    "# Binary matrix creation\n",
    "os.chdir(output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580aded6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract ALL genes from BioBERT column correctly\n",
    "BioBERT_dfslw[\"Extracted_Genesslw\"] = BioBERT_dfslw[\"BioBERT\"].apply(lambda x: [gene.strip() for gene in x.split(',') if gene.strip()])\n",
    "\n",
    "# Create binary matrix for all genes in gene_list\n",
    "binary_gene_dataslw = {gene: BioBERT_dfslw[\"Extracted_Genesslw\"].apply(lambda genes: 1 if gene in genes else 0) for gene in gene_list}\n",
    "\n",
    "# Convert to df and merge with BioBERT_df\n",
    "binary_gene_dfslw = pd.DataFrame(binary_gene_dataslw)\n",
    "BioBERT_dfslw = pd.concat([BioBERT_dfslw, binary_gene_dfslw], axis=1)\n",
    "\n",
    "# Add a sum column to count gene mentions\n",
    "BioBERT_dfslw[\"Sum_Gene_Mentions\"] = binary_gene_dfslw.sum(axis=1)\n",
    "\n",
    "# Restore the original BioBERT column (unchanged)\n",
    "BioBERT_dfslw[\"BioBERT\"] = BioBERT_originalslw\n",
    "\n",
    "# Drop temporary extracted genes column\n",
    "BioBERT_dfslw.drop(columns=[\"Extracted_Genesslw\"], inplace=True)\n",
    "\n",
    "print(BioBERT_dfslw.head(20))\n",
    "\n",
    "# Save as CSV\n",
    "# Change the working directory\n",
    "os.chdir(output_directory)\n",
    "\n",
    "output_filename = f\"sliding_window_gene_binary_matrix_BioBERT_MyGeneinfo_{num_articles}.csv\"\n",
    "BioBERT_dfslw.to_csv(output_filename, index=False)\n",
    "\n",
    "print(f\"File saved as: {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900e193b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all gene columns are numeric\n",
    "binary_gene_columnsslw = [col for col in BioBERT_dfslw.columns if col in gene_list]  # Select only valid gene columns\n",
    "BioBERT_dfslw[binary_gene_columnsslw] = BioBERT_dfslw[binary_gene_columnsslw].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Calculate the total sum of the \"Sum_Gene_Mentions\" column\n",
    "total_gene_mentionsslw = BioBERT_dfslw[\"Sum_Gene_Mentions\"].sum()\n",
    "\n",
    "# Calculate the total number of 1s in the binary matrix (ensuring correct columns are used)\n",
    "total_binary_sumslw = BioBERT_dfslw[binary_gene_columnsslw].sum().sum()\n",
    "\n",
    "# Create a dictionary with results\n",
    "results_dict_slw = {\n",
    "    \"Metric\": [\"Total_Sum_Gene_Mentions\", \"Total_Binary_Matrix_Sum\"],\n",
    "    \"Value\": [total_gene_mentionsslw, total_binary_sumslw]\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_df_slw = pd.DataFrame(results_dict_slw)\n",
    "\n",
    "# Save to CSV file (tab-separated for .txt format)\n",
    "results_df_slw.to_csv(\"sliding_window_Sum_Gene_Mentions.txt\", sep=\"\\t\", index=False)\n",
    "\n",
    "print(f\"Results saved to 'sliding_window_Sum_Gene_Mentions.txt'\")\n",
    "print(f\"Total sum of 'Sum_Gene_Mentions' column: {total_gene_mentions}\")\n",
    "print(f\"Cross-check: Total sum of all binary matrix values (1s in the matrix): {total_binary_sum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608b5811",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Create subset of hits only\n",
    "\n",
    "# Change the working directory\n",
    "os.chdir(output_directory)\n",
    "\n",
    "# Load the previously saved CSV file\n",
    "input_filename = f\"sliding_window_gene_binary_matrix_BioBERT_MyGeneinfo_{num_articles}.csv\"\n",
    "BioBERT_df = pd.read_csv(input_filename)\n",
    "\n",
    "# Drop rows where \"Sum_Gene_Mentions\" is 0\n",
    "BioBERT_df_filteredslw = BioBERT_dfslw[BioBERT_dfslw[\"Sum_Gene_Mentions\"] > 0]\n",
    "\n",
    "# Count the number of remaining rows\n",
    "num_filtered_rowsslw = len(BioBERT_df_filteredslw)\n",
    "\n",
    "# Calculate the total sum of gene mentions after filtering\n",
    "total_gene_mentionsslw = BioBERT_df_filteredslw[\"Sum_Gene_Mentions\"].sum()\n",
    "\n",
    "# Save the filtered dataframe as a new CSV file\n",
    "output_filename = f\"sliding_window_gene_binary_matrix_BioBERT_MyGeneinfo_hits_only{num_articles}.csv\"\n",
    "BioBERT_df_filteredslw.to_csv(output_filename, index=False)\n",
    "\n",
    "# Print final row count and sum of gene mentions\n",
    "print(f\"Number of rows after filtering: {num_filtered_rowsslw}\")\n",
    "print(f\"Total sum of 'Sum_Gene_Mentions' after filtering: {total_gene_mentionsslw}\")\n",
    "print(f\"Filtered file saved as: {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bef4b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print summary statistics\n",
    "articles_wogenementionsslw = len(BioBERT_dfslw) - len(BioBERT_df_filteredslw)\n",
    "print(f\"{articles_wogenementionsslw:,}\")\n",
    "print(f\"Articles before gene filtering: {len(BioBERT_dfslw):,}\")\n",
    "print(f\"Articles without gene mentions: {articles_wogenementionsslw:,}\")\n",
    "print(f\"Articles with gene mentions:      {len(BioBERT_df_filteredslw):,}\")\n",
    "print(len(BioBERT_df_filteredslw)/len(BioBERT_dfslw)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e706636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer dataset to new variable for cleaning and normalization\n",
    "BioBERT_df_filtered=BioBERT_df_filteredslw.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8ea5b2",
   "metadata": {},
   "source": [
    "# ==================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858364b5",
   "metadata": {},
   "source": [
    "## 4) Cleaning and normalization of the new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf90bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset only if it's not already in memory\n",
    "if \"BioBERT_df_filtered\" not in globals():\n",
    "    print(\"Loading dataset from file...\")\n",
    "    BioBERT_df_filtered = pd.read_csv(\"sliding_window_gene_binary_matrix_BioBERT_MyGeneinfo_hits_only2128318.csv.csv\")\n",
    "\n",
    "# Make a copy\n",
    "BioBERT_df_filtered_cleanedv1 = BioBERT_df_filtered.copy()\n",
    "print(f\"Length of copy:      {len(BioBERT_df_filtered_cleanedv1):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7900d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the timer\n",
    "start_time = time.time()\n",
    "tqdm.pandas()\n",
    "\n",
    "### Clean PaperTitle\n",
    "title_patterns = [\n",
    "    r'^\\d{2,3}:\\s*',  # Matches \"00:\" to \"999:\"\n",
    "    r'^\\d{4}:\\s*',  # Matches \"0000:\" to \"9999:\"\n",
    "    r'^\\d{5}:\\s*',  # Matches \"00000:\" to \"99999:\"\n",
    "    r'^#\\d{3,4}\\s*',  # Matches \"#000\" and \"# 0000\"\n",
    "    r'^<PHONE>:\\s*',  # Matches \"<PHONE>: \"\n",
    "]\n",
    "title_pattern = re.compile(\"|\".join(title_patterns))\n",
    "\n",
    "print(\" Cleaning PaperTitle column...\")\n",
    "BioBERT_df_filtered_cleanedv1[\"PaperTitle\"] = BioBERT_df_filtered_cleanedv1[\"PaperTitle\"].astype(str).progress_apply(\n",
    "    lambda x: re.sub(title_pattern, '', x)\n",
    ")\n",
    "\n",
    "### Clean Abstract\n",
    "# Patterns to remove:\n",
    "abstract_patterns = [\n",
    "    r'^\\d{1,5}\\^?\\s+(?=Background)',  # Matches 1-5 digit numbers before \"Background\", with or without \"^\"\n",
    "    r'^\\d{1,5}\\s+(?=Objectives[:]?|Abstract[:]?)'  # Matches 1-5 digit numbers before \"Objectives\" or \"Abstract\"\n",
    "]\n",
    "abstract_pattern = re.compile(\"|\".join(abstract_patterns), re.IGNORECASE)\n",
    "\n",
    "print(\"Cleaning Abstract column...\")\n",
    "BioBERT_df_filtered_cleanedv1[\"Abstract\"] = BioBERT_df_filtered_cleanedv1[\"Abstract\"].astype(str).progress_apply(\n",
    "    lambda x: re.sub(abstract_pattern, '', x)\n",
    ")\n",
    "\n",
    "\n",
    "# Save cleaned dataset\n",
    "cleaned_file_path = \"cleaned_BioBERT_data.csv\"\n",
    "BioBERT_df_filtered_cleanedv1.to_csv(cleaned_file_path, index=False)\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "execution_time = round(end_time - start_time, 2)\n",
    "\n",
    "# Display runtime and confirmation message\n",
    "print(f\"\\n Cleaning complete! Dataset saved as '{cleaned_file_path}'.\")\n",
    "print(f\" Total execution time: {execution_time} seconds.\")\n",
    "\n",
    "# Display first few rows\n",
    "print(BioBERT_df_filtered_cleanedv1.head())\n",
    "print(f\"Length of full test dataset: {len(BioBERT_df_filtered_cleanedv1):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadba643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate dataset length\n",
    "len_gene_df = len(BioBERT_df_filtered_cleanedv1)\n",
    "\n",
    "# Print output\n",
    "print(f\"Length of full test dataset: {len_gene_df:,}\")\n",
    "with open(\"len_gene_df.txt\", \"w\") as file:\n",
    "    file.write(str(len_gene_df))\n",
    "print(\"File saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d0f658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate values\n",
    "articles_before = len(BioBERT_df)\n",
    "articles_without_genes = articles_wogenementions\n",
    "articles_with_genes = len(BioBERT_df_filtered)\n",
    "percentage_with_genes = (articles_with_genes / articles_before) * 100\n",
    "\n",
    "# Save output to a text file in the current directory\n",
    "with open(\"gene_NER_article_statistics.txt\", \"w\") as file:\n",
    "    file.write(f\"Articles before gene filtering: {articles_before:,}\\n\")\n",
    "    file.write(f\"Articles without gene mentions: {articles_without_genes:,}\\n\")\n",
    "    file.write(f\"Articles with gene mentions:      {articles_with_genes:,}\\n\")\n",
    "    file.write(f\"Percentage of relevant articles with gene mentions: {percentage_with_genes:.2f}%\\n\")\n",
    "    \n",
    "# Print summary\n",
    "print(f\"Articles before gene filtering: {articles_before:,}\")\n",
    "print(f\"Articles without gene mentions: {articles_without_genes:,}\")\n",
    "print(f\"Articles with gene mentions:      {articles_with_genes:,}\")\n",
    "print(f\"Percentage of relevant articles with gene mentions: {percentage_with_genes:.2f}%\")\n",
    "print(\"File saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0582ddd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final dataset:cleaned_BioBERT_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
